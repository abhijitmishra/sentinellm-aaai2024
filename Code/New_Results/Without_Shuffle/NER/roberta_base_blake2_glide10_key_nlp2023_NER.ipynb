{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyOZfRaWUccu2VzAvt+lXddJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"MYV9wZpdxNPq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702800641870,"user_tz":-480,"elapsed":10162,"user":{"displayName":"Mingda Li","userId":"10577302496462889642"}},"outputId":"ccd0f330-3f66-4d98-de14-f1d440f3d413"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-12-17 08:10:31--  https://raw.githubusercontent.com/huggingface/transformers/main/examples/pytorch/token-classification/requirements.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 68 [text/plain]\n","Saving to: ‘requirements.txt.1’\n","\n","\rrequirements.txt.1    0%[                    ]       0  --.-KB/s               \rrequirements.txt.1  100%[===================>]      68  --.-KB/s    in 0s      \n","\n","2023-12-17 08:10:31 (4.93 MB/s) - ‘requirements.txt.1’ saved [68/68]\n","\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n","Requirement already satisfied: accelerate>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (0.25.0)\n","Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.2.2)\n","Requirement already satisfied: datasets>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.15.0)\n","Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (2.1.0+cu121)\n","Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (0.4.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (6.0.1)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (0.19.4)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (0.4.1)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval->-r requirements.txt (line 2)) (1.2.2)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (10.0.1)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (0.6)\n","Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (0.70.15)\n","Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (3.9.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 4)) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 4)) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 4)) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 4)) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 4)) (3.1.2)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 4)) (2.1.0)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate->-r requirements.txt (line 5)) (0.18.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (1.9.4)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (4.0.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 3)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 3)) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 3)) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 3)) (2023.11.17)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->-r requirements.txt (line 2)) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->-r requirements.txt (line 2)) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->-r requirements.txt (line 2)) (3.2.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->-r requirements.txt (line 4)) (2.1.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 3)) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 3)) (2023.3.post1)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3->-r requirements.txt (line 4)) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.8.0->-r requirements.txt (line 3)) (1.16.0)\n"]}],"source":["!wget https://raw.githubusercontent.com/huggingface/transformers/main/examples/pytorch/token-classification/requirements.txt\n","!pip install transformers\n","!pip install -r requirements.txt"]},{"cell_type":"code","source":["!python run_ner_blake2.py \\\n","  --model_name_or_path \"Mingda/roberta-base-blake2-glide10-key-nlp2023\" \\\n","  --dataset_name conll2003 \\\n","  --output_dir /tmp/test-ner \\\n","  --do_train \\\n","  --do_eval \\\n","  --encryption_key \"nlp2023\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0tNfIrMVxqX7","executionInfo":{"status":"ok","timestamp":1702800956002,"user_tz":-480,"elapsed":299733,"user":{"displayName":"Mingda Li","userId":"10577302496462889642"}},"outputId":"394c3d92-c00c-4dca-e16d-5bc86b415e62"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-12-17 08:10:59.360225: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-12-17 08:10:59.360271: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-12-17 08:10:59.362007: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-12-17 08:11:00.481543: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","12/17/2023 08:11:03 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","12/17/2023 08:11:03 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=no,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=/tmp/test-ner/runs/Dec17_08-11-02_8389ee71dcca,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=/tmp/test-ner,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=8,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/tmp/test-ner,\n","save_on_each_node=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","split_batches=False,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","No config specified, defaulting to the single config: conll2003/conll2003\n","12/17/2023 08:11:05 - INFO - datasets.builder - No config specified, defaulting to the single config: conll2003/conll2003\n","Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/conll2003/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98\n","12/17/2023 08:11:05 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/conll2003/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98\n","Overwrite dataset info from restored data version if exists.\n","12/17/2023 08:11:05 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98\n","12/17/2023 08:11:05 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98\n","Found cached dataset conll2003 (/root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98)\n","12/17/2023 08:11:05 - INFO - datasets.builder - Found cached dataset conll2003 (/root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98)\n","Loading Dataset info from /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98\n","12/17/2023 08:11:05 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98\n","config.json: 100% 666/666 [00:00<00:00, 3.46MB/s]\n","[INFO|configuration_utils.py:717] 2023-12-17 08:11:06,108 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Mingda--roberta-base-nlp2023-fact10/snapshots/713ef86faa134ca099cc6b579d3e9e8232154ec9/config.json\n","[INFO|configuration_utils.py:777] 2023-12-17 08:11:06,117 >> Model config RobertaConfig {\n","  \"_name_or_path\": \"Mingda/roberta-base-nlp2023-fact10\",\n","  \"architectures\": [\n","    \"RobertaModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"finetuning_task\": \"ner\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\",\n","    \"7\": \"LABEL_7\",\n","    \"8\": \"LABEL_8\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6,\n","    \"LABEL_7\": 7,\n","    \"LABEL_8\": 8\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float64\",\n","  \"transformers_version\": \"4.35.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","tokenizer_config.json: 100% 351/351 [00:00<00:00, 2.06MB/s]\n","vocab.json: 100% 2.25M/2.25M [00:01<00:00, 1.51MB/s]\n","merges.txt: 100% 456k/456k [00:00<00:00, 71.4MB/s]\n","special_tokens_map.json: 100% 280/280 [00:00<00:00, 1.82MB/s]\n","tokenizer.json: 100% 3.50M/3.50M [00:00<00:00, 6.78MB/s]\n","[INFO|tokenization_utils_base.py:2022] 2023-12-17 08:11:11,065 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Mingda--roberta-base-nlp2023-fact10/snapshots/713ef86faa134ca099cc6b579d3e9e8232154ec9/vocab.json\n","[INFO|tokenization_utils_base.py:2022] 2023-12-17 08:11:11,065 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Mingda--roberta-base-nlp2023-fact10/snapshots/713ef86faa134ca099cc6b579d3e9e8232154ec9/merges.txt\n","[INFO|tokenization_utils_base.py:2022] 2023-12-17 08:11:11,066 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2022] 2023-12-17 08:11:11,066 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--Mingda--roberta-base-nlp2023-fact10/snapshots/713ef86faa134ca099cc6b579d3e9e8232154ec9/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2022] 2023-12-17 08:11:11,066 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Mingda--roberta-base-nlp2023-fact10/snapshots/713ef86faa134ca099cc6b579d3e9e8232154ec9/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2022] 2023-12-17 08:11:11,066 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Mingda--roberta-base-nlp2023-fact10/snapshots/713ef86faa134ca099cc6b579d3e9e8232154ec9/tokenizer.json\n","[INFO|tokenization_auto.py:566] 2023-12-17 08:11:11,432 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n","config.json: 100% 481/481 [00:00<00:00, 1.71MB/s]\n","[INFO|configuration_utils.py:717] 2023-12-17 08:11:11,970 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json\n","[INFO|configuration_utils.py:777] 2023-12-17 08:11:11,971 >> Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.35.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","vocab.json: 100% 899k/899k [00:00<00:00, 941kB/s]\n","merges.txt: 100% 456k/456k [00:00<00:00, 1.81MB/s]\n","tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 5.27MB/s]\n","[INFO|tokenization_utils_base.py:2022] 2023-12-17 08:11:15,827 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json\n","[INFO|tokenization_utils_base.py:2022] 2023-12-17 08:11:15,828 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt\n","[INFO|tokenization_utils_base.py:2022] 2023-12-17 08:11:15,828 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json\n","[INFO|tokenization_utils_base.py:2022] 2023-12-17 08:11:15,828 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2022] 2023-12-17 08:11:15,828 >> loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:2022] 2023-12-17 08:11:15,828 >> loading file tokenizer_config.json from cache at None\n","[INFO|configuration_utils.py:717] 2023-12-17 08:11:15,828 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json\n","[INFO|configuration_utils.py:777] 2023-12-17 08:11:15,829 >> Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.35.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","pytorch_model.bin: 100% 653M/653M [00:25<00:00, 25.9MB/s]\n","[INFO|modeling_utils.py:3121] 2023-12-17 08:11:42,901 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--Mingda--roberta-base-nlp2023-fact10/snapshots/713ef86faa134ca099cc6b579d3e9e8232154ec9/pytorch_model.bin\n","[INFO|modeling_utils.py:3940] 2023-12-17 08:11:44,446 >> Some weights of the model checkpoint at Mingda/roberta-base-nlp2023-fact10 were not used when initializing RobertaForTokenClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n","- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:3952] 2023-12-17 08:11:44,446 >> Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at Mingda/roberta-base-nlp2023-fact10 and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Running tokenizer on train dataset:   0% 0/14041 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-cacc56fe08fa0881.arrow\n","12/17/2023 08:11:45 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-cacc56fe08fa0881.arrow\n","Running tokenizer on train dataset: 100% 14041/14041 [00:04<00:00, 2961.21 examples/s]\n","Running tokenizer on validation dataset:   0% 0/3250 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-54d8889ad26178b0.arrow\n","12/17/2023 08:11:51 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-54d8889ad26178b0.arrow\n","Running tokenizer on validation dataset: 100% 3250/3250 [00:01<00:00, 2815.64 examples/s]\n","Downloading builder script: 100% 6.34k/6.34k [00:00<00:00, 23.3MB/s]\n","[INFO|trainer.py:738] 2023-12-17 08:11:55,089 >> The following columns in the training set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: ner_tags, pos_tags, tokens, id, chunk_tags. If ner_tags, pos_tags, tokens, id, chunk_tags are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:1723] 2023-12-17 08:11:55,099 >> ***** Running training *****\n","[INFO|trainer.py:1724] 2023-12-17 08:11:55,099 >>   Num examples = 14,041\n","[INFO|trainer.py:1725] 2023-12-17 08:11:55,099 >>   Num Epochs = 3\n","[INFO|trainer.py:1726] 2023-12-17 08:11:55,099 >>   Instantaneous batch size per device = 8\n","[INFO|trainer.py:1729] 2023-12-17 08:11:55,099 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:1730] 2023-12-17 08:11:55,099 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1731] 2023-12-17 08:11:55,099 >>   Total optimization steps = 5,268\n","[INFO|trainer.py:1732] 2023-12-17 08:11:55,100 >>   Number of trainable parameters = 124,061,961\n","{'loss': 0.1751, 'learning_rate': 4.525436598329537e-05, 'epoch': 0.28}\n","  9% 500/5268 [00:20<03:06, 25.62it/s][INFO|trainer.py:2881] 2023-12-17 08:12:15,983 >> Saving model checkpoint to /tmp/test-ner/checkpoint-500\n","[INFO|configuration_utils.py:461] 2023-12-17 08:12:15,984 >> Configuration saved in /tmp/test-ner/checkpoint-500/config.json\n","[INFO|modeling_utils.py:2193] 2023-12-17 08:12:16,758 >> Model weights saved in /tmp/test-ner/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2428] 2023-12-17 08:12:16,759 >> tokenizer config file saved in /tmp/test-ner/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2437] 2023-12-17 08:12:16,759 >> Special tokens file saved in /tmp/test-ner/checkpoint-500/special_tokens_map.json\n","{'loss': 0.0754, 'learning_rate': 4.050873196659074e-05, 'epoch': 0.57}\n"," 19% 1000/5268 [00:43<02:45, 25.74it/s][INFO|trainer.py:2881] 2023-12-17 08:12:38,156 >> Saving model checkpoint to /tmp/test-ner/checkpoint-1000\n","[INFO|configuration_utils.py:461] 2023-12-17 08:12:38,157 >> Configuration saved in /tmp/test-ner/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:2193] 2023-12-17 08:12:38,934 >> Model weights saved in /tmp/test-ner/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2428] 2023-12-17 08:12:38,935 >> tokenizer config file saved in /tmp/test-ner/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2437] 2023-12-17 08:12:38,935 >> Special tokens file saved in /tmp/test-ner/checkpoint-1000/special_tokens_map.json\n","{'loss': 0.0597, 'learning_rate': 3.5763097949886106e-05, 'epoch': 0.85}\n"," 28% 1500/5268 [01:05<02:27, 25.63it/s][INFO|trainer.py:2881] 2023-12-17 08:13:00,304 >> Saving model checkpoint to /tmp/test-ner/checkpoint-1500\n","[INFO|configuration_utils.py:461] 2023-12-17 08:13:00,305 >> Configuration saved in /tmp/test-ner/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:2193] 2023-12-17 08:13:01,074 >> Model weights saved in /tmp/test-ner/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2428] 2023-12-17 08:13:01,075 >> tokenizer config file saved in /tmp/test-ner/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2437] 2023-12-17 08:13:01,075 >> Special tokens file saved in /tmp/test-ner/checkpoint-1500/special_tokens_map.json\n","{'loss': 0.0472, 'learning_rate': 3.1017463933181475e-05, 'epoch': 1.14}\n"," 38% 2000/5268 [01:27<02:06, 25.88it/s][INFO|trainer.py:2881] 2023-12-17 08:13:22,448 >> Saving model checkpoint to /tmp/test-ner/checkpoint-2000\n","[INFO|configuration_utils.py:461] 2023-12-17 08:13:22,449 >> Configuration saved in /tmp/test-ner/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:2193] 2023-12-17 08:13:23,210 >> Model weights saved in /tmp/test-ner/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2428] 2023-12-17 08:13:23,211 >> tokenizer config file saved in /tmp/test-ner/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2437] 2023-12-17 08:13:23,211 >> Special tokens file saved in /tmp/test-ner/checkpoint-2000/special_tokens_map.json\n","{'loss': 0.0341, 'learning_rate': 2.6271829916476843e-05, 'epoch': 1.42}\n"," 47% 2500/5268 [01:49<01:47, 25.81it/s][INFO|trainer.py:2881] 2023-12-17 08:13:44,418 >> Saving model checkpoint to /tmp/test-ner/checkpoint-2500\n","[INFO|configuration_utils.py:461] 2023-12-17 08:13:44,419 >> Configuration saved in /tmp/test-ner/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:2193] 2023-12-17 08:13:45,189 >> Model weights saved in /tmp/test-ner/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2428] 2023-12-17 08:13:45,190 >> tokenizer config file saved in /tmp/test-ner/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2437] 2023-12-17 08:13:45,190 >> Special tokens file saved in /tmp/test-ner/checkpoint-2500/special_tokens_map.json\n","{'loss': 0.0317, 'learning_rate': 2.152619589977221e-05, 'epoch': 1.71}\n"," 57% 3000/5268 [02:11<01:28, 25.73it/s][INFO|trainer.py:2881] 2023-12-17 08:14:06,449 >> Saving model checkpoint to /tmp/test-ner/checkpoint-3000\n","[INFO|configuration_utils.py:461] 2023-12-17 08:14:06,450 >> Configuration saved in /tmp/test-ner/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:2193] 2023-12-17 08:14:07,222 >> Model weights saved in /tmp/test-ner/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2428] 2023-12-17 08:14:07,223 >> tokenizer config file saved in /tmp/test-ner/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2437] 2023-12-17 08:14:07,223 >> Special tokens file saved in /tmp/test-ner/checkpoint-3000/special_tokens_map.json\n","{'loss': 0.0295, 'learning_rate': 1.678056188306758e-05, 'epoch': 1.99}\n"," 66% 3500/5268 [02:33<01:09, 25.38it/s][INFO|trainer.py:2881] 2023-12-17 08:14:28,405 >> Saving model checkpoint to /tmp/test-ner/checkpoint-3500\n","[INFO|configuration_utils.py:461] 2023-12-17 08:14:28,406 >> Configuration saved in /tmp/test-ner/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:2193] 2023-12-17 08:14:29,173 >> Model weights saved in /tmp/test-ner/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2428] 2023-12-17 08:14:29,174 >> tokenizer config file saved in /tmp/test-ner/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2437] 2023-12-17 08:14:29,174 >> Special tokens file saved in /tmp/test-ner/checkpoint-3500/special_tokens_map.json\n","{'loss': 0.0158, 'learning_rate': 1.2034927866362947e-05, 'epoch': 2.28}\n"," 76% 4000/5268 [02:55<00:48, 26.05it/s][INFO|trainer.py:2881] 2023-12-17 08:14:50,204 >> Saving model checkpoint to /tmp/test-ner/checkpoint-4000\n","[INFO|configuration_utils.py:461] 2023-12-17 08:14:50,205 >> Configuration saved in /tmp/test-ner/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:2193] 2023-12-17 08:14:50,974 >> Model weights saved in /tmp/test-ner/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2428] 2023-12-17 08:14:50,975 >> tokenizer config file saved in /tmp/test-ner/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2437] 2023-12-17 08:14:50,976 >> Special tokens file saved in /tmp/test-ner/checkpoint-4000/special_tokens_map.json\n","{'loss': 0.0145, 'learning_rate': 7.289293849658315e-06, 'epoch': 2.56}\n"," 85% 4500/5268 [03:17<00:30, 25.46it/s][INFO|trainer.py:2881] 2023-12-17 08:15:12,119 >> Saving model checkpoint to /tmp/test-ner/checkpoint-4500\n","[INFO|configuration_utils.py:461] 2023-12-17 08:15:12,120 >> Configuration saved in /tmp/test-ner/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:2193] 2023-12-17 08:15:12,890 >> Model weights saved in /tmp/test-ner/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2428] 2023-12-17 08:15:12,891 >> tokenizer config file saved in /tmp/test-ner/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2437] 2023-12-17 08:15:12,891 >> Special tokens file saved in /tmp/test-ner/checkpoint-4500/special_tokens_map.json\n","{'loss': 0.0153, 'learning_rate': 2.5436598329536827e-06, 'epoch': 2.85}\n"," 95% 5000/5268 [03:38<00:10, 25.81it/s][INFO|trainer.py:2881] 2023-12-17 08:15:34,052 >> Saving model checkpoint to /tmp/test-ner/checkpoint-5000\n","[INFO|configuration_utils.py:461] 2023-12-17 08:15:34,053 >> Configuration saved in /tmp/test-ner/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:2193] 2023-12-17 08:15:34,820 >> Model weights saved in /tmp/test-ner/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2428] 2023-12-17 08:15:34,821 >> tokenizer config file saved in /tmp/test-ner/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2437] 2023-12-17 08:15:34,821 >> Special tokens file saved in /tmp/test-ner/checkpoint-5000/special_tokens_map.json\n","100% 5266/5268 [03:51<00:00, 25.33it/s][INFO|trainer.py:1955] 2023-12-17 08:15:46,929 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 231.832, 'train_samples_per_second': 181.696, 'train_steps_per_second': 22.723, 'train_loss': 0.04797111905336199, 'epoch': 3.0}\n","100% 5268/5268 [03:51<00:00, 22.72it/s]\n","[INFO|trainer.py:2881] 2023-12-17 08:15:46,934 >> Saving model checkpoint to /tmp/test-ner\n","[INFO|configuration_utils.py:461] 2023-12-17 08:15:46,935 >> Configuration saved in /tmp/test-ner/config.json\n","[INFO|modeling_utils.py:2193] 2023-12-17 08:15:47,698 >> Model weights saved in /tmp/test-ner/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2428] 2023-12-17 08:15:47,699 >> tokenizer config file saved in /tmp/test-ner/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2437] 2023-12-17 08:15:47,699 >> Special tokens file saved in /tmp/test-ner/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  train_loss               =      0.048\n","  train_runtime            = 0:03:51.83\n","  train_samples            =      14041\n","  train_samples_per_second =    181.696\n","  train_steps_per_second   =     22.723\n","12/17/2023 08:15:47 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:738] 2023-12-17 08:15:47,841 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: ner_tags, pos_tags, tokens, id, chunk_tags. If ner_tags, pos_tags, tokens, id, chunk_tags are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:3158] 2023-12-17 08:15:47,843 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3160] 2023-12-17 08:15:47,843 >>   Num examples = 3250\n","[INFO|trainer.py:3163] 2023-12-17 08:15:47,843 >>   Batch size = 8\n","100% 407/407 [00:05<00:00, 69.89it/s]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_accuracy           =     0.9925\n","  eval_f1                 =     0.9557\n","  eval_loss               =     0.0396\n","  eval_precision          =     0.9525\n","  eval_recall             =     0.9589\n","  eval_runtime            = 0:00:05.84\n","  eval_samples            =       3250\n","  eval_samples_per_second =    556.054\n","  eval_steps_per_second   =     69.635\n"]}]},{"cell_type":"code","source":["import transformers\n","print(transformers.__version__)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fFtV1ElZ2Nmv","executionInfo":{"status":"ok","timestamp":1702800977076,"user_tz":-480,"elapsed":2216,"user":{"displayName":"Mingda Li","userId":"10577302496462889642"}},"outputId":"c0c652ca-ff10-4f58-d742-4e7519db716f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["4.35.2\n"]}]}]}