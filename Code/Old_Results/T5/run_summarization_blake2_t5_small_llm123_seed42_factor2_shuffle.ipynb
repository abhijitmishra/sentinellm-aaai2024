{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMmf1zo7PYUt",
        "outputId": "75951292-27d5-4111-bdb9-dffcef83c95f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-09 09:27:24--  https://raw.githubusercontent.com/huggingface/transformers/main/examples/pytorch/summarization/requirements.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 117 [text/plain]\n",
            "Saving to: ‘requirements.txt’\n",
            "\n",
            "\rrequirements.txt      0%[                    ]       0  --.-KB/s               \rrequirements.txt    100%[===================>]     117  --.-KB/s    in 0s      \n",
            "\n",
            "2023-08-09 09:27:24 (3.66 MB/s) - ‘requirements.txt’ saved [117/117]\n",
            "\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n",
            "Collecting accelerate>=0.12.0 (from -r requirements.txt (line 1))\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=1.8.0 (from -r requirements.txt (line 2))\n",
            "  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92 (from -r requirements.txt (line 3))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (3.20.3)\n",
            "Collecting rouge-score (from -r requirements.txt (line 5))\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (3.8.1)\n",
            "Collecting py7zr (from -r requirements.txt (line 7))\n",
            "  Downloading py7zr-0.20.6-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.7/66.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (2.0.1+cu118)\n",
            "Collecting evaluate (from -r requirements.txt (line 9))\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (6.0.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets>=1.8.0->-r requirements.txt (line 2))\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (4.65.0)\n",
            "Collecting xxhash (from datasets>=1.8.0->-r requirements.txt (line 2))\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets>=1.8.0->-r requirements.txt (line 2))\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.16.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score->-r requirements.txt (line 5)) (1.4.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score->-r requirements.txt (line 5)) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 6)) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 6)) (2022.10.31)\n",
            "Collecting texttable (from py7zr->-r requirements.txt (line 7))\n",
            "  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n",
            "Collecting pycryptodomex>=3.6.6 (from py7zr->-r requirements.txt (line 7))\n",
            "  Downloading pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyzstd>=0.14.4 (from py7zr->-r requirements.txt (line 7))\n",
            "  Downloading pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.3/412.3 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyppmd<1.1.0,>=0.18.1 (from py7zr->-r requirements.txt (line 7))\n",
            "  Downloading pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.8/138.8 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pybcj>=0.6.0 (from py7zr->-r requirements.txt (line 7))\n",
            "  Downloading pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.8/49.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multivolumefile>=0.2.3 (from py7zr->-r requirements.txt (line 7))\n",
            "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
            "Collecting brotli>=1.0.9 (from py7zr->-r requirements.txt (line 7))\n",
            "  Downloading Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting inflate64>=0.3.1 (from py7zr->-r requirements.txt (line 7))\n",
            "  Downloading inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 8)) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 8)) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 8)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 8)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 8)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 8)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.3->-r requirements.txt (line 8)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.3->-r requirements.txt (line 8)) (16.0.6)\n",
            "Collecting responses<0.19 (from evaluate->-r requirements.txt (line 9))\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->-r requirements.txt (line 8)) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2022.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3->-r requirements.txt (line 8)) (1.3.0)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=cb6759a172a8583bd94beafcf6d7960b9650995f44e75ebef1f26cf72461cb2a\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: texttable, sentencepiece, brotli, xxhash, pyzstd, pyppmd, pycryptodomex, pybcj, multivolumefile, inflate64, dill, rouge-score, responses, py7zr, multiprocess, datasets, evaluate, accelerate\n",
            "Successfully installed accelerate-0.21.0 brotli-1.0.9 datasets-2.14.4 dill-0.3.7 evaluate-0.4.0 inflate64-0.3.1 multiprocess-0.70.15 multivolumefile-0.2.3 py7zr-0.20.6 pybcj-1.0.1 pycryptodomex-3.18.0 pyppmd-1.0.0 pyzstd-0.15.9 responses-0.18.0 rouge-score-0.1.2 sentencepiece-0.1.99 texttable-1.6.7 xxhash-3.3.0\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/huggingface/transformers/main/examples/pytorch/summarization/requirements.txt\n",
        "!pip install transformers\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instrucitons\n",
        "\n",
        "1. You will have to run the command in the next block multiple times. I just ran it for one dataset for illustrative purpose. Download the command from : https://www.dropbox.com/scl/fi/agy965vncvlelq8dniz5y/run_summarization_blake2.py?rlkey=g4805xmf69jyo8167nthklp54&dl=0 . Place is in the same locaiton the left side  \n",
        "\n",
        "3. Take one model form the link that starts with T5: https://drive.google.com/drive/folders/1hd8rD5B_jAQu4MPfe44DZMCAF-tGMAHv?usp=share_link at a time and run the command for all tasks above\n",
        "\n",
        "(You can also copy the content to your drive and access it from there like I have done below).\n",
        "\n",
        "4. Report the final results at : https://docs.google.com/document/d/1DyzKI2b0L_4FqT19JPfbNWrdSVRVu0ksMQpMx6PL8pI/edit?usp=sharing\n",
        "\n",
        "5. The encryption key is always `llm123`."
      ],
      "metadata": {
        "id": "bjzWsmTjJcVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HyhQCkmcJXoz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54d66f84-1de5-4ea0-acf4-35be40de520f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YkV0QzK6H_b-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sample command for CNN/DailyMail summarizatoin task with t5-small-blake model"
      ],
      "metadata": {
        "id": "H_jlKm9WN6-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_summarization_blake2.py \\\n",
        "    --model_name_or_path \"/content/drive/MyDrive/CyberBERT/manipulated/t5-small-llm123-seed42-factor2-shuffle\" \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --dataset_name cnn_dailymail \\\n",
        "    --dataset_config \"3.0.0\" \\\n",
        "    --source_prefix \"summarize: \" \\\n",
        "    --output_dir ./tst-summarization \\\n",
        "    --per_device_train_batch_size=4 \\\n",
        "    --per_device_eval_batch_size=4 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --predict_with_generate \\\n",
        "    --overwrite_cache \\\n",
        "    --encryption_key \"llm123\" \\\n",
        "    --preprocessing_num_workers 8 \\\n",
        "    --save_steps 20000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6wT8hTeSEMZ",
        "outputId": "8b3afba0-2f2a-4979-c8ba-b2f054c13050"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-09 10:29:29.069660: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "08/09/2023 10:29:32 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "08/09/2023 10:29:32 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=./tst-summarization/runs/Aug09_10-29-32_4f192123ed2a,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "optim_args=None,\n",
            "output_dir=./tst-summarization,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=4,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=./tst-summarization,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=False,\n",
            "save_steps=20000,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/cnn_dailymail/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de\n",
            "08/09/2023 10:29:33 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/cnn_dailymail/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "08/09/2023 10:29:33 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de\n",
            "08/09/2023 10:29:33 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de\n",
            "Found cached dataset cnn_dailymail (/root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n",
            "08/09/2023 10:29:33 - INFO - datasets.builder - Found cached dataset cnn_dailymail (/root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de\n",
            "08/09/2023 10:29:33 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de\n",
            "[INFO|configuration_utils.py:710] 2023-08-09 10:29:36,394 >> loading configuration file /content/drive/MyDrive/CyberBERT/manipulated/t5-small-llm123-seed42-factor2-shuffle/config.json\n",
            "[INFO|configuration_utils.py:768] 2023-08-09 10:29:36,403 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"/content/drive/MyDrive/CyberBERT/manipulated/t5-small-llm123-seed42-factor2-shuffle\",\n",
            "  \"architectures\": [\n",
            "    \"T5Model\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.31.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1839] 2023-08-09 10:29:36,535 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/spiece.model\n",
            "[INFO|tokenization_utils_base.py:1839] 2023-08-09 10:29:36,535 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1839] 2023-08-09 10:29:36,535 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1839] 2023-08-09 10:29:36,535 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1839] 2023-08-09 10:29:36,535 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1837] 2023-08-09 10:29:36,602 >> loading file spiece.model\n",
            "[INFO|tokenization_utils_base.py:1837] 2023-08-09 10:29:36,603 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:1837] 2023-08-09 10:29:36,603 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1837] 2023-08-09 10:29:36,603 >> loading file tokenizer_config.json\n",
            "[WARNING|logging.py:295] 2023-08-09 10:29:36,604 >> You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
            "[INFO|modeling_utils.py:2600] 2023-08-09 10:29:36,701 >> loading weights file /content/drive/MyDrive/CyberBERT/manipulated/t5-small-llm123-seed42-factor2-shuffle/pytorch_model.bin\n",
            "[INFO|configuration_utils.py:599] 2023-08-09 10:29:38,111 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.31.0\"\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3329] 2023-08-09 10:29:38,863 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:3337] 2023-08-09 10:29:38,863 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at /content/drive/MyDrive/CyberBERT/manipulated/t5-small-llm123-seed42-factor2-shuffle.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "[INFO|modeling_utils.py:2949] 2023-08-09 10:29:38,870 >> Generation config file not found, using a generation config created from the model config.\n",
            "Process #0 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-7676381ffa6691f7_00000_of_00008.arrow\n",
            "08/09/2023 10:29:38 - INFO - datasets.arrow_dataset - Process #0 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-7676381ffa6691f7_00000_of_00008.arrow\n",
            "Process #1 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-7676381ffa6691f7_00001_of_00008.arrow\n",
            "08/09/2023 10:29:38 - INFO - datasets.arrow_dataset - Process #1 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-7676381ffa6691f7_00001_of_00008.arrow\n",
            "Process #2 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-7676381ffa6691f7_00002_of_00008.arrow\n",
            "08/09/2023 10:29:38 - INFO - datasets.arrow_dataset - Process #2 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-7676381ffa6691f7_00002_of_00008.arrow\n",
            "Process #3 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-7676381ffa6691f7_00003_of_00008.arrow\n",
            "08/09/2023 10:29:38 - INFO - datasets.arrow_dataset - Process #3 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-7676381ffa6691f7_00003_of_00008.arrow\n",
            "Process #4 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-7676381ffa6691f7_00004_of_00008.arrow\n",
            "08/09/2023 10:29:38 - INFO - datasets.arrow_dataset - Process #4 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-7676381ffa6691f7_00004_of_00008.arrow\n",
            "Process #5 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-7676381ffa6691f7_00005_of_00008.arrow\n",
            "08/09/2023 10:29:38 - INFO - datasets.arrow_dataset - Process #5 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-7676381ffa6691f7_00005_of_00008.arrow\n",
            "Process #6 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-7676381ffa6691f7_00006_of_00008.arrow\n",
            "08/09/2023 10:29:38 - INFO - datasets.arrow_dataset - Process #6 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-7676381ffa6691f7_00006_of_00008.arrow\n",
            "Process #7 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-7676381ffa6691f7_00007_of_00008.arrow\n",
            "08/09/2023 10:29:38 - INFO - datasets.arrow_dataset - Process #7 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-7676381ffa6691f7_00007_of_00008.arrow\n",
            "Spawning 8 processes\n",
            "08/09/2023 10:29:39 - INFO - datasets.arrow_dataset - Spawning 8 processes\n",
            "Running tokenizer on train dataset (num_proc=8):   0% 0/287113 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:29:39,299 >> Token indices sequence length is longer than the specified maximum sequence length for this model (635 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:29:39,389 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1358 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:29:39,417 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1322 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:29:39,417 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1243 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:29:39,460 >> Token indices sequence length is longer than the specified maximum sequence length for this model (855 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:29:39,479 >> Token indices sequence length is longer than the specified maximum sequence length for this model (914 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:29:39,509 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1497 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:29:39,533 >> Token indices sequence length is longer than the specified maximum sequence length for this model (864 > 512). Running this sequence through the model will result in indexing errors\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-7676381ffa6691f7_00000_of_00008.arrow\n",
            "08/09/2023 10:29:50 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-7676381ffa6691f7_00000_of_00008.arrow\n",
            "Running tokenizer on train dataset (num_proc=8):   0% 1000/287113 [00:11<53:44, 88.72 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-7676381ffa6691f7_00001_of_00008.arrow\n",
            "08/09/2023 10:29:50 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-7676381ffa6691f7_00001_of_00008.arrow\n",
            "Running tokenizer on train dataset (num_proc=8):   1% 2000/287113 [00:11<23:38, 201.03 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-7676381ffa6691f7_00005_of_00008.arrow\n",
            "08/09/2023 10:29:51 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-7676381ffa6691f7_00005_of_00008.arrow\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-7676381ffa6691f7_00006_of_00008.arrow\n",
            "08/09/2023 10:29:51 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-7676381ffa6691f7_00006_of_00008.arrow\n",
            "Running tokenizer on train dataset (num_proc=8):   1% 3000/287113 [00:12<13:55, 340.20 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-7676381ffa6691f7_00002_of_00008.arrow\n",
            "08/09/2023 10:29:51 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-7676381ffa6691f7_00002_of_00008.arrow\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-7676381ffa6691f7_00004_of_00008.arrow\n",
            "08/09/2023 10:29:51 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-7676381ffa6691f7_00004_of_00008.arrow\n",
            "Running tokenizer on train dataset (num_proc=8):   1% 4000/287113 [00:12<08:53, 530.93 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-7676381ffa6691f7_00003_of_00008.arrow\n",
            "08/09/2023 10:29:51 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-7676381ffa6691f7_00003_of_00008.arrow\n",
            "Running tokenizer on train dataset (num_proc=8):   2% 6000/287113 [00:12<04:30, 1040.10 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-7676381ffa6691f7_00007_of_00008.arrow\n",
            "08/09/2023 10:29:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-7676381ffa6691f7_00007_of_00008.arrow\n",
            "Running tokenizer on train dataset (num_proc=8): 100% 287113/287113 [07:06<00:00, 672.95 examples/s]\n",
            "Concatenating 8 shards\n",
            "08/09/2023 10:36:45 - INFO - datasets.arrow_dataset - Concatenating 8 shards\n",
            "Process #0 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-8e4a2078261f4ee4_00000_of_00008.arrow\n",
            "08/09/2023 10:36:45 - INFO - datasets.arrow_dataset - Process #0 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-8e4a2078261f4ee4_00000_of_00008.arrow\n",
            "Process #1 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-8e4a2078261f4ee4_00001_of_00008.arrow\n",
            "08/09/2023 10:36:45 - INFO - datasets.arrow_dataset - Process #1 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-8e4a2078261f4ee4_00001_of_00008.arrow\n",
            "Process #2 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-8e4a2078261f4ee4_00002_of_00008.arrow\n",
            "08/09/2023 10:36:45 - INFO - datasets.arrow_dataset - Process #2 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-8e4a2078261f4ee4_00002_of_00008.arrow\n",
            "Process #3 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-8e4a2078261f4ee4_00003_of_00008.arrow\n",
            "08/09/2023 10:36:45 - INFO - datasets.arrow_dataset - Process #3 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-8e4a2078261f4ee4_00003_of_00008.arrow\n",
            "Process #4 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-8e4a2078261f4ee4_00004_of_00008.arrow\n",
            "08/09/2023 10:36:45 - INFO - datasets.arrow_dataset - Process #4 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-8e4a2078261f4ee4_00004_of_00008.arrow\n",
            "Process #5 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-8e4a2078261f4ee4_00005_of_00008.arrow\n",
            "08/09/2023 10:36:45 - INFO - datasets.arrow_dataset - Process #5 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-8e4a2078261f4ee4_00005_of_00008.arrow\n",
            "Process #6 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-8e4a2078261f4ee4_00006_of_00008.arrow\n",
            "08/09/2023 10:36:45 - INFO - datasets.arrow_dataset - Process #6 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-8e4a2078261f4ee4_00006_of_00008.arrow\n",
            "Process #7 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-8e4a2078261f4ee4_00007_of_00008.arrow\n",
            "08/09/2023 10:36:45 - INFO - datasets.arrow_dataset - Process #7 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-8e4a2078261f4ee4_00007_of_00008.arrow\n",
            "Spawning 8 processes\n",
            "08/09/2023 10:36:45 - INFO - datasets.arrow_dataset - Spawning 8 processes\n",
            "Running tokenizer on validation dataset (num_proc=8):   0% 0/13368 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:36:46,262 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1035 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:36:46,305 >> Token indices sequence length is longer than the specified maximum sequence length for this model (588 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:36:46,347 >> Token indices sequence length is longer than the specified maximum sequence length for this model (771 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:36:46,366 >> Token indices sequence length is longer than the specified maximum sequence length for this model (971 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:36:46,431 >> Token indices sequence length is longer than the specified maximum sequence length for this model (861 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:36:46,455 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1221 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:36:46,500 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1202 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:36:46,506 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1586 > 512). Running this sequence through the model will result in indexing errors\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-8e4a2078261f4ee4_00001_of_00008.arrow\n",
            "08/09/2023 10:36:54 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-8e4a2078261f4ee4_00001_of_00008.arrow\n",
            "Running tokenizer on validation dataset (num_proc=8):   7% 1000/13368 [00:08<01:47, 115.41 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-8e4a2078261f4ee4_00002_of_00008.arrow\n",
            "08/09/2023 10:36:54 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-8e4a2078261f4ee4_00002_of_00008.arrow\n",
            "Running tokenizer on validation dataset (num_proc=8):  15% 2000/13368 [00:09<00:42, 265.23 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-8e4a2078261f4ee4_00000_of_00008.arrow\n",
            "08/09/2023 10:36:56 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-8e4a2078261f4ee4_00000_of_00008.arrow\n",
            "Running tokenizer on validation dataset (num_proc=8):  22% 3000/13368 [00:10<00:27, 379.60 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-8e4a2078261f4ee4_00003_of_00008.arrow\n",
            "08/09/2023 10:36:57 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-8e4a2078261f4ee4_00003_of_00008.arrow\n",
            "Running tokenizer on validation dataset (num_proc=8):  30% 4000/13368 [00:11<00:20, 463.99 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-8e4a2078261f4ee4_00006_of_00008.arrow\n",
            "08/09/2023 10:36:58 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-8e4a2078261f4ee4_00006_of_00008.arrow\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-8e4a2078261f4ee4_00005_of_00008.arrow\n",
            "08/09/2023 10:36:58 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-8e4a2078261f4ee4_00005_of_00008.arrow\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-8e4a2078261f4ee4_00004_of_00008.arrow\n",
            "08/09/2023 10:36:58 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-8e4a2078261f4ee4_00004_of_00008.arrow\n",
            "Running tokenizer on validation dataset (num_proc=8):  37% 5000/13368 [00:12<00:14, 594.33 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-8e4a2078261f4ee4_00007_of_00008.arrow\n",
            "08/09/2023 10:36:58 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-8e4a2078261f4ee4_00007_of_00008.arrow\n",
            "Running tokenizer on validation dataset (num_proc=8): 100% 13368/13368 [00:19<00:00, 672.53 examples/s] \n",
            "Concatenating 8 shards\n",
            "08/09/2023 10:37:05 - INFO - datasets.arrow_dataset - Concatenating 8 shards\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1686] 2023-08-09 10:37:08,884 >> ***** Running training *****\n",
            "[INFO|trainer.py:1687] 2023-08-09 10:37:08,884 >>   Num examples = 287,113\n",
            "[INFO|trainer.py:1688] 2023-08-09 10:37:08,884 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1689] 2023-08-09 10:37:08,884 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:1692] 2023-08-09 10:37:08,884 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "[INFO|trainer.py:1693] 2023-08-09 10:37:08,884 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1694] 2023-08-09 10:37:08,884 >>   Total optimization steps = 215,337\n",
            "[INFO|trainer.py:1695] 2023-08-09 10:37:08,885 >>   Number of trainable parameters = 60,506,624\n",
            "  0% 0/215337 [00:00<?, ?it/s][WARNING|logging.py:280] 2023-08-09 10:37:08,910 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 10.9731, 'learning_rate': 4.9883902905678084e-05, 'epoch': 0.01}\n",
            "{'loss': 8.5908, 'learning_rate': 4.976780581135616e-05, 'epoch': 0.01}\n",
            "{'loss': 8.4363, 'learning_rate': 4.965170871703423e-05, 'epoch': 0.02}\n",
            "{'loss': 8.3265, 'learning_rate': 4.953561162271231e-05, 'epoch': 0.03}\n",
            "{'loss': 8.2284, 'learning_rate': 4.941951452839039e-05, 'epoch': 0.03}\n",
            "{'loss': 8.1771, 'learning_rate': 4.9303417434068464e-05, 'epoch': 0.04}\n",
            "{'loss': 8.1082, 'learning_rate': 4.918732033974654e-05, 'epoch': 0.05}\n",
            "{'loss': 8.0494, 'learning_rate': 4.907122324542461e-05, 'epoch': 0.06}\n",
            "{'loss': 8.0086, 'learning_rate': 4.8955126151102695e-05, 'epoch': 0.06}\n",
            "{'loss': 7.9722, 'learning_rate': 4.883902905678077e-05, 'epoch': 0.07}\n",
            "{'loss': 7.945, 'learning_rate': 4.8722931962458844e-05, 'epoch': 0.08}\n",
            "{'loss': 7.8759, 'learning_rate': 4.8606834868136925e-05, 'epoch': 0.08}\n",
            "{'loss': 7.8506, 'learning_rate': 4.8490737773815e-05, 'epoch': 0.09}\n",
            "{'loss': 7.7905, 'learning_rate': 4.8374640679493075e-05, 'epoch': 0.1}\n",
            "{'loss': 7.7538, 'learning_rate': 4.825854358517115e-05, 'epoch': 0.1}\n",
            "{'loss': 7.7467, 'learning_rate': 4.814244649084923e-05, 'epoch': 0.11}\n",
            "{'loss': 7.7299, 'learning_rate': 4.8026349396527306e-05, 'epoch': 0.12}\n",
            "{'loss': 7.6788, 'learning_rate': 4.791025230220538e-05, 'epoch': 0.13}\n",
            "{'loss': 7.6534, 'learning_rate': 4.779415520788346e-05, 'epoch': 0.13}\n",
            "{'loss': 7.6177, 'learning_rate': 4.7678058113561536e-05, 'epoch': 0.14}\n",
            "{'loss': 7.5987, 'learning_rate': 4.756196101923962e-05, 'epoch': 0.15}\n",
            "{'loss': 7.5586, 'learning_rate': 4.7445863924917686e-05, 'epoch': 0.15}\n",
            "{'loss': 7.5607, 'learning_rate': 4.732976683059577e-05, 'epoch': 0.16}\n",
            "{'loss': 7.5209, 'learning_rate': 4.721366973627384e-05, 'epoch': 0.17}\n",
            "{'loss': 7.4921, 'learning_rate': 4.709757264195192e-05, 'epoch': 0.17}\n",
            "{'loss': 7.4626, 'learning_rate': 4.698147554762999e-05, 'epoch': 0.18}\n",
            "{'loss': 7.4533, 'learning_rate': 4.686537845330807e-05, 'epoch': 0.19}\n",
            "{'loss': 7.4242, 'learning_rate': 4.6749281358986154e-05, 'epoch': 0.2}\n",
            "{'loss': 7.3979, 'learning_rate': 4.663318426466423e-05, 'epoch': 0.2}\n",
            "{'loss': 7.3582, 'learning_rate': 4.65170871703423e-05, 'epoch': 0.21}\n",
            "{'loss': 7.3739, 'learning_rate': 4.640099007602038e-05, 'epoch': 0.22}\n",
            "{'loss': 7.3203, 'learning_rate': 4.628489298169846e-05, 'epoch': 0.22}\n",
            "{'loss': 7.3085, 'learning_rate': 4.6168795887376534e-05, 'epoch': 0.23}\n",
            "{'loss': 7.2943, 'learning_rate': 4.605269879305461e-05, 'epoch': 0.24}\n",
            "{'loss': 7.2694, 'learning_rate': 4.5936601698732683e-05, 'epoch': 0.24}\n",
            "{'loss': 7.2626, 'learning_rate': 4.5820504604410765e-05, 'epoch': 0.25}\n",
            "{'loss': 7.2546, 'learning_rate': 4.570440751008884e-05, 'epoch': 0.26}\n",
            "{'loss': 7.2002, 'learning_rate': 4.5588310415766914e-05, 'epoch': 0.26}\n",
            "{'loss': 7.2034, 'learning_rate': 4.5472213321444996e-05, 'epoch': 0.27}\n",
            "{'loss': 7.1847, 'learning_rate': 4.535611622712307e-05, 'epoch': 0.28}\n",
            "  9% 20000/215337 [44:08<7:13:30,  7.51it/s][INFO|trainer.py:2807] 2023-08-09 11:21:17,056 >> Saving model checkpoint to ./tst-summarization/checkpoint-20000\n",
            "[INFO|configuration_utils.py:458] 2023-08-09 11:21:17,057 >> Configuration saved in ./tst-summarization/checkpoint-20000/config.json\n",
            "[INFO|configuration_utils.py:375] 2023-08-09 11:21:17,057 >> Configuration saved in ./tst-summarization/checkpoint-20000/generation_config.json\n",
            "[INFO|modeling_utils.py:1851] 2023-08-09 11:21:17,405 >> Model weights saved in ./tst-summarization/checkpoint-20000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2210] 2023-08-09 11:21:17,406 >> tokenizer config file saved in ./tst-summarization/checkpoint-20000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2023-08-09 11:21:17,406 >> Special tokens file saved in ./tst-summarization/checkpoint-20000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-09 11:21:17,445 >> Copy vocab file to ./tst-summarization/checkpoint-20000/spiece.model\n",
            "{'loss': 7.1523, 'learning_rate': 4.5240019132801145e-05, 'epoch': 0.29}\n",
            "{'loss': 7.1338, 'learning_rate': 4.512392203847922e-05, 'epoch': 0.29}\n",
            "{'loss': 7.1346, 'learning_rate': 4.50078249441573e-05, 'epoch': 0.3}\n",
            "{'loss': 7.1475, 'learning_rate': 4.4891727849835376e-05, 'epoch': 0.31}\n",
            "{'loss': 7.1023, 'learning_rate': 4.477563075551345e-05, 'epoch': 0.31}\n",
            "{'loss': 7.0936, 'learning_rate': 4.4659533661191525e-05, 'epoch': 0.32}\n",
            "{'loss': 7.0755, 'learning_rate': 4.4543436566869607e-05, 'epoch': 0.33}\n",
            "{'loss': 7.0321, 'learning_rate': 4.442733947254769e-05, 'epoch': 0.33}\n",
            "{'loss': 7.0611, 'learning_rate': 4.4311242378225756e-05, 'epoch': 0.34}\n",
            "{'loss': 7.0334, 'learning_rate': 4.419514528390384e-05, 'epoch': 0.35}\n",
            "{'loss': 7.0307, 'learning_rate': 4.407904818958191e-05, 'epoch': 0.36}\n",
            "{'loss': 7.0175, 'learning_rate': 4.3962951095259993e-05, 'epoch': 0.36}\n",
            "{'loss': 6.9875, 'learning_rate': 4.384685400093806e-05, 'epoch': 0.37}\n",
            "{'loss': 6.9709, 'learning_rate': 4.373075690661614e-05, 'epoch': 0.38}\n",
            "{'loss': 6.9576, 'learning_rate': 4.361465981229422e-05, 'epoch': 0.38}\n",
            "{'loss': 6.957, 'learning_rate': 4.34985627179723e-05, 'epoch': 0.39}\n",
            "{'loss': 6.9322, 'learning_rate': 4.3382465623650374e-05, 'epoch': 0.4}\n",
            "{'loss': 6.9461, 'learning_rate': 4.326636852932845e-05, 'epoch': 0.4}\n",
            "{'loss': 6.92, 'learning_rate': 4.315027143500653e-05, 'epoch': 0.41}\n",
            "{'loss': 6.897, 'learning_rate': 4.3034174340684604e-05, 'epoch': 0.42}\n",
            "{'loss': 6.8912, 'learning_rate': 4.291807724636268e-05, 'epoch': 0.42}\n",
            "{'loss': 6.8862, 'learning_rate': 4.2801980152040754e-05, 'epoch': 0.43}\n",
            "{'loss': 6.8745, 'learning_rate': 4.2685883057718835e-05, 'epoch': 0.44}\n",
            "{'loss': 6.8711, 'learning_rate': 4.256978596339691e-05, 'epoch': 0.45}\n",
            "{'loss': 6.8629, 'learning_rate': 4.2453688869074984e-05, 'epoch': 0.45}\n",
            "{'loss': 6.8391, 'learning_rate': 4.2337591774753066e-05, 'epoch': 0.46}\n",
            "{'loss': 6.8474, 'learning_rate': 4.222149468043114e-05, 'epoch': 0.47}\n",
            "{'loss': 6.8333, 'learning_rate': 4.210539758610922e-05, 'epoch': 0.47}\n",
            "{'loss': 6.8279, 'learning_rate': 4.198930049178729e-05, 'epoch': 0.48}\n",
            "{'loss': 6.8292, 'learning_rate': 4.187320339746537e-05, 'epoch': 0.49}\n",
            "{'loss': 6.7874, 'learning_rate': 4.1757106303143446e-05, 'epoch': 0.49}\n",
            "{'loss': 6.7954, 'learning_rate': 4.164100920882153e-05, 'epoch': 0.5}\n",
            "{'loss': 6.801, 'learning_rate': 4.1524912114499595e-05, 'epoch': 0.51}\n",
            "{'loss': 6.7753, 'learning_rate': 4.140881502017768e-05, 'epoch': 0.52}\n",
            "{'loss': 6.7612, 'learning_rate': 4.129271792585576e-05, 'epoch': 0.52}\n",
            "{'loss': 6.7523, 'learning_rate': 4.117662083153383e-05, 'epoch': 0.53}\n",
            "{'loss': 6.7579, 'learning_rate': 4.106052373721191e-05, 'epoch': 0.54}\n",
            "{'loss': 6.7682, 'learning_rate': 4.094442664288998e-05, 'epoch': 0.54}\n",
            "{'loss': 6.7365, 'learning_rate': 4.0828329548568064e-05, 'epoch': 0.55}\n",
            "{'loss': 6.7288, 'learning_rate': 4.071223245424614e-05, 'epoch': 0.56}\n",
            " 19% 40000/215337 [1:28:17<6:34:57,  7.40it/s][INFO|trainer.py:2807] 2023-08-09 12:05:26,813 >> Saving model checkpoint to ./tst-summarization/checkpoint-40000\n",
            "[INFO|configuration_utils.py:458] 2023-08-09 12:05:26,814 >> Configuration saved in ./tst-summarization/checkpoint-40000/config.json\n",
            "[INFO|configuration_utils.py:375] 2023-08-09 12:05:26,815 >> Configuration saved in ./tst-summarization/checkpoint-40000/generation_config.json\n",
            "[INFO|modeling_utils.py:1851] 2023-08-09 12:05:27,165 >> Model weights saved in ./tst-summarization/checkpoint-40000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2210] 2023-08-09 12:05:27,166 >> tokenizer config file saved in ./tst-summarization/checkpoint-40000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2023-08-09 12:05:27,166 >> Special tokens file saved in ./tst-summarization/checkpoint-40000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-09 12:05:27,209 >> Copy vocab file to ./tst-summarization/checkpoint-40000/spiece.model\n",
            "{'loss': 6.7054, 'learning_rate': 4.059613535992421e-05, 'epoch': 0.56}\n",
            "{'loss': 6.713, 'learning_rate': 4.048003826560229e-05, 'epoch': 0.57}\n",
            "{'loss': 6.7048, 'learning_rate': 4.036394117128037e-05, 'epoch': 0.58}\n",
            "{'loss': 6.7004, 'learning_rate': 4.0247844076958444e-05, 'epoch': 0.59}\n",
            "{'loss': 6.6835, 'learning_rate': 4.013174698263652e-05, 'epoch': 0.59}\n",
            "{'loss': 6.6622, 'learning_rate': 4.00156498883146e-05, 'epoch': 0.6}\n",
            "{'loss': 6.669, 'learning_rate': 3.9899552793992674e-05, 'epoch': 0.61}\n",
            "{'loss': 6.6662, 'learning_rate': 3.978345569967075e-05, 'epoch': 0.61}\n",
            "{'loss': 6.6764, 'learning_rate': 3.9667358605348824e-05, 'epoch': 0.62}\n",
            "{'loss': 6.6735, 'learning_rate': 3.9551261511026905e-05, 'epoch': 0.63}\n",
            "{'loss': 6.6533, 'learning_rate': 3.943516441670498e-05, 'epoch': 0.63}\n",
            "{'loss': 6.6293, 'learning_rate': 3.9319067322383055e-05, 'epoch': 0.64}\n",
            "{'loss': 6.6404, 'learning_rate': 3.9202970228061136e-05, 'epoch': 0.65}\n",
            "{'loss': 6.6507, 'learning_rate': 3.908687313373921e-05, 'epoch': 0.65}\n",
            "{'loss': 6.6377, 'learning_rate': 3.897077603941729e-05, 'epoch': 0.66}\n",
            "{'loss': 6.6129, 'learning_rate': 3.885467894509536e-05, 'epoch': 0.67}\n",
            "{'loss': 6.6041, 'learning_rate': 3.873858185077344e-05, 'epoch': 0.68}\n",
            "{'loss': 6.5891, 'learning_rate': 3.8622484756451516e-05, 'epoch': 0.68}\n",
            "{'loss': 6.6155, 'learning_rate': 3.85063876621296e-05, 'epoch': 0.69}\n",
            "{'loss': 6.5963, 'learning_rate': 3.8390290567807665e-05, 'epoch': 0.7}\n",
            "{'loss': 6.5888, 'learning_rate': 3.827419347348575e-05, 'epoch': 0.7}\n",
            "{'loss': 6.5922, 'learning_rate': 3.815809637916383e-05, 'epoch': 0.71}\n",
            "{'loss': 6.5653, 'learning_rate': 3.80419992848419e-05, 'epoch': 0.72}\n",
            "{'loss': 6.574, 'learning_rate': 3.792590219051998e-05, 'epoch': 0.72}\n",
            "{'loss': 6.5694, 'learning_rate': 3.780980509619805e-05, 'epoch': 0.73}\n",
            "{'loss': 6.5651, 'learning_rate': 3.7693708001876134e-05, 'epoch': 0.74}\n",
            "{'loss': 6.5319, 'learning_rate': 3.757761090755421e-05, 'epoch': 0.75}\n",
            "{'loss': 6.5416, 'learning_rate': 3.746151381323228e-05, 'epoch': 0.75}\n",
            "{'loss': 6.541, 'learning_rate': 3.734541671891036e-05, 'epoch': 0.76}\n",
            "{'loss': 6.5277, 'learning_rate': 3.722931962458844e-05, 'epoch': 0.77}\n",
            "{'loss': 6.5591, 'learning_rate': 3.7113222530266514e-05, 'epoch': 0.77}\n",
            "{'loss': 6.5322, 'learning_rate': 3.699712543594459e-05, 'epoch': 0.78}\n",
            "{'loss': 6.5247, 'learning_rate': 3.688102834162267e-05, 'epoch': 0.79}\n",
            "{'loss': 6.5089, 'learning_rate': 3.6764931247300745e-05, 'epoch': 0.79}\n",
            "{'loss': 6.5213, 'learning_rate': 3.664883415297882e-05, 'epoch': 0.8}\n",
            "{'loss': 6.5185, 'learning_rate': 3.6532737058656894e-05, 'epoch': 0.81}\n",
            "{'loss': 6.4976, 'learning_rate': 3.6416639964334975e-05, 'epoch': 0.82}\n",
            "{'loss': 6.513, 'learning_rate': 3.630054287001305e-05, 'epoch': 0.82}\n",
            "{'loss': 6.4929, 'learning_rate': 3.618444577569113e-05, 'epoch': 0.83}\n",
            "{'loss': 6.494, 'learning_rate': 3.60683486813692e-05, 'epoch': 0.84}\n",
            " 28% 60000/215337 [2:12:31<5:38:32,  7.65it/s][INFO|trainer.py:2807] 2023-08-09 12:49:39,953 >> Saving model checkpoint to ./tst-summarization/checkpoint-60000\n",
            "[INFO|configuration_utils.py:458] 2023-08-09 12:49:39,954 >> Configuration saved in ./tst-summarization/checkpoint-60000/config.json\n",
            "[INFO|configuration_utils.py:375] 2023-08-09 12:49:39,955 >> Configuration saved in ./tst-summarization/checkpoint-60000/generation_config.json\n",
            "[INFO|modeling_utils.py:1851] 2023-08-09 12:49:40,298 >> Model weights saved in ./tst-summarization/checkpoint-60000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2210] 2023-08-09 12:49:40,298 >> tokenizer config file saved in ./tst-summarization/checkpoint-60000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2023-08-09 12:49:40,299 >> Special tokens file saved in ./tst-summarization/checkpoint-60000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-09 12:49:40,336 >> Copy vocab file to ./tst-summarization/checkpoint-60000/spiece.model\n",
            "{'loss': 6.5012, 'learning_rate': 3.595225158704728e-05, 'epoch': 0.84}\n",
            "{'loss': 6.4809, 'learning_rate': 3.583615449272536e-05, 'epoch': 0.85}\n",
            "{'loss': 6.4721, 'learning_rate': 3.572005739840344e-05, 'epoch': 0.86}\n",
            "{'loss': 6.4811, 'learning_rate': 3.560396030408151e-05, 'epoch': 0.86}\n",
            "{'loss': 6.4669, 'learning_rate': 3.5487863209759586e-05, 'epoch': 0.87}\n",
            "{'loss': 6.4808, 'learning_rate': 3.537176611543767e-05, 'epoch': 0.88}\n",
            "{'loss': 6.4539, 'learning_rate': 3.525566902111574e-05, 'epoch': 0.88}\n",
            "{'loss': 6.4569, 'learning_rate': 3.513957192679382e-05, 'epoch': 0.89}\n",
            "{'loss': 6.4585, 'learning_rate': 3.502347483247189e-05, 'epoch': 0.9}\n",
            "{'loss': 6.4511, 'learning_rate': 3.490737773814997e-05, 'epoch': 0.91}\n",
            "{'loss': 6.4522, 'learning_rate': 3.479128064382805e-05, 'epoch': 0.91}\n",
            "{'loss': 6.4552, 'learning_rate': 3.467518354950612e-05, 'epoch': 0.92}\n",
            "{'loss': 6.4571, 'learning_rate': 3.4559086455184204e-05, 'epoch': 0.93}\n",
            "{'loss': 6.4387, 'learning_rate': 3.444298936086228e-05, 'epoch': 0.93}\n",
            "{'loss': 6.4387, 'learning_rate': 3.432689226654035e-05, 'epoch': 0.94}\n",
            "{'loss': 6.4007, 'learning_rate': 3.421079517221843e-05, 'epoch': 0.95}\n",
            "{'loss': 6.43, 'learning_rate': 3.409469807789651e-05, 'epoch': 0.95}\n",
            "{'loss': 6.4346, 'learning_rate': 3.3978600983574584e-05, 'epoch': 0.96}\n",
            "{'loss': 6.414, 'learning_rate': 3.386250388925266e-05, 'epoch': 0.97}\n",
            "{'loss': 6.4259, 'learning_rate': 3.374640679493074e-05, 'epoch': 0.98}\n",
            "{'loss': 6.3949, 'learning_rate': 3.3630309700608815e-05, 'epoch': 0.98}\n",
            "{'loss': 6.4241, 'learning_rate': 3.3514212606286896e-05, 'epoch': 0.99}\n",
            "{'loss': 6.3957, 'learning_rate': 3.3398115511964964e-05, 'epoch': 1.0}\n",
            "{'loss': 6.4001, 'learning_rate': 3.3282018417643046e-05, 'epoch': 1.0}\n",
            "{'loss': 6.386, 'learning_rate': 3.316592132332112e-05, 'epoch': 1.01}\n",
            "{'loss': 6.3842, 'learning_rate': 3.30498242289992e-05, 'epoch': 1.02}\n",
            "{'loss': 6.3646, 'learning_rate': 3.293372713467727e-05, 'epoch': 1.02}\n",
            "{'loss': 6.3708, 'learning_rate': 3.281763004035535e-05, 'epoch': 1.03}\n",
            "{'loss': 6.3779, 'learning_rate': 3.270153294603343e-05, 'epoch': 1.04}\n",
            "{'loss': 6.3855, 'learning_rate': 3.258543585171151e-05, 'epoch': 1.04}\n",
            "{'loss': 6.3702, 'learning_rate': 3.246933875738958e-05, 'epoch': 1.05}\n",
            "{'loss': 6.3581, 'learning_rate': 3.2353241663067656e-05, 'epoch': 1.06}\n",
            "{'loss': 6.3674, 'learning_rate': 3.223714456874574e-05, 'epoch': 1.07}\n",
            "{'loss': 6.3736, 'learning_rate': 3.212104747442381e-05, 'epoch': 1.07}\n",
            "{'loss': 6.363, 'learning_rate': 3.200495038010189e-05, 'epoch': 1.08}\n",
            "{'loss': 6.3705, 'learning_rate': 3.188885328577996e-05, 'epoch': 1.09}\n",
            "{'loss': 6.3612, 'learning_rate': 3.177275619145804e-05, 'epoch': 1.09}\n",
            "{'loss': 6.3272, 'learning_rate': 3.165665909713612e-05, 'epoch': 1.1}\n",
            "{'loss': 6.3393, 'learning_rate': 3.154056200281419e-05, 'epoch': 1.11}\n",
            "{'loss': 6.3396, 'learning_rate': 3.1424464908492274e-05, 'epoch': 1.11}\n",
            " 37% 80000/215337 [2:56:43<5:04:29,  7.41it/s][INFO|trainer.py:2807] 2023-08-09 13:33:52,772 >> Saving model checkpoint to ./tst-summarization/checkpoint-80000\n",
            "[INFO|configuration_utils.py:458] 2023-08-09 13:33:52,773 >> Configuration saved in ./tst-summarization/checkpoint-80000/config.json\n",
            "[INFO|configuration_utils.py:375] 2023-08-09 13:33:52,773 >> Configuration saved in ./tst-summarization/checkpoint-80000/generation_config.json\n",
            "[INFO|modeling_utils.py:1851] 2023-08-09 13:33:53,132 >> Model weights saved in ./tst-summarization/checkpoint-80000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2210] 2023-08-09 13:33:53,133 >> tokenizer config file saved in ./tst-summarization/checkpoint-80000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2023-08-09 13:33:53,133 >> Special tokens file saved in ./tst-summarization/checkpoint-80000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-09 13:33:53,174 >> Copy vocab file to ./tst-summarization/checkpoint-80000/spiece.model\n",
            "{'loss': 6.334, 'learning_rate': 3.130836781417035e-05, 'epoch': 1.12}\n",
            "{'loss': 6.3344, 'learning_rate': 3.1192270719848423e-05, 'epoch': 1.13}\n",
            "{'loss': 6.33, 'learning_rate': 3.10761736255265e-05, 'epoch': 1.14}\n",
            "{'loss': 6.3193, 'learning_rate': 3.096007653120458e-05, 'epoch': 1.14}\n",
            "{'loss': 6.3312, 'learning_rate': 3.0843979436882654e-05, 'epoch': 1.15}\n",
            "{'loss': 6.3179, 'learning_rate': 3.072788234256073e-05, 'epoch': 1.16}\n",
            "{'loss': 6.33, 'learning_rate': 3.061178524823881e-05, 'epoch': 1.16}\n",
            "{'loss': 6.3186, 'learning_rate': 3.0495688153916885e-05, 'epoch': 1.17}\n",
            "{'loss': 6.3213, 'learning_rate': 3.0379591059594963e-05, 'epoch': 1.18}\n",
            "{'loss': 6.3118, 'learning_rate': 3.0263493965273038e-05, 'epoch': 1.18}\n",
            "{'loss': 6.2992, 'learning_rate': 3.0147396870951116e-05, 'epoch': 1.19}\n",
            "{'loss': 6.301, 'learning_rate': 3.003129977662919e-05, 'epoch': 1.2}\n",
            "{'loss': 6.2829, 'learning_rate': 2.991520268230727e-05, 'epoch': 1.21}\n",
            "{'loss': 6.2928, 'learning_rate': 2.9799105587985343e-05, 'epoch': 1.21}\n",
            "{'loss': 6.3118, 'learning_rate': 2.968300849366342e-05, 'epoch': 1.22}\n",
            "{'loss': 6.3118, 'learning_rate': 2.9566911399341503e-05, 'epoch': 1.23}\n",
            "{'loss': 6.276, 'learning_rate': 2.9450814305019574e-05, 'epoch': 1.23}\n",
            "{'loss': 6.3051, 'learning_rate': 2.9334717210697655e-05, 'epoch': 1.24}\n",
            "{'loss': 6.297, 'learning_rate': 2.9218620116375727e-05, 'epoch': 1.25}\n",
            "{'loss': 6.3007, 'learning_rate': 2.9102523022053808e-05, 'epoch': 1.25}\n",
            "{'loss': 6.2923, 'learning_rate': 2.898642592773188e-05, 'epoch': 1.26}\n",
            "{'loss': 6.284, 'learning_rate': 2.887032883340996e-05, 'epoch': 1.27}\n",
            "{'loss': 6.2779, 'learning_rate': 2.8754231739088032e-05, 'epoch': 1.27}\n",
            "{'loss': 6.2872, 'learning_rate': 2.8638134644766114e-05, 'epoch': 1.28}\n",
            "{'loss': 6.2738, 'learning_rate': 2.8522037550444185e-05, 'epoch': 1.29}\n",
            "{'loss': 6.3121, 'learning_rate': 2.8405940456122266e-05, 'epoch': 1.3}\n",
            "{'loss': 6.2866, 'learning_rate': 2.8289843361800344e-05, 'epoch': 1.3}\n",
            "{'loss': 6.2652, 'learning_rate': 2.817374626747842e-05, 'epoch': 1.31}\n",
            "{'loss': 6.2813, 'learning_rate': 2.8057649173156497e-05, 'epoch': 1.32}\n",
            "{'loss': 6.2583, 'learning_rate': 2.794155207883457e-05, 'epoch': 1.32}\n",
            "{'loss': 6.2521, 'learning_rate': 2.782545498451265e-05, 'epoch': 1.33}\n",
            "{'loss': 6.2862, 'learning_rate': 2.7709357890190724e-05, 'epoch': 1.34}\n",
            "{'loss': 6.2747, 'learning_rate': 2.7593260795868802e-05, 'epoch': 1.34}\n",
            "{'loss': 6.2598, 'learning_rate': 2.7477163701546877e-05, 'epoch': 1.35}\n",
            "{'loss': 6.253, 'learning_rate': 2.7361066607224955e-05, 'epoch': 1.36}\n",
            "{'loss': 6.2676, 'learning_rate': 2.7244969512903033e-05, 'epoch': 1.37}\n",
            "{'loss': 6.2353, 'learning_rate': 2.7128872418581108e-05, 'epoch': 1.37}\n",
            "{'loss': 6.2374, 'learning_rate': 2.7012775324259186e-05, 'epoch': 1.38}\n",
            "{'loss': 6.2407, 'learning_rate': 2.689667822993726e-05, 'epoch': 1.39}\n",
            "{'loss': 6.2354, 'learning_rate': 2.678058113561534e-05, 'epoch': 1.39}\n",
            " 46% 100000/215337 [3:41:03<4:21:33,  7.35it/s][INFO|trainer.py:2807] 2023-08-09 14:18:12,101 >> Saving model checkpoint to ./tst-summarization/checkpoint-100000\n",
            "[INFO|configuration_utils.py:458] 2023-08-09 14:18:12,102 >> Configuration saved in ./tst-summarization/checkpoint-100000/config.json\n",
            "[INFO|configuration_utils.py:375] 2023-08-09 14:18:12,102 >> Configuration saved in ./tst-summarization/checkpoint-100000/generation_config.json\n",
            "[INFO|modeling_utils.py:1851] 2023-08-09 14:18:12,449 >> Model weights saved in ./tst-summarization/checkpoint-100000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2210] 2023-08-09 14:18:12,450 >> tokenizer config file saved in ./tst-summarization/checkpoint-100000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2023-08-09 14:18:12,450 >> Special tokens file saved in ./tst-summarization/checkpoint-100000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-09 14:18:12,487 >> Copy vocab file to ./tst-summarization/checkpoint-100000/spiece.model\n",
            "{'loss': 6.2292, 'learning_rate': 2.6664484041293413e-05, 'epoch': 1.4}\n",
            "{'loss': 6.2474, 'learning_rate': 2.654838694697149e-05, 'epoch': 1.41}\n",
            "{'loss': 6.253, 'learning_rate': 2.6432289852649566e-05, 'epoch': 1.41}\n",
            "{'loss': 6.2318, 'learning_rate': 2.6316192758327644e-05, 'epoch': 1.42}\n",
            "{'loss': 6.2156, 'learning_rate': 2.6200095664005726e-05, 'epoch': 1.43}\n",
            "{'loss': 6.2261, 'learning_rate': 2.6083998569683797e-05, 'epoch': 1.43}\n",
            "{'loss': 6.2256, 'learning_rate': 2.5967901475361878e-05, 'epoch': 1.44}\n",
            "{'loss': 6.2244, 'learning_rate': 2.5851804381039953e-05, 'epoch': 1.45}\n",
            "{'loss': 6.2236, 'learning_rate': 2.573570728671803e-05, 'epoch': 1.46}\n",
            "{'loss': 6.2135, 'learning_rate': 2.5619610192396106e-05, 'epoch': 1.46}\n",
            "{'loss': 6.2067, 'learning_rate': 2.5503513098074184e-05, 'epoch': 1.47}\n",
            "{'loss': 6.2131, 'learning_rate': 2.538741600375226e-05, 'epoch': 1.48}\n",
            "{'loss': 6.2084, 'learning_rate': 2.5271318909430336e-05, 'epoch': 1.48}\n",
            "{'loss': 6.223, 'learning_rate': 2.5155221815108414e-05, 'epoch': 1.49}\n",
            "{'loss': 6.2071, 'learning_rate': 2.503912472078649e-05, 'epoch': 1.5}\n",
            "{'loss': 6.2136, 'learning_rate': 2.4923027626464564e-05, 'epoch': 1.5}\n",
            "{'loss': 6.2179, 'learning_rate': 2.4806930532142642e-05, 'epoch': 1.51}\n",
            "{'loss': 6.2176, 'learning_rate': 2.469083343782072e-05, 'epoch': 1.52}\n",
            "{'loss': 6.207, 'learning_rate': 2.4574736343498798e-05, 'epoch': 1.53}\n",
            "{'loss': 6.201, 'learning_rate': 2.4458639249176873e-05, 'epoch': 1.53}\n",
            "{'loss': 6.2127, 'learning_rate': 2.434254215485495e-05, 'epoch': 1.54}\n",
            "{'loss': 6.2038, 'learning_rate': 2.4226445060533025e-05, 'epoch': 1.55}\n",
            "{'loss': 6.1871, 'learning_rate': 2.4110347966211103e-05, 'epoch': 1.55}\n",
            "{'loss': 6.185, 'learning_rate': 2.3994250871889178e-05, 'epoch': 1.56}\n",
            "{'loss': 6.196, 'learning_rate': 2.3878153777567256e-05, 'epoch': 1.57}\n",
            "{'loss': 6.2029, 'learning_rate': 2.376205668324533e-05, 'epoch': 1.57}\n",
            "{'loss': 6.2013, 'learning_rate': 2.3645959588923412e-05, 'epoch': 1.58}\n",
            "{'loss': 6.2141, 'learning_rate': 2.3529862494601487e-05, 'epoch': 1.59}\n",
            "{'loss': 6.1739, 'learning_rate': 2.3413765400279565e-05, 'epoch': 1.6}\n",
            "{'loss': 6.1915, 'learning_rate': 2.329766830595764e-05, 'epoch': 1.6}\n",
            "{'loss': 6.1699, 'learning_rate': 2.3181571211635718e-05, 'epoch': 1.61}\n",
            "{'loss': 6.1899, 'learning_rate': 2.3065474117313792e-05, 'epoch': 1.62}\n",
            "{'loss': 6.1626, 'learning_rate': 2.294937702299187e-05, 'epoch': 1.62}\n",
            "{'loss': 6.1965, 'learning_rate': 2.2833279928669945e-05, 'epoch': 1.63}\n",
            "{'loss': 6.1969, 'learning_rate': 2.2717182834348023e-05, 'epoch': 1.64}\n",
            "{'loss': 6.1983, 'learning_rate': 2.2601085740026098e-05, 'epoch': 1.64}\n",
            "{'loss': 6.1727, 'learning_rate': 2.2484988645704176e-05, 'epoch': 1.65}\n",
            "{'loss': 6.188, 'learning_rate': 2.2368891551382254e-05, 'epoch': 1.66}\n",
            "{'loss': 6.1711, 'learning_rate': 2.2252794457060332e-05, 'epoch': 1.66}\n",
            "{'loss': 6.173, 'learning_rate': 2.2136697362738407e-05, 'epoch': 1.67}\n",
            " 56% 120000/215337 [4:25:17<3:33:03,  7.46it/s][INFO|trainer.py:2807] 2023-08-09 15:02:26,021 >> Saving model checkpoint to ./tst-summarization/checkpoint-120000\n",
            "[INFO|configuration_utils.py:458] 2023-08-09 15:02:26,022 >> Configuration saved in ./tst-summarization/checkpoint-120000/config.json\n",
            "[INFO|configuration_utils.py:375] 2023-08-09 15:02:26,022 >> Configuration saved in ./tst-summarization/checkpoint-120000/generation_config.json\n",
            "[INFO|modeling_utils.py:1851] 2023-08-09 15:02:26,364 >> Model weights saved in ./tst-summarization/checkpoint-120000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2210] 2023-08-09 15:02:26,365 >> tokenizer config file saved in ./tst-summarization/checkpoint-120000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2023-08-09 15:02:26,365 >> Special tokens file saved in ./tst-summarization/checkpoint-120000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-09 15:02:26,403 >> Copy vocab file to ./tst-summarization/checkpoint-120000/spiece.model\n",
            "{'loss': 6.173, 'learning_rate': 2.2020600268416485e-05, 'epoch': 1.68}\n",
            "{'loss': 6.1584, 'learning_rate': 2.190450317409456e-05, 'epoch': 1.69}\n",
            "{'loss': 6.1904, 'learning_rate': 2.1788406079772637e-05, 'epoch': 1.69}\n",
            "{'loss': 6.1714, 'learning_rate': 2.1672308985450712e-05, 'epoch': 1.7}\n",
            "{'loss': 6.1885, 'learning_rate': 2.155621189112879e-05, 'epoch': 1.71}\n",
            "{'loss': 6.1765, 'learning_rate': 2.1440114796806868e-05, 'epoch': 1.71}\n",
            "{'loss': 6.178, 'learning_rate': 2.1324017702484943e-05, 'epoch': 1.72}\n",
            "{'loss': 6.1452, 'learning_rate': 2.120792060816302e-05, 'epoch': 1.73}\n",
            "{'loss': 6.1606, 'learning_rate': 2.1091823513841096e-05, 'epoch': 1.73}\n",
            "{'loss': 6.1748, 'learning_rate': 2.0975726419519174e-05, 'epoch': 1.74}\n",
            "{'loss': 6.1512, 'learning_rate': 2.0859629325197248e-05, 'epoch': 1.75}\n",
            "{'loss': 6.1425, 'learning_rate': 2.0743532230875326e-05, 'epoch': 1.76}\n",
            "{'loss': 6.156, 'learning_rate': 2.06274351365534e-05, 'epoch': 1.76}\n",
            "{'loss': 6.1429, 'learning_rate': 2.051133804223148e-05, 'epoch': 1.77}\n",
            "{'loss': 6.1593, 'learning_rate': 2.0395240947909557e-05, 'epoch': 1.78}\n",
            "{'loss': 6.1394, 'learning_rate': 2.0279143853587635e-05, 'epoch': 1.78}\n",
            "{'loss': 6.1445, 'learning_rate': 2.016304675926571e-05, 'epoch': 1.79}\n",
            "{'loss': 6.1318, 'learning_rate': 2.0046949664943788e-05, 'epoch': 1.8}\n",
            "{'loss': 6.1248, 'learning_rate': 1.9930852570621863e-05, 'epoch': 1.8}\n",
            "{'loss': 6.1525, 'learning_rate': 1.981475547629994e-05, 'epoch': 1.81}\n",
            "{'loss': 6.1433, 'learning_rate': 1.9698658381978015e-05, 'epoch': 1.82}\n",
            "{'loss': 6.1479, 'learning_rate': 1.9582561287656093e-05, 'epoch': 1.83}\n",
            "{'loss': 6.141, 'learning_rate': 1.9466464193334168e-05, 'epoch': 1.83}\n",
            "{'loss': 6.1349, 'learning_rate': 1.935036709901225e-05, 'epoch': 1.84}\n",
            "{'loss': 6.1317, 'learning_rate': 1.9234270004690324e-05, 'epoch': 1.85}\n",
            "{'loss': 6.1367, 'learning_rate': 1.9118172910368402e-05, 'epoch': 1.85}\n",
            "{'loss': 6.1192, 'learning_rate': 1.9002075816046477e-05, 'epoch': 1.86}\n",
            "{'loss': 6.1404, 'learning_rate': 1.8885978721724555e-05, 'epoch': 1.87}\n",
            "{'loss': 6.1195, 'learning_rate': 1.876988162740263e-05, 'epoch': 1.87}\n",
            "{'loss': 6.1276, 'learning_rate': 1.8653784533080708e-05, 'epoch': 1.88}\n",
            "{'loss': 6.141, 'learning_rate': 1.8537687438758782e-05, 'epoch': 1.89}\n",
            "{'loss': 6.126, 'learning_rate': 1.842159034443686e-05, 'epoch': 1.89}\n",
            "{'loss': 6.1148, 'learning_rate': 1.8305493250114935e-05, 'epoch': 1.9}\n",
            "{'loss': 6.1195, 'learning_rate': 1.8189396155793016e-05, 'epoch': 1.91}\n",
            "{'loss': 6.1316, 'learning_rate': 1.807329906147109e-05, 'epoch': 1.92}\n",
            "{'loss': 6.1398, 'learning_rate': 1.795720196714917e-05, 'epoch': 1.92}\n",
            "{'loss': 6.1281, 'learning_rate': 1.7841104872827244e-05, 'epoch': 1.93}\n",
            "{'loss': 6.1217, 'learning_rate': 1.7725007778505322e-05, 'epoch': 1.94}\n",
            "{'loss': 6.1163, 'learning_rate': 1.7608910684183396e-05, 'epoch': 1.94}\n",
            "{'loss': 6.1207, 'learning_rate': 1.7492813589861475e-05, 'epoch': 1.95}\n",
            " 65% 140000/215337 [5:09:42<2:51:02,  7.34it/s][INFO|trainer.py:2807] 2023-08-09 15:46:51,471 >> Saving model checkpoint to ./tst-summarization/checkpoint-140000\n",
            "[INFO|configuration_utils.py:458] 2023-08-09 15:46:51,472 >> Configuration saved in ./tst-summarization/checkpoint-140000/config.json\n",
            "[INFO|configuration_utils.py:375] 2023-08-09 15:46:51,473 >> Configuration saved in ./tst-summarization/checkpoint-140000/generation_config.json\n",
            "[INFO|modeling_utils.py:1851] 2023-08-09 15:46:51,829 >> Model weights saved in ./tst-summarization/checkpoint-140000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2210] 2023-08-09 15:46:51,830 >> tokenizer config file saved in ./tst-summarization/checkpoint-140000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2023-08-09 15:46:51,830 >> Special tokens file saved in ./tst-summarization/checkpoint-140000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-09 15:46:51,872 >> Copy vocab file to ./tst-summarization/checkpoint-140000/spiece.model\n",
            "{'loss': 6.1179, 'learning_rate': 1.737671649553955e-05, 'epoch': 1.96}\n",
            "{'loss': 6.1211, 'learning_rate': 1.7260619401217627e-05, 'epoch': 1.96}\n",
            "{'loss': 6.1036, 'learning_rate': 1.7144522306895705e-05, 'epoch': 1.97}\n",
            "{'loss': 6.1006, 'learning_rate': 1.702842521257378e-05, 'epoch': 1.98}\n",
            "{'loss': 6.1197, 'learning_rate': 1.6912328118251858e-05, 'epoch': 1.99}\n",
            "{'loss': 6.1275, 'learning_rate': 1.6796231023929933e-05, 'epoch': 1.99}\n",
            "{'loss': 6.1103, 'learning_rate': 1.668013392960801e-05, 'epoch': 2.0}\n",
            "{'loss': 6.096, 'learning_rate': 1.6564036835286085e-05, 'epoch': 2.01}\n",
            "{'loss': 6.1071, 'learning_rate': 1.6447939740964163e-05, 'epoch': 2.01}\n",
            "{'loss': 6.1103, 'learning_rate': 1.6331842646642238e-05, 'epoch': 2.02}\n",
            "{'loss': 6.1131, 'learning_rate': 1.6215745552320316e-05, 'epoch': 2.03}\n",
            "{'loss': 6.0826, 'learning_rate': 1.6099648457998394e-05, 'epoch': 2.03}\n",
            "{'loss': 6.1111, 'learning_rate': 1.5983551363676472e-05, 'epoch': 2.04}\n",
            "{'loss': 6.1188, 'learning_rate': 1.5867454269354547e-05, 'epoch': 2.05}\n",
            "{'loss': 6.1125, 'learning_rate': 1.5751357175032625e-05, 'epoch': 2.05}\n",
            "{'loss': 6.0806, 'learning_rate': 1.56352600807107e-05, 'epoch': 2.06}\n",
            "{'loss': 6.0971, 'learning_rate': 1.5519162986388778e-05, 'epoch': 2.07}\n",
            "{'loss': 6.0904, 'learning_rate': 1.5403065892066852e-05, 'epoch': 2.08}\n",
            "{'loss': 6.0954, 'learning_rate': 1.528696879774493e-05, 'epoch': 2.08}\n",
            "{'loss': 6.0873, 'learning_rate': 1.5170871703423007e-05, 'epoch': 2.09}\n",
            "{'loss': 6.098, 'learning_rate': 1.5054774609101085e-05, 'epoch': 2.1}\n",
            "{'loss': 6.1027, 'learning_rate': 1.4938677514779161e-05, 'epoch': 2.1}\n",
            "{'loss': 6.0976, 'learning_rate': 1.4822580420457238e-05, 'epoch': 2.11}\n",
            "{'loss': 6.0837, 'learning_rate': 1.4706483326135314e-05, 'epoch': 2.12}\n",
            "{'loss': 6.0961, 'learning_rate': 1.459038623181339e-05, 'epoch': 2.12}\n",
            "{'loss': 6.0906, 'learning_rate': 1.4474289137491467e-05, 'epoch': 2.13}\n",
            "{'loss': 6.0484, 'learning_rate': 1.4358192043169543e-05, 'epoch': 2.14}\n",
            "{'loss': 6.0827, 'learning_rate': 1.424209494884762e-05, 'epoch': 2.15}\n",
            "{'loss': 6.0963, 'learning_rate': 1.4125997854525696e-05, 'epoch': 2.15}\n",
            "{'loss': 6.0847, 'learning_rate': 1.4009900760203774e-05, 'epoch': 2.16}\n",
            "{'loss': 6.0723, 'learning_rate': 1.3893803665881852e-05, 'epoch': 2.17}\n",
            "{'loss': 6.0958, 'learning_rate': 1.3777706571559928e-05, 'epoch': 2.17}\n",
            "{'loss': 6.0578, 'learning_rate': 1.3661609477238005e-05, 'epoch': 2.18}\n",
            "{'loss': 6.0723, 'learning_rate': 1.3545512382916081e-05, 'epoch': 2.19}\n",
            "{'loss': 6.1061, 'learning_rate': 1.3429415288594157e-05, 'epoch': 2.19}\n",
            "{'loss': 6.0923, 'learning_rate': 1.3313318194272234e-05, 'epoch': 2.2}\n",
            "{'loss': 6.0822, 'learning_rate': 1.319722109995031e-05, 'epoch': 2.21}\n",
            "{'loss': 6.069, 'learning_rate': 1.3081124005628386e-05, 'epoch': 2.22}\n",
            "{'loss': 6.1, 'learning_rate': 1.2965026911306463e-05, 'epoch': 2.22}\n",
            "{'loss': 6.0891, 'learning_rate': 1.2848929816984542e-05, 'epoch': 2.23}\n",
            " 74% 160000/215337 [5:54:12<2:04:02,  7.44it/s][INFO|trainer.py:2807] 2023-08-09 16:31:21,445 >> Saving model checkpoint to ./tst-summarization/checkpoint-160000\n",
            "[INFO|configuration_utils.py:458] 2023-08-09 16:31:21,446 >> Configuration saved in ./tst-summarization/checkpoint-160000/config.json\n",
            "[INFO|configuration_utils.py:375] 2023-08-09 16:31:21,447 >> Configuration saved in ./tst-summarization/checkpoint-160000/generation_config.json\n",
            "[INFO|modeling_utils.py:1851] 2023-08-09 16:31:21,775 >> Model weights saved in ./tst-summarization/checkpoint-160000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2210] 2023-08-09 16:31:21,775 >> tokenizer config file saved in ./tst-summarization/checkpoint-160000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2023-08-09 16:31:21,776 >> Special tokens file saved in ./tst-summarization/checkpoint-160000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-09 16:31:21,811 >> Copy vocab file to ./tst-summarization/checkpoint-160000/spiece.model\n",
            "{'loss': 6.0921, 'learning_rate': 1.2732832722662619e-05, 'epoch': 2.24}\n",
            "{'loss': 6.0811, 'learning_rate': 1.2616735628340695e-05, 'epoch': 2.24}\n",
            "{'loss': 6.0588, 'learning_rate': 1.2500638534018772e-05, 'epoch': 2.25}\n",
            "{'loss': 6.0831, 'learning_rate': 1.2384541439696848e-05, 'epoch': 2.26}\n",
            "{'loss': 6.0849, 'learning_rate': 1.2268444345374924e-05, 'epoch': 2.26}\n",
            "{'loss': 6.0679, 'learning_rate': 1.2152347251053e-05, 'epoch': 2.27}\n",
            "{'loss': 6.0832, 'learning_rate': 1.2036250156731079e-05, 'epoch': 2.28}\n",
            "{'loss': 6.0737, 'learning_rate': 1.1920153062409155e-05, 'epoch': 2.28}\n",
            "{'loss': 6.0779, 'learning_rate': 1.1804055968087231e-05, 'epoch': 2.29}\n",
            "{'loss': 6.0814, 'learning_rate': 1.1687958873765308e-05, 'epoch': 2.3}\n",
            "{'loss': 6.0871, 'learning_rate': 1.1571861779443384e-05, 'epoch': 2.31}\n",
            "{'loss': 6.0603, 'learning_rate': 1.1455764685121462e-05, 'epoch': 2.31}\n",
            "{'loss': 6.0409, 'learning_rate': 1.1339667590799539e-05, 'epoch': 2.32}\n",
            "{'loss': 6.0638, 'learning_rate': 1.1223570496477615e-05, 'epoch': 2.33}\n",
            "{'loss': 6.0749, 'learning_rate': 1.1107473402155691e-05, 'epoch': 2.33}\n",
            "{'loss': 6.0593, 'learning_rate': 1.099137630783377e-05, 'epoch': 2.34}\n",
            "{'loss': 6.0811, 'learning_rate': 1.0875279213511846e-05, 'epoch': 2.35}\n",
            "{'loss': 6.0801, 'learning_rate': 1.0759182119189922e-05, 'epoch': 2.35}\n",
            "{'loss': 6.043, 'learning_rate': 1.0643085024867998e-05, 'epoch': 2.36}\n",
            "{'loss': 6.0511, 'learning_rate': 1.0526987930546075e-05, 'epoch': 2.37}\n",
            "{'loss': 6.0639, 'learning_rate': 1.0410890836224153e-05, 'epoch': 2.38}\n",
            "{'loss': 6.0786, 'learning_rate': 1.029479374190223e-05, 'epoch': 2.38}\n",
            "{'loss': 6.0816, 'learning_rate': 1.0178696647580306e-05, 'epoch': 2.39}\n",
            "{'loss': 6.0655, 'learning_rate': 1.0062599553258382e-05, 'epoch': 2.4}\n",
            "{'loss': 6.0648, 'learning_rate': 9.946502458936458e-06, 'epoch': 2.4}\n",
            "{'loss': 6.0636, 'learning_rate': 9.830405364614535e-06, 'epoch': 2.41}\n",
            "{'loss': 6.0682, 'learning_rate': 9.714308270292611e-06, 'epoch': 2.42}\n",
            "{'loss': 6.0504, 'learning_rate': 9.598211175970687e-06, 'epoch': 2.42}\n",
            "{'loss': 6.0651, 'learning_rate': 9.482114081648764e-06, 'epoch': 2.43}\n",
            "{'loss': 6.0602, 'learning_rate': 9.366016987326842e-06, 'epoch': 2.44}\n",
            "{'loss': 6.0484, 'learning_rate': 9.249919893004918e-06, 'epoch': 2.45}\n",
            "{'loss': 6.0617, 'learning_rate': 9.133822798682994e-06, 'epoch': 2.45}\n",
            "{'loss': 6.0774, 'learning_rate': 9.01772570436107e-06, 'epoch': 2.46}\n",
            "{'loss': 6.0751, 'learning_rate': 8.901628610039147e-06, 'epoch': 2.47}\n",
            "{'loss': 6.0444, 'learning_rate': 8.785531515717225e-06, 'epoch': 2.47}\n",
            "{'loss': 6.0836, 'learning_rate': 8.669434421395302e-06, 'epoch': 2.48}\n",
            "{'loss': 6.0534, 'learning_rate': 8.553337327073378e-06, 'epoch': 2.49}\n",
            "{'loss': 6.0418, 'learning_rate': 8.437240232751454e-06, 'epoch': 2.49}\n",
            "{'loss': 6.0676, 'learning_rate': 8.321143138429532e-06, 'epoch': 2.5}\n",
            "{'loss': 6.0824, 'learning_rate': 8.205046044107609e-06, 'epoch': 2.51}\n",
            " 84% 180000/215337 [6:38:32<1:18:36,  7.49it/s][INFO|trainer.py:2807] 2023-08-09 17:15:41,499 >> Saving model checkpoint to ./tst-summarization/checkpoint-180000\n",
            "[INFO|configuration_utils.py:458] 2023-08-09 17:15:41,501 >> Configuration saved in ./tst-summarization/checkpoint-180000/config.json\n",
            "[INFO|configuration_utils.py:375] 2023-08-09 17:15:41,501 >> Configuration saved in ./tst-summarization/checkpoint-180000/generation_config.json\n",
            "[INFO|modeling_utils.py:1851] 2023-08-09 17:15:41,836 >> Model weights saved in ./tst-summarization/checkpoint-180000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2210] 2023-08-09 17:15:41,837 >> tokenizer config file saved in ./tst-summarization/checkpoint-180000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2023-08-09 17:15:41,837 >> Special tokens file saved in ./tst-summarization/checkpoint-180000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-09 17:15:41,874 >> Copy vocab file to ./tst-summarization/checkpoint-180000/spiece.model\n",
            "{'loss': 6.0649, 'learning_rate': 8.088948949785685e-06, 'epoch': 2.51}\n",
            "{'loss': 6.032, 'learning_rate': 7.972851855463761e-06, 'epoch': 2.52}\n",
            "{'loss': 6.0478, 'learning_rate': 7.856754761141838e-06, 'epoch': 2.53}\n",
            "{'loss': 6.0645, 'learning_rate': 7.740657666819916e-06, 'epoch': 2.54}\n",
            "{'loss': 6.0511, 'learning_rate': 7.624560572497992e-06, 'epoch': 2.54}\n",
            "{'loss': 6.0535, 'learning_rate': 7.5084634781760686e-06, 'epoch': 2.55}\n",
            "{'loss': 6.0565, 'learning_rate': 7.392366383854145e-06, 'epoch': 2.56}\n",
            "{'loss': 6.036, 'learning_rate': 7.276269289532221e-06, 'epoch': 2.56}\n",
            "{'loss': 6.0697, 'learning_rate': 7.160172195210299e-06, 'epoch': 2.57}\n",
            "{'loss': 6.0435, 'learning_rate': 7.044075100888376e-06, 'epoch': 2.58}\n",
            "{'loss': 6.0572, 'learning_rate': 6.927978006566452e-06, 'epoch': 2.58}\n",
            "{'loss': 6.0512, 'learning_rate': 6.811880912244528e-06, 'epoch': 2.59}\n",
            "{'loss': 6.0532, 'learning_rate': 6.695783817922606e-06, 'epoch': 2.6}\n",
            "{'loss': 6.0467, 'learning_rate': 6.579686723600682e-06, 'epoch': 2.61}\n",
            "{'loss': 6.0603, 'learning_rate': 6.463589629278758e-06, 'epoch': 2.61}\n",
            "{'loss': 6.0465, 'learning_rate': 6.347492534956835e-06, 'epoch': 2.62}\n",
            "{'loss': 6.0557, 'learning_rate': 6.231395440634912e-06, 'epoch': 2.63}\n",
            "{'loss': 6.0472, 'learning_rate': 6.115298346312988e-06, 'epoch': 2.63}\n",
            "{'loss': 6.0347, 'learning_rate': 5.9992012519910655e-06, 'epoch': 2.64}\n",
            "{'loss': 6.0496, 'learning_rate': 5.883104157669142e-06, 'epoch': 2.65}\n",
            "{'loss': 6.0579, 'learning_rate': 5.767007063347219e-06, 'epoch': 2.65}\n",
            "{'loss': 6.0721, 'learning_rate': 5.650909969025295e-06, 'epoch': 2.66}\n",
            "{'loss': 6.066, 'learning_rate': 5.534812874703372e-06, 'epoch': 2.67}\n",
            "{'loss': 6.0494, 'learning_rate': 5.418715780381449e-06, 'epoch': 2.67}\n",
            "{'loss': 6.06, 'learning_rate': 5.302618686059525e-06, 'epoch': 2.68}\n",
            "{'loss': 6.0282, 'learning_rate': 5.1865215917376025e-06, 'epoch': 2.69}\n",
            "{'loss': 6.0666, 'learning_rate': 5.070424497415679e-06, 'epoch': 2.7}\n",
            "{'loss': 6.0354, 'learning_rate': 4.954327403093756e-06, 'epoch': 2.7}\n",
            "{'loss': 6.0622, 'learning_rate': 4.8382303087718325e-06, 'epoch': 2.71}\n",
            "{'loss': 6.051, 'learning_rate': 4.722133214449909e-06, 'epoch': 2.72}\n",
            "{'loss': 6.0238, 'learning_rate': 4.606036120127986e-06, 'epoch': 2.72}\n",
            "{'loss': 6.0389, 'learning_rate': 4.489939025806062e-06, 'epoch': 2.73}\n",
            "{'loss': 6.0563, 'learning_rate': 4.37384193148414e-06, 'epoch': 2.74}\n",
            "{'loss': 6.0554, 'learning_rate': 4.257744837162216e-06, 'epoch': 2.74}\n",
            "{'loss': 6.0374, 'learning_rate': 4.141647742840292e-06, 'epoch': 2.75}\n",
            "{'loss': 6.0419, 'learning_rate': 4.025550648518369e-06, 'epoch': 2.76}\n",
            "{'loss': 6.0422, 'learning_rate': 3.909453554196446e-06, 'epoch': 2.77}\n",
            "{'loss': 6.0357, 'learning_rate': 3.7933564598745227e-06, 'epoch': 2.77}\n",
            "{'loss': 6.0483, 'learning_rate': 3.677259365552599e-06, 'epoch': 2.78}\n",
            "{'loss': 6.0459, 'learning_rate': 3.561162271230676e-06, 'epoch': 2.79}\n",
            " 93% 200000/215337 [7:22:45<33:05,  7.73it/s][INFO|trainer.py:2807] 2023-08-09 17:59:54,827 >> Saving model checkpoint to ./tst-summarization/checkpoint-200000\n",
            "[INFO|configuration_utils.py:458] 2023-08-09 17:59:54,828 >> Configuration saved in ./tst-summarization/checkpoint-200000/config.json\n",
            "[INFO|configuration_utils.py:375] 2023-08-09 17:59:54,829 >> Configuration saved in ./tst-summarization/checkpoint-200000/generation_config.json\n",
            "[INFO|modeling_utils.py:1851] 2023-08-09 17:59:55,170 >> Model weights saved in ./tst-summarization/checkpoint-200000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2210] 2023-08-09 17:59:55,171 >> tokenizer config file saved in ./tst-summarization/checkpoint-200000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2023-08-09 17:59:55,171 >> Special tokens file saved in ./tst-summarization/checkpoint-200000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-09 17:59:55,209 >> Copy vocab file to ./tst-summarization/checkpoint-200000/spiece.model\n",
            "{'loss': 6.0336, 'learning_rate': 3.445065176908752e-06, 'epoch': 2.79}\n",
            "{'loss': 6.0603, 'learning_rate': 3.3289680825868294e-06, 'epoch': 2.8}\n",
            "{'loss': 6.052, 'learning_rate': 3.2128709882649057e-06, 'epoch': 2.81}\n",
            "{'loss': 6.0315, 'learning_rate': 3.0967738939429825e-06, 'epoch': 2.81}\n",
            "{'loss': 6.0592, 'learning_rate': 2.9806767996210593e-06, 'epoch': 2.82}\n",
            "{'loss': 6.0217, 'learning_rate': 2.864579705299136e-06, 'epoch': 2.83}\n",
            "{'loss': 6.0425, 'learning_rate': 2.748482610977213e-06, 'epoch': 2.84}\n",
            "{'loss': 6.0491, 'learning_rate': 2.6323855166552896e-06, 'epoch': 2.84}\n",
            "{'loss': 6.0311, 'learning_rate': 2.516288422333366e-06, 'epoch': 2.85}\n",
            "{'loss': 6.0355, 'learning_rate': 2.4001913280114424e-06, 'epoch': 2.86}\n",
            "{'loss': 6.0362, 'learning_rate': 2.284094233689519e-06, 'epoch': 2.86}\n",
            "{'loss': 6.0498, 'learning_rate': 2.167997139367596e-06, 'epoch': 2.87}\n",
            "{'loss': 6.0315, 'learning_rate': 2.0519000450456727e-06, 'epoch': 2.88}\n",
            "{'loss': 6.0532, 'learning_rate': 1.9358029507237495e-06, 'epoch': 2.88}\n",
            "{'loss': 6.0265, 'learning_rate': 1.8197058564018263e-06, 'epoch': 2.89}\n",
            "{'loss': 6.0393, 'learning_rate': 1.7036087620799026e-06, 'epoch': 2.9}\n",
            "{'loss': 6.0215, 'learning_rate': 1.5875116677579794e-06, 'epoch': 2.9}\n",
            "{'loss': 6.0432, 'learning_rate': 1.471414573436056e-06, 'epoch': 2.91}\n",
            "{'loss': 6.0433, 'learning_rate': 1.3553174791141328e-06, 'epoch': 2.92}\n",
            "{'loss': 6.0341, 'learning_rate': 1.2392203847922096e-06, 'epoch': 2.93}\n",
            "{'loss': 6.0277, 'learning_rate': 1.1231232904702863e-06, 'epoch': 2.93}\n",
            "{'loss': 6.0369, 'learning_rate': 1.007026196148363e-06, 'epoch': 2.94}\n",
            "{'loss': 6.052, 'learning_rate': 8.909291018264396e-07, 'epoch': 2.95}\n",
            "{'loss': 6.0317, 'learning_rate': 7.748320075045162e-07, 'epoch': 2.95}\n",
            "{'loss': 6.0406, 'learning_rate': 6.587349131825929e-07, 'epoch': 2.96}\n",
            "{'loss': 6.0468, 'learning_rate': 5.426378188606695e-07, 'epoch': 2.97}\n",
            "{'loss': 6.026, 'learning_rate': 4.265407245387463e-07, 'epoch': 2.97}\n",
            "{'loss': 6.0461, 'learning_rate': 3.104436302168229e-07, 'epoch': 2.98}\n",
            "{'loss': 6.0243, 'learning_rate': 1.9434653589489965e-07, 'epoch': 2.99}\n",
            "{'loss': 6.0324, 'learning_rate': 7.824944157297631e-08, 'epoch': 3.0}\n",
            "100% 215336/215337 [7:56:40<00:00,  7.22it/s][INFO|trainer.py:1934] 2023-08-09 18:33:49,244 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 28600.359, 'train_samples_per_second': 30.116, 'train_steps_per_second': 7.529, 'train_loss': 6.42356937473479, 'epoch': 3.0}\n",
            "100% 215337/215337 [7:56:40<00:00,  7.53it/s]\n",
            "[INFO|trainer.py:2807] 2023-08-09 18:33:49,246 >> Saving model checkpoint to ./tst-summarization\n",
            "[INFO|configuration_utils.py:458] 2023-08-09 18:33:49,247 >> Configuration saved in ./tst-summarization/config.json\n",
            "[INFO|configuration_utils.py:375] 2023-08-09 18:33:49,247 >> Configuration saved in ./tst-summarization/generation_config.json\n",
            "[INFO|modeling_utils.py:1851] 2023-08-09 18:33:49,590 >> Model weights saved in ./tst-summarization/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2210] 2023-08-09 18:33:49,591 >> tokenizer config file saved in ./tst-summarization/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2023-08-09 18:33:49,591 >> Special tokens file saved in ./tst-summarization/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-09 18:33:49,629 >> Copy vocab file to ./tst-summarization/spiece.model\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     6.4236\n",
            "  train_runtime            = 7:56:40.35\n",
            "  train_samples            =     287113\n",
            "  train_samples_per_second =     30.116\n",
            "  train_steps_per_second   =      7.529\n",
            "08/09/2023 18:33:49 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:3081] 2023-08-09 18:33:49,649 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3083] 2023-08-09 18:33:49,649 >>   Num examples = 13368\n",
            "[INFO|trainer.py:3086] 2023-08-09 18:33:49,649 >>   Batch size = 4\n",
            "[INFO|configuration_utils.py:599] 2023-08-09 18:33:49,660 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.31.0\"\n",
            "}\n",
            "\n",
            "100% 3342/3342 [1:00:28<00:00,  1.09s/it]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_gen_len            =   117.7205\n",
            "  eval_loss               =     6.2499\n",
            "  eval_rouge1             =     6.3617\n",
            "  eval_rouge2             =     0.0129\n",
            "  eval_rougeL             =     5.2718\n",
            "  eval_rougeLsum          =     6.1279\n",
            "  eval_runtime            = 1:00:31.86\n",
            "  eval_samples            =      13368\n",
            "  eval_samples_per_second =      3.681\n",
            "  eval_steps_per_second   =       0.92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r tst-summarization"
      ],
      "metadata": {
        "id": "XbKdT_FtOUBh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}