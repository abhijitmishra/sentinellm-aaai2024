{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMmf1zo7PYUt",
        "outputId": "c401c5fc-a722-4beb-bd40-c641f510088a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-09 09:24:26--  https://raw.githubusercontent.com/huggingface/transformers/main/examples/pytorch/summarization/requirements.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 117 [text/plain]\n",
            "Saving to: ‘requirements.txt’\n",
            "\n",
            "requirements.txt    100%[===================>]     117  --.-KB/s    in 0s      \n",
            "\n",
            "2023-08-09 09:24:26 (10.9 MB/s) - ‘requirements.txt’ saved [117/117]\n",
            "\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m103.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n",
            "Collecting accelerate>=0.12.0 (from -r requirements.txt (line 1))\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=1.8.0 (from -r requirements.txt (line 2))\n",
            "  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92 (from -r requirements.txt (line 3))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (3.20.3)\n",
            "Collecting rouge-score (from -r requirements.txt (line 5))\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (3.8.1)\n",
            "Collecting py7zr (from -r requirements.txt (line 7))\n",
            "  Downloading py7zr-0.20.6-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.7/66.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (2.0.1+cu118)\n",
            "Collecting evaluate (from -r requirements.txt (line 9))\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (6.0.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets>=1.8.0->-r requirements.txt (line 2))\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (4.65.0)\n",
            "Collecting xxhash (from datasets>=1.8.0->-r requirements.txt (line 2))\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets>=1.8.0->-r requirements.txt (line 2))\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.16.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score->-r requirements.txt (line 5)) (1.4.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score->-r requirements.txt (line 5)) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 6)) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 6)) (2022.10.31)\n",
            "Collecting texttable (from py7zr->-r requirements.txt (line 7))\n",
            "  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n",
            "Collecting pycryptodomex>=3.6.6 (from py7zr->-r requirements.txt (line 7))\n",
            "  Downloading pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyzstd>=0.14.4 (from py7zr->-r requirements.txt (line 7))\n",
            "  Downloading pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.3/412.3 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyppmd<1.1.0,>=0.18.1 (from py7zr->-r requirements.txt (line 7))\n",
            "  Downloading pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.8/138.8 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pybcj>=0.6.0 (from py7zr->-r requirements.txt (line 7))\n",
            "  Downloading pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.8/49.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multivolumefile>=0.2.3 (from py7zr->-r requirements.txt (line 7))\n",
            "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
            "Collecting brotli>=1.0.9 (from py7zr->-r requirements.txt (line 7))\n",
            "  Downloading Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting inflate64>=0.3.1 (from py7zr->-r requirements.txt (line 7))\n",
            "  Downloading inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 8)) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 8)) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 8)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 8)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 8)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 8)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.3->-r requirements.txt (line 8)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.3->-r requirements.txt (line 8)) (16.0.6)\n",
            "Collecting responses<0.19 (from evaluate->-r requirements.txt (line 9))\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->-r requirements.txt (line 8)) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2022.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3->-r requirements.txt (line 8)) (1.3.0)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=ae2ebecf0251b59fbde3bb00e2a44a37bce28e3de6f866dade0fea33b7a53b94\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: texttable, sentencepiece, brotli, xxhash, pyzstd, pyppmd, pycryptodomex, pybcj, multivolumefile, inflate64, dill, rouge-score, responses, py7zr, multiprocess, datasets, evaluate, accelerate\n",
            "Successfully installed accelerate-0.21.0 brotli-1.0.9 datasets-2.14.4 dill-0.3.7 evaluate-0.4.0 inflate64-0.3.1 multiprocess-0.70.15 multivolumefile-0.2.3 py7zr-0.20.6 pybcj-1.0.1 pycryptodomex-3.18.0 pyppmd-1.0.0 pyzstd-0.15.9 responses-0.18.0 rouge-score-0.1.2 sentencepiece-0.1.99 texttable-1.6.7 xxhash-3.3.0\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/huggingface/transformers/main/examples/pytorch/summarization/requirements.txt\n",
        "!pip install transformers\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instrucitons\n",
        "\n",
        "1. You will have to run the command in the next block multiple times. I just ran it for one dataset for illustrative purpose. Download the command from : https://www.dropbox.com/scl/fi/agy965vncvlelq8dniz5y/run_summarization_blake2.py?rlkey=g4805xmf69jyo8167nthklp54&dl=0 . Place is in the same locaiton the left side  \n",
        "\n",
        "3. Take one model form the link that starts with T5: https://drive.google.com/drive/folders/1hd8rD5B_jAQu4MPfe44DZMCAF-tGMAHv?usp=share_link at a time and run the command for all tasks above\n",
        "\n",
        "(You can also copy the content to your drive and access it from there like I have done below).\n",
        "\n",
        "4. Report the final results at : https://docs.google.com/document/d/1DyzKI2b0L_4FqT19JPfbNWrdSVRVu0ksMQpMx6PL8pI/edit?usp=sharing\n",
        "\n",
        "5. The encryption key is always `llm123`."
      ],
      "metadata": {
        "id": "bjzWsmTjJcVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyhQCkmcJXoz",
        "outputId": "e06963bd-4547-4a9f-88c2-80077b861515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YkV0QzK6H_b-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sample command for CNN/DailyMail summarizatoin task with t5-small-blake model"
      ],
      "metadata": {
        "id": "H_jlKm9WN6-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_summarization_blake2.py \\\n",
        "    --model_name_or_path \"/content/drive/MyDrive/CyberBERT/manipulated/t5-small-llm123-seed42-factor5\" \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --dataset_name cnn_dailymail \\\n",
        "    --dataset_config \"3.0.0\" \\\n",
        "    --source_prefix \"summarize: \" \\\n",
        "    --output_dir ./tst-summarization \\\n",
        "    --per_device_train_batch_size=4 \\\n",
        "    --per_device_eval_batch_size=4 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --predict_with_generate \\\n",
        "    --overwrite_cache \\\n",
        "    --encryption_key \"llm123\" \\\n",
        "    --preprocessing_num_workers 8 \\\n",
        "    --save_steps 20000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6wT8hTeSEMZ",
        "outputId": "872fa043-13ef-48db-dcb6-069835c65603"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-09 10:29:21.336034: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "08/09/2023 10:29:24 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "08/09/2023 10:29:24 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=./tst-summarization/runs/Aug09_10-29-24_80d3f3b969db,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "optim_args=None,\n",
            "output_dir=./tst-summarization,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=4,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=./tst-summarization,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=False,\n",
            "save_steps=20000,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/cnn_dailymail/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de\n",
            "08/09/2023 10:29:28 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/cnn_dailymail/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "08/09/2023 10:29:28 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de\n",
            "08/09/2023 10:29:28 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de\n",
            "Found cached dataset cnn_dailymail (/root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n",
            "08/09/2023 10:29:28 - INFO - datasets.builder - Found cached dataset cnn_dailymail (/root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de\n",
            "08/09/2023 10:29:28 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de\n",
            "[INFO|configuration_utils.py:710] 2023-08-09 10:29:31,476 >> loading configuration file /content/drive/MyDrive/CyberBERT/manipulated/t5-small-llm123-seed42-factor5/config.json\n",
            "[INFO|configuration_utils.py:768] 2023-08-09 10:29:31,484 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"/content/drive/MyDrive/CyberBERT/manipulated/t5-small-llm123-seed42-factor5\",\n",
            "  \"architectures\": [\n",
            "    \"T5Model\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.31.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1839] 2023-08-09 10:29:31,877 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/spiece.model\n",
            "[INFO|tokenization_utils_base.py:1839] 2023-08-09 10:29:31,877 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1839] 2023-08-09 10:29:31,877 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1839] 2023-08-09 10:29:31,877 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1839] 2023-08-09 10:29:31,877 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1837] 2023-08-09 10:29:31,943 >> loading file spiece.model\n",
            "[INFO|tokenization_utils_base.py:1837] 2023-08-09 10:29:31,943 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:1837] 2023-08-09 10:29:31,943 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1837] 2023-08-09 10:29:31,943 >> loading file tokenizer_config.json\n",
            "[WARNING|logging.py:295] 2023-08-09 10:29:31,945 >> You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
            "[INFO|modeling_utils.py:2600] 2023-08-09 10:29:32,039 >> loading weights file /content/drive/MyDrive/CyberBERT/manipulated/t5-small-llm123-seed42-factor5/pytorch_model.bin\n",
            "[INFO|configuration_utils.py:599] 2023-08-09 10:29:33,282 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.31.0\"\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3329] 2023-08-09 10:29:34,061 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:3337] 2023-08-09 10:29:34,062 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at /content/drive/MyDrive/CyberBERT/manipulated/t5-small-llm123-seed42-factor5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "[INFO|modeling_utils.py:2949] 2023-08-09 10:29:34,070 >> Generation config file not found, using a generation config created from the model config.\n",
            "Process #0 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-65b6c42f142b7e87_00000_of_00008.arrow\n",
            "08/09/2023 10:29:34 - INFO - datasets.arrow_dataset - Process #0 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-65b6c42f142b7e87_00000_of_00008.arrow\n",
            "Process #1 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-65b6c42f142b7e87_00001_of_00008.arrow\n",
            "08/09/2023 10:29:34 - INFO - datasets.arrow_dataset - Process #1 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-65b6c42f142b7e87_00001_of_00008.arrow\n",
            "Process #2 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-65b6c42f142b7e87_00002_of_00008.arrow\n",
            "08/09/2023 10:29:34 - INFO - datasets.arrow_dataset - Process #2 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-65b6c42f142b7e87_00002_of_00008.arrow\n",
            "Process #3 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-65b6c42f142b7e87_00003_of_00008.arrow\n",
            "08/09/2023 10:29:34 - INFO - datasets.arrow_dataset - Process #3 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-65b6c42f142b7e87_00003_of_00008.arrow\n",
            "Process #4 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-65b6c42f142b7e87_00004_of_00008.arrow\n",
            "08/09/2023 10:29:34 - INFO - datasets.arrow_dataset - Process #4 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-65b6c42f142b7e87_00004_of_00008.arrow\n",
            "Process #5 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-65b6c42f142b7e87_00005_of_00008.arrow\n",
            "08/09/2023 10:29:34 - INFO - datasets.arrow_dataset - Process #5 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-65b6c42f142b7e87_00005_of_00008.arrow\n",
            "Process #6 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-65b6c42f142b7e87_00006_of_00008.arrow\n",
            "08/09/2023 10:29:34 - INFO - datasets.arrow_dataset - Process #6 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-65b6c42f142b7e87_00006_of_00008.arrow\n",
            "Process #7 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-65b6c42f142b7e87_00007_of_00008.arrow\n",
            "08/09/2023 10:29:34 - INFO - datasets.arrow_dataset - Process #7 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-65b6c42f142b7e87_00007_of_00008.arrow\n",
            "Spawning 8 processes\n",
            "08/09/2023 10:29:34 - INFO - datasets.arrow_dataset - Spawning 8 processes\n",
            "Running tokenizer on train dataset (num_proc=8):   0% 0/287113 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:29:34,526 >> Token indices sequence length is longer than the specified maximum sequence length for this model (635 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:29:34,552 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1358 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:29:34,567 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1243 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:29:34,626 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1322 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:29:34,642 >> Token indices sequence length is longer than the specified maximum sequence length for this model (855 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:29:34,667 >> Token indices sequence length is longer than the specified maximum sequence length for this model (914 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:29:34,676 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1497 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:29:34,688 >> Token indices sequence length is longer than the specified maximum sequence length for this model (864 > 512). Running this sequence through the model will result in indexing errors\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-65b6c42f142b7e87_00001_of_00008.arrow\n",
            "08/09/2023 10:29:40 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-65b6c42f142b7e87_00001_of_00008.arrow\n",
            "Running tokenizer on train dataset (num_proc=8):   0% 1000/287113 [00:06<31:28, 151.52 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-65b6c42f142b7e87_00002_of_00008.arrow\n",
            "08/09/2023 10:29:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-65b6c42f142b7e87_00002_of_00008.arrow\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-65b6c42f142b7e87_00003_of_00008.arrow\n",
            "08/09/2023 10:29:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-65b6c42f142b7e87_00003_of_00008.arrow\n",
            "Running tokenizer on train dataset (num_proc=8):   1% 2000/287113 [00:07<14:09, 335.61 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-65b6c42f142b7e87_00007_of_00008.arrow\n",
            "08/09/2023 10:29:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-65b6c42f142b7e87_00007_of_00008.arrow\n",
            "Running tokenizer on train dataset (num_proc=8):   1% 4000/287113 [00:07<06:13, 758.08 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-65b6c42f142b7e87_00000_of_00008.arrow\n",
            "08/09/2023 10:29:43 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-65b6c42f142b7e87_00000_of_00008.arrow\n",
            "Running tokenizer on train dataset (num_proc=8):   2% 5000/287113 [00:08<06:02, 778.31 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-65b6c42f142b7e87_00006_of_00008.arrow\n",
            "08/09/2023 10:29:43 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-65b6c42f142b7e87_00006_of_00008.arrow\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-65b6c42f142b7e87_00005_of_00008.arrow\n",
            "08/09/2023 10:29:43 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-65b6c42f142b7e87_00005_of_00008.arrow\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-65b6c42f142b7e87_00004_of_00008.arrow\n",
            "08/09/2023 10:29:43 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-65b6c42f142b7e87_00004_of_00008.arrow\n",
            "Running tokenizer on train dataset (num_proc=8): 100% 287113/287113 [05:21<00:00, 892.29 examples/s]\n",
            "Concatenating 8 shards\n",
            "08/09/2023 10:34:56 - INFO - datasets.arrow_dataset - Concatenating 8 shards\n",
            "Process #0 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-0c70901bde4c79c7_00000_of_00008.arrow\n",
            "08/09/2023 10:34:56 - INFO - datasets.arrow_dataset - Process #0 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-0c70901bde4c79c7_00000_of_00008.arrow\n",
            "Process #1 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-0c70901bde4c79c7_00001_of_00008.arrow\n",
            "08/09/2023 10:34:56 - INFO - datasets.arrow_dataset - Process #1 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-0c70901bde4c79c7_00001_of_00008.arrow\n",
            "Process #2 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-0c70901bde4c79c7_00002_of_00008.arrow\n",
            "08/09/2023 10:34:56 - INFO - datasets.arrow_dataset - Process #2 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-0c70901bde4c79c7_00002_of_00008.arrow\n",
            "Process #3 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-0c70901bde4c79c7_00003_of_00008.arrow\n",
            "08/09/2023 10:34:56 - INFO - datasets.arrow_dataset - Process #3 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-0c70901bde4c79c7_00003_of_00008.arrow\n",
            "Process #4 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-0c70901bde4c79c7_00004_of_00008.arrow\n",
            "08/09/2023 10:34:56 - INFO - datasets.arrow_dataset - Process #4 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-0c70901bde4c79c7_00004_of_00008.arrow\n",
            "Process #5 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-0c70901bde4c79c7_00005_of_00008.arrow\n",
            "08/09/2023 10:34:56 - INFO - datasets.arrow_dataset - Process #5 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-0c70901bde4c79c7_00005_of_00008.arrow\n",
            "Process #6 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-0c70901bde4c79c7_00006_of_00008.arrow\n",
            "08/09/2023 10:34:56 - INFO - datasets.arrow_dataset - Process #6 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-0c70901bde4c79c7_00006_of_00008.arrow\n",
            "Process #7 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-0c70901bde4c79c7_00007_of_00008.arrow\n",
            "08/09/2023 10:34:56 - INFO - datasets.arrow_dataset - Process #7 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-0c70901bde4c79c7_00007_of_00008.arrow\n",
            "Spawning 8 processes\n",
            "08/09/2023 10:34:56 - INFO - datasets.arrow_dataset - Spawning 8 processes\n",
            "Running tokenizer on validation dataset (num_proc=8):   0% 0/13368 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:34:56,493 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1035 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:34:56,513 >> Token indices sequence length is longer than the specified maximum sequence length for this model (588 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:34:56,557 >> Token indices sequence length is longer than the specified maximum sequence length for this model (771 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:34:56,608 >> Token indices sequence length is longer than the specified maximum sequence length for this model (971 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:34:56,619 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1221 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:34:56,620 >> Token indices sequence length is longer than the specified maximum sequence length for this model (861 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:34:56,663 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1586 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:34:56,667 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1202 > 512). Running this sequence through the model will result in indexing errors\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-0c70901bde4c79c7_00000_of_00008.arrow\n",
            "08/09/2023 10:35:02 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-0c70901bde4c79c7_00000_of_00008.arrow\n",
            "Running tokenizer on validation dataset (num_proc=8):   7% 1000/13368 [00:06<01:23, 147.75 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-0c70901bde4c79c7_00001_of_00008.arrow\n",
            "08/09/2023 10:35:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-0c70901bde4c79c7_00001_of_00008.arrow\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-0c70901bde4c79c7_00006_of_00008.arrow\n",
            "08/09/2023 10:35:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-0c70901bde4c79c7_00006_of_00008.arrow\n",
            "Running tokenizer on validation dataset (num_proc=8):  15% 2000/13368 [00:06<00:32, 346.88 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-0c70901bde4c79c7_00005_of_00008.arrow\n",
            "08/09/2023 10:35:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-0c70901bde4c79c7_00005_of_00008.arrow\n",
            "Running tokenizer on validation dataset (num_proc=8):  30% 4000/13368 [00:07<00:09, 976.80 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-0c70901bde4c79c7_00007_of_00008.arrow\n",
            "08/09/2023 10:35:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-0c70901bde4c79c7_00007_of_00008.arrow\n",
            "Running tokenizer on validation dataset (num_proc=8):  37% 5000/13368 [00:07<00:06, 1246.32 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-0c70901bde4c79c7_00002_of_00008.arrow\n",
            "08/09/2023 10:35:04 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-0c70901bde4c79c7_00002_of_00008.arrow\n",
            "Running tokenizer on validation dataset (num_proc=8):  45% 6000/13368 [00:08<00:06, 1132.98 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-0c70901bde4c79c7_00003_of_00008.arrow\n",
            "08/09/2023 10:35:06 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-0c70901bde4c79c7_00003_of_00008.arrow\n",
            "Running tokenizer on validation dataset (num_proc=8):  52% 7000/13368 [00:10<00:06, 911.82 examples/s] Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-0c70901bde4c79c7_00004_of_00008.arrow\n",
            "08/09/2023 10:35:07 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-0c70901bde4c79c7_00004_of_00008.arrow\n",
            "Running tokenizer on validation dataset (num_proc=8): 100% 13368/13368 [00:16<00:00, 834.80 examples/s]\n",
            "Concatenating 8 shards\n",
            "08/09/2023 10:35:12 - INFO - datasets.arrow_dataset - Concatenating 8 shards\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1686] 2023-08-09 10:35:16,160 >> ***** Running training *****\n",
            "[INFO|trainer.py:1687] 2023-08-09 10:35:16,160 >>   Num examples = 287,113\n",
            "[INFO|trainer.py:1688] 2023-08-09 10:35:16,160 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1689] 2023-08-09 10:35:16,160 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:1692] 2023-08-09 10:35:16,160 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "[INFO|trainer.py:1693] 2023-08-09 10:35:16,160 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1694] 2023-08-09 10:35:16,160 >>   Total optimization steps = 215,337\n",
            "[INFO|trainer.py:1695] 2023-08-09 10:35:16,161 >>   Number of trainable parameters = 60,506,624\n",
            "  0% 0/215337 [00:00<?, ?it/s][WARNING|logging.py:280] 2023-08-09 10:35:16,186 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 9.4365, 'learning_rate': 4.9883902905678084e-05, 'epoch': 0.01}\n",
            "{'loss': 5.2146, 'learning_rate': 4.976780581135616e-05, 'epoch': 0.01}\n",
            "{'loss': 4.8647, 'learning_rate': 4.965170871703423e-05, 'epoch': 0.02}\n",
            "{'loss': 4.6975, 'learning_rate': 4.953561162271231e-05, 'epoch': 0.03}\n",
            "{'loss': 4.5967, 'learning_rate': 4.941951452839039e-05, 'epoch': 0.03}\n",
            "{'loss': 4.5412, 'learning_rate': 4.9303417434068464e-05, 'epoch': 0.04}\n",
            "{'loss': 4.4862, 'learning_rate': 4.918732033974654e-05, 'epoch': 0.05}\n",
            "{'loss': 4.442, 'learning_rate': 4.907122324542461e-05, 'epoch': 0.06}\n",
            "{'loss': 4.4019, 'learning_rate': 4.8955126151102695e-05, 'epoch': 0.06}\n",
            "{'loss': 4.3767, 'learning_rate': 4.883902905678077e-05, 'epoch': 0.07}\n",
            "{'loss': 4.3706, 'learning_rate': 4.8722931962458844e-05, 'epoch': 0.08}\n",
            "{'loss': 4.3418, 'learning_rate': 4.8606834868136925e-05, 'epoch': 0.08}\n",
            "{'loss': 4.3175, 'learning_rate': 4.8490737773815e-05, 'epoch': 0.09}\n",
            "{'loss': 4.2981, 'learning_rate': 4.8374640679493075e-05, 'epoch': 0.1}\n",
            "{'loss': 4.2653, 'learning_rate': 4.825854358517115e-05, 'epoch': 0.1}\n",
            "{'loss': 4.2719, 'learning_rate': 4.814244649084923e-05, 'epoch': 0.11}\n",
            "{'loss': 4.27, 'learning_rate': 4.8026349396527306e-05, 'epoch': 0.12}\n",
            "{'loss': 4.2439, 'learning_rate': 4.791025230220538e-05, 'epoch': 0.13}\n",
            "{'loss': 4.2383, 'learning_rate': 4.779415520788346e-05, 'epoch': 0.13}\n",
            "{'loss': 4.245, 'learning_rate': 4.7678058113561536e-05, 'epoch': 0.14}\n",
            "{'loss': 4.2168, 'learning_rate': 4.756196101923962e-05, 'epoch': 0.15}\n",
            "{'loss': 4.2074, 'learning_rate': 4.7445863924917686e-05, 'epoch': 0.15}\n",
            "{'loss': 4.2067, 'learning_rate': 4.732976683059577e-05, 'epoch': 0.16}\n",
            "{'loss': 4.1993, 'learning_rate': 4.721366973627384e-05, 'epoch': 0.17}\n",
            "{'loss': 4.1974, 'learning_rate': 4.709757264195192e-05, 'epoch': 0.17}\n",
            "{'loss': 4.1926, 'learning_rate': 4.698147554762999e-05, 'epoch': 0.18}\n",
            "{'loss': 4.1915, 'learning_rate': 4.686537845330807e-05, 'epoch': 0.19}\n",
            "{'loss': 4.163, 'learning_rate': 4.6749281358986154e-05, 'epoch': 0.2}\n",
            "{'loss': 4.172, 'learning_rate': 4.663318426466423e-05, 'epoch': 0.2}\n",
            "{'loss': 4.1454, 'learning_rate': 4.65170871703423e-05, 'epoch': 0.21}\n",
            "{'loss': 4.1874, 'learning_rate': 4.640099007602038e-05, 'epoch': 0.22}\n",
            "{'loss': 4.1581, 'learning_rate': 4.628489298169846e-05, 'epoch': 0.22}\n",
            "{'loss': 4.1572, 'learning_rate': 4.6168795887376534e-05, 'epoch': 0.23}\n",
            "{'loss': 4.1468, 'learning_rate': 4.605269879305461e-05, 'epoch': 0.24}\n",
            "{'loss': 4.139, 'learning_rate': 4.5936601698732683e-05, 'epoch': 0.24}\n",
            "{'loss': 4.1494, 'learning_rate': 4.5820504604410765e-05, 'epoch': 0.25}\n",
            "{'loss': 4.1523, 'learning_rate': 4.570440751008884e-05, 'epoch': 0.26}\n",
            "{'loss': 4.1228, 'learning_rate': 4.5588310415766914e-05, 'epoch': 0.26}\n",
            "{'loss': 4.1339, 'learning_rate': 4.5472213321444996e-05, 'epoch': 0.27}\n",
            "{'loss': 4.1189, 'learning_rate': 4.535611622712307e-05, 'epoch': 0.28}\n",
            "  9% 20000/215337 [37:25<6:13:32,  8.72it/s][INFO|trainer.py:2807] 2023-08-09 11:12:41,926 >> Saving model checkpoint to ./tst-summarization/checkpoint-20000\n",
            "[INFO|configuration_utils.py:458] 2023-08-09 11:12:41,927 >> Configuration saved in ./tst-summarization/checkpoint-20000/config.json\n",
            "[INFO|configuration_utils.py:375] 2023-08-09 11:12:41,927 >> Configuration saved in ./tst-summarization/checkpoint-20000/generation_config.json\n",
            "[INFO|modeling_utils.py:1851] 2023-08-09 11:12:42,267 >> Model weights saved in ./tst-summarization/checkpoint-20000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2210] 2023-08-09 11:12:42,268 >> tokenizer config file saved in ./tst-summarization/checkpoint-20000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2023-08-09 11:12:42,268 >> Special tokens file saved in ./tst-summarization/checkpoint-20000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-09 11:12:42,304 >> Copy vocab file to ./tst-summarization/checkpoint-20000/spiece.model\n",
            "{'loss': 4.1269, 'learning_rate': 4.5240019132801145e-05, 'epoch': 0.29}\n",
            "{'loss': 4.1047, 'learning_rate': 4.512392203847922e-05, 'epoch': 0.29}\n",
            "{'loss': 4.1082, 'learning_rate': 4.50078249441573e-05, 'epoch': 0.3}\n",
            "{'loss': 4.1187, 'learning_rate': 4.4891727849835376e-05, 'epoch': 0.31}\n",
            "{'loss': 4.0922, 'learning_rate': 4.477563075551345e-05, 'epoch': 0.31}\n",
            "{'loss': 4.121, 'learning_rate': 4.4659533661191525e-05, 'epoch': 0.32}\n",
            "{'loss': 4.0981, 'learning_rate': 4.4543436566869607e-05, 'epoch': 0.33}\n",
            "{'loss': 4.0779, 'learning_rate': 4.442733947254769e-05, 'epoch': 0.33}\n",
            "{'loss': 4.1079, 'learning_rate': 4.4311242378225756e-05, 'epoch': 0.34}\n",
            "{'loss': 4.0822, 'learning_rate': 4.419514528390384e-05, 'epoch': 0.35}\n",
            "{'loss': 4.0957, 'learning_rate': 4.407904818958191e-05, 'epoch': 0.36}\n",
            "{'loss': 4.0915, 'learning_rate': 4.3962951095259993e-05, 'epoch': 0.36}\n",
            "{'loss': 4.0716, 'learning_rate': 4.384685400093806e-05, 'epoch': 0.37}\n",
            "{'loss': 4.0726, 'learning_rate': 4.373075690661614e-05, 'epoch': 0.38}\n",
            "{'loss': 4.0598, 'learning_rate': 4.361465981229422e-05, 'epoch': 0.38}\n",
            "{'loss': 4.0694, 'learning_rate': 4.34985627179723e-05, 'epoch': 0.39}\n",
            "{'loss': 4.0794, 'learning_rate': 4.3382465623650374e-05, 'epoch': 0.4}\n",
            "{'loss': 4.0818, 'learning_rate': 4.326636852932845e-05, 'epoch': 0.4}\n",
            "{'loss': 4.0765, 'learning_rate': 4.315027143500653e-05, 'epoch': 0.41}\n",
            "{'loss': 4.0546, 'learning_rate': 4.3034174340684604e-05, 'epoch': 0.42}\n",
            "{'loss': 4.0733, 'learning_rate': 4.291807724636268e-05, 'epoch': 0.42}\n",
            "{'loss': 4.0653, 'learning_rate': 4.2801980152040754e-05, 'epoch': 0.43}\n",
            "{'loss': 4.05, 'learning_rate': 4.2685883057718835e-05, 'epoch': 0.44}\n",
            "{'loss': 4.0531, 'learning_rate': 4.256978596339691e-05, 'epoch': 0.45}\n",
            "{'loss': 4.0581, 'learning_rate': 4.2453688869074984e-05, 'epoch': 0.45}\n",
            "{'loss': 4.0456, 'learning_rate': 4.2337591774753066e-05, 'epoch': 0.46}\n",
            "{'loss': 4.0602, 'learning_rate': 4.222149468043114e-05, 'epoch': 0.47}\n",
            "{'loss': 4.0375, 'learning_rate': 4.210539758610922e-05, 'epoch': 0.47}\n",
            "{'loss': 4.0603, 'learning_rate': 4.198930049178729e-05, 'epoch': 0.48}\n",
            "{'loss': 4.0432, 'learning_rate': 4.187320339746537e-05, 'epoch': 0.49}\n",
            "{'loss': 4.0385, 'learning_rate': 4.1757106303143446e-05, 'epoch': 0.49}\n",
            "{'loss': 4.0435, 'learning_rate': 4.164100920882153e-05, 'epoch': 0.5}\n",
            "{'loss': 4.0401, 'learning_rate': 4.1524912114499595e-05, 'epoch': 0.51}\n",
            "{'loss': 4.0397, 'learning_rate': 4.140881502017768e-05, 'epoch': 0.52}\n",
            "{'loss': 4.0326, 'learning_rate': 4.129271792585576e-05, 'epoch': 0.52}\n",
            "{'loss': 4.0382, 'learning_rate': 4.117662083153383e-05, 'epoch': 0.53}\n",
            "{'loss': 4.0379, 'learning_rate': 4.106052373721191e-05, 'epoch': 0.54}\n",
            "{'loss': 4.0403, 'learning_rate': 4.094442664288998e-05, 'epoch': 0.54}\n",
            "{'loss': 4.0242, 'learning_rate': 4.0828329548568064e-05, 'epoch': 0.55}\n",
            "{'loss': 4.0335, 'learning_rate': 4.071223245424614e-05, 'epoch': 0.56}\n",
            " 19% 40000/215337 [1:14:52<5:30:27,  8.84it/s][INFO|trainer.py:2807] 2023-08-09 11:50:09,154 >> Saving model checkpoint to ./tst-summarization/checkpoint-40000\n",
            "[INFO|configuration_utils.py:458] 2023-08-09 11:50:09,155 >> Configuration saved in ./tst-summarization/checkpoint-40000/config.json\n",
            "[INFO|configuration_utils.py:375] 2023-08-09 11:50:09,155 >> Configuration saved in ./tst-summarization/checkpoint-40000/generation_config.json\n",
            "[INFO|modeling_utils.py:1851] 2023-08-09 11:50:09,485 >> Model weights saved in ./tst-summarization/checkpoint-40000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2210] 2023-08-09 11:50:09,486 >> tokenizer config file saved in ./tst-summarization/checkpoint-40000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2023-08-09 11:50:09,486 >> Special tokens file saved in ./tst-summarization/checkpoint-40000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-09 11:50:09,522 >> Copy vocab file to ./tst-summarization/checkpoint-40000/spiece.model\n",
            "{'loss': 4.0105, 'learning_rate': 4.059613535992421e-05, 'epoch': 0.56}\n",
            "{'loss': 4.0313, 'learning_rate': 4.048003826560229e-05, 'epoch': 0.57}\n",
            "{'loss': 4.0123, 'learning_rate': 4.036394117128037e-05, 'epoch': 0.58}\n",
            "{'loss': 4.0196, 'learning_rate': 4.0247844076958444e-05, 'epoch': 0.59}\n",
            "{'loss': 4.0242, 'learning_rate': 4.013174698263652e-05, 'epoch': 0.59}\n",
            "{'loss': 4.0149, 'learning_rate': 4.00156498883146e-05, 'epoch': 0.6}\n",
            "{'loss': 3.9993, 'learning_rate': 3.9899552793992674e-05, 'epoch': 0.61}\n",
            "{'loss': 4.0177, 'learning_rate': 3.978345569967075e-05, 'epoch': 0.61}\n",
            "{'loss': 4.0162, 'learning_rate': 3.9667358605348824e-05, 'epoch': 0.62}\n",
            "{'loss': 4.0246, 'learning_rate': 3.9551261511026905e-05, 'epoch': 0.63}\n",
            "{'loss': 4.0173, 'learning_rate': 3.943516441670498e-05, 'epoch': 0.63}\n",
            "{'loss': 4.0077, 'learning_rate': 3.9319067322383055e-05, 'epoch': 0.64}\n",
            "{'loss': 4.0101, 'learning_rate': 3.9202970228061136e-05, 'epoch': 0.65}\n",
            "{'loss': 4.0309, 'learning_rate': 3.908687313373921e-05, 'epoch': 0.65}\n",
            "{'loss': 4.0077, 'learning_rate': 3.897077603941729e-05, 'epoch': 0.66}\n",
            "{'loss': 3.9974, 'learning_rate': 3.885467894509536e-05, 'epoch': 0.67}\n",
            "{'loss': 3.9887, 'learning_rate': 3.873858185077344e-05, 'epoch': 0.68}\n",
            "{'loss': 3.9834, 'learning_rate': 3.8622484756451516e-05, 'epoch': 0.68}\n",
            "{'loss': 4.0054, 'learning_rate': 3.85063876621296e-05, 'epoch': 0.69}\n",
            "{'loss': 4.0088, 'learning_rate': 3.8390290567807665e-05, 'epoch': 0.7}\n",
            "{'loss': 4.0039, 'learning_rate': 3.827419347348575e-05, 'epoch': 0.7}\n",
            "{'loss': 4.0064, 'learning_rate': 3.815809637916383e-05, 'epoch': 0.71}\n",
            "{'loss': 3.9967, 'learning_rate': 3.80419992848419e-05, 'epoch': 0.72}\n",
            "{'loss': 3.9965, 'learning_rate': 3.792590219051998e-05, 'epoch': 0.72}\n",
            "{'loss': 4.0016, 'learning_rate': 3.780980509619805e-05, 'epoch': 0.73}\n",
            "{'loss': 3.9982, 'learning_rate': 3.7693708001876134e-05, 'epoch': 0.74}\n",
            "{'loss': 3.9888, 'learning_rate': 3.757761090755421e-05, 'epoch': 0.75}\n",
            "{'loss': 3.9912, 'learning_rate': 3.746151381323228e-05, 'epoch': 0.75}\n",
            "{'loss': 3.988, 'learning_rate': 3.734541671891036e-05, 'epoch': 0.76}\n",
            "{'loss': 3.9867, 'learning_rate': 3.722931962458844e-05, 'epoch': 0.77}\n",
            "{'loss': 4.0129, 'learning_rate': 3.7113222530266514e-05, 'epoch': 0.77}\n",
            "{'loss': 3.9895, 'learning_rate': 3.699712543594459e-05, 'epoch': 0.78}\n",
            "{'loss': 3.988, 'learning_rate': 3.688102834162267e-05, 'epoch': 0.79}\n",
            "{'loss': 3.9911, 'learning_rate': 3.6764931247300745e-05, 'epoch': 0.79}\n",
            "{'loss': 3.9932, 'learning_rate': 3.664883415297882e-05, 'epoch': 0.8}\n",
            "{'loss': 3.9903, 'learning_rate': 3.6532737058656894e-05, 'epoch': 0.81}\n",
            "{'loss': 3.9697, 'learning_rate': 3.6416639964334975e-05, 'epoch': 0.82}\n",
            "{'loss': 3.9929, 'learning_rate': 3.630054287001305e-05, 'epoch': 0.82}\n",
            "{'loss': 3.9743, 'learning_rate': 3.618444577569113e-05, 'epoch': 0.83}\n",
            "{'loss': 3.9797, 'learning_rate': 3.60683486813692e-05, 'epoch': 0.84}\n",
            " 28% 60000/215337 [1:52:21<4:45:03,  9.08it/s][INFO|trainer.py:2807] 2023-08-09 12:27:37,582 >> Saving model checkpoint to ./tst-summarization/checkpoint-60000\n",
            "[INFO|configuration_utils.py:458] 2023-08-09 12:27:37,583 >> Configuration saved in ./tst-summarization/checkpoint-60000/config.json\n",
            "[INFO|configuration_utils.py:375] 2023-08-09 12:27:37,583 >> Configuration saved in ./tst-summarization/checkpoint-60000/generation_config.json\n",
            "[INFO|modeling_utils.py:1851] 2023-08-09 12:27:37,915 >> Model weights saved in ./tst-summarization/checkpoint-60000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2210] 2023-08-09 12:27:37,916 >> tokenizer config file saved in ./tst-summarization/checkpoint-60000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2023-08-09 12:27:37,916 >> Special tokens file saved in ./tst-summarization/checkpoint-60000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-09 12:27:37,955 >> Copy vocab file to ./tst-summarization/checkpoint-60000/spiece.model\n",
            "{'loss': 4.0004, 'learning_rate': 3.595225158704728e-05, 'epoch': 0.84}\n",
            "{'loss': 3.9594, 'learning_rate': 3.583615449272536e-05, 'epoch': 0.85}\n",
            "{'loss': 3.9677, 'learning_rate': 3.572005739840344e-05, 'epoch': 0.86}\n",
            "{'loss': 3.9683, 'learning_rate': 3.560396030408151e-05, 'epoch': 0.86}\n",
            "{'loss': 3.9663, 'learning_rate': 3.5487863209759586e-05, 'epoch': 0.87}\n",
            "{'loss': 3.9824, 'learning_rate': 3.537176611543767e-05, 'epoch': 0.88}\n",
            "{'loss': 3.9849, 'learning_rate': 3.525566902111574e-05, 'epoch': 0.88}\n",
            "{'loss': 3.9669, 'learning_rate': 3.513957192679382e-05, 'epoch': 0.89}\n",
            "{'loss': 3.9826, 'learning_rate': 3.502347483247189e-05, 'epoch': 0.9}\n",
            "{'loss': 3.9722, 'learning_rate': 3.490737773814997e-05, 'epoch': 0.91}\n",
            "{'loss': 3.9659, 'learning_rate': 3.479128064382805e-05, 'epoch': 0.91}\n",
            "{'loss': 3.9765, 'learning_rate': 3.467518354950612e-05, 'epoch': 0.92}\n",
            "{'loss': 3.9827, 'learning_rate': 3.4559086455184204e-05, 'epoch': 0.93}\n",
            "{'loss': 3.9665, 'learning_rate': 3.444298936086228e-05, 'epoch': 0.93}\n",
            "{'loss': 3.9749, 'learning_rate': 3.432689226654035e-05, 'epoch': 0.94}\n",
            "{'loss': 3.9547, 'learning_rate': 3.421079517221843e-05, 'epoch': 0.95}\n",
            "{'loss': 3.9705, 'learning_rate': 3.409469807789651e-05, 'epoch': 0.95}\n",
            "{'loss': 3.982, 'learning_rate': 3.3978600983574584e-05, 'epoch': 0.96}\n",
            "{'loss': 3.9616, 'learning_rate': 3.386250388925266e-05, 'epoch': 0.97}\n",
            "{'loss': 3.9764, 'learning_rate': 3.374640679493074e-05, 'epoch': 0.98}\n",
            "{'loss': 3.9617, 'learning_rate': 3.3630309700608815e-05, 'epoch': 0.98}\n",
            "{'loss': 3.9845, 'learning_rate': 3.3514212606286896e-05, 'epoch': 0.99}\n",
            "{'loss': 3.9477, 'learning_rate': 3.3398115511964964e-05, 'epoch': 1.0}\n",
            "{'loss': 3.9639, 'learning_rate': 3.3282018417643046e-05, 'epoch': 1.0}\n",
            "{'loss': 3.9422, 'learning_rate': 3.316592132332112e-05, 'epoch': 1.01}\n",
            "{'loss': 3.9465, 'learning_rate': 3.30498242289992e-05, 'epoch': 1.02}\n",
            "{'loss': 3.9383, 'learning_rate': 3.293372713467727e-05, 'epoch': 1.02}\n",
            "{'loss': 3.9377, 'learning_rate': 3.281763004035535e-05, 'epoch': 1.03}\n",
            "{'loss': 3.962, 'learning_rate': 3.270153294603343e-05, 'epoch': 1.04}\n",
            "{'loss': 3.9535, 'learning_rate': 3.258543585171151e-05, 'epoch': 1.04}\n",
            "{'loss': 3.9597, 'learning_rate': 3.246933875738958e-05, 'epoch': 1.05}\n",
            "{'loss': 3.9441, 'learning_rate': 3.2353241663067656e-05, 'epoch': 1.06}\n",
            "{'loss': 3.954, 'learning_rate': 3.223714456874574e-05, 'epoch': 1.07}\n",
            "{'loss': 3.9414, 'learning_rate': 3.212104747442381e-05, 'epoch': 1.07}\n",
            "{'loss': 3.9394, 'learning_rate': 3.200495038010189e-05, 'epoch': 1.08}\n",
            "{'loss': 3.9565, 'learning_rate': 3.188885328577996e-05, 'epoch': 1.09}\n",
            "{'loss': 3.9465, 'learning_rate': 3.177275619145804e-05, 'epoch': 1.09}\n",
            "{'loss': 3.9218, 'learning_rate': 3.165665909713612e-05, 'epoch': 1.1}\n",
            "{'loss': 3.9273, 'learning_rate': 3.154056200281419e-05, 'epoch': 1.11}\n",
            "{'loss': 3.9358, 'learning_rate': 3.1424464908492274e-05, 'epoch': 1.11}\n",
            " 37% 80000/215337 [2:29:50<4:16:40,  8.79it/s][INFO|trainer.py:2807] 2023-08-09 13:05:06,494 >> Saving model checkpoint to ./tst-summarization/checkpoint-80000\n",
            "[INFO|configuration_utils.py:458] 2023-08-09 13:05:06,495 >> Configuration saved in ./tst-summarization/checkpoint-80000/config.json\n",
            "[INFO|configuration_utils.py:375] 2023-08-09 13:05:06,495 >> Configuration saved in ./tst-summarization/checkpoint-80000/generation_config.json\n",
            "[INFO|modeling_utils.py:1851] 2023-08-09 13:05:06,823 >> Model weights saved in ./tst-summarization/checkpoint-80000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2210] 2023-08-09 13:05:06,824 >> tokenizer config file saved in ./tst-summarization/checkpoint-80000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2023-08-09 13:05:06,824 >> Special tokens file saved in ./tst-summarization/checkpoint-80000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-09 13:05:06,860 >> Copy vocab file to ./tst-summarization/checkpoint-80000/spiece.model\n",
            "{'loss': 3.9317, 'learning_rate': 3.130836781417035e-05, 'epoch': 1.12}\n",
            "{'loss': 3.9387, 'learning_rate': 3.1192270719848423e-05, 'epoch': 1.13}\n",
            "{'loss': 3.9332, 'learning_rate': 3.10761736255265e-05, 'epoch': 1.14}\n",
            "{'loss': 3.9388, 'learning_rate': 3.096007653120458e-05, 'epoch': 1.14}\n",
            "{'loss': 3.9344, 'learning_rate': 3.0843979436882654e-05, 'epoch': 1.15}\n",
            "{'loss': 3.9137, 'learning_rate': 3.072788234256073e-05, 'epoch': 1.16}\n",
            "{'loss': 3.9538, 'learning_rate': 3.061178524823881e-05, 'epoch': 1.16}\n",
            "{'loss': 3.9309, 'learning_rate': 3.0495688153916885e-05, 'epoch': 1.17}\n",
            "{'loss': 3.936, 'learning_rate': 3.0379591059594963e-05, 'epoch': 1.18}\n",
            "{'loss': 3.9425, 'learning_rate': 3.0263493965273038e-05, 'epoch': 1.18}\n",
            "{'loss': 3.9225, 'learning_rate': 3.0147396870951116e-05, 'epoch': 1.19}\n",
            "{'loss': 3.9372, 'learning_rate': 3.003129977662919e-05, 'epoch': 1.2}\n",
            "{'loss': 3.9076, 'learning_rate': 2.991520268230727e-05, 'epoch': 1.21}\n",
            "{'loss': 3.9277, 'learning_rate': 2.9799105587985343e-05, 'epoch': 1.21}\n",
            "{'loss': 3.9332, 'learning_rate': 2.968300849366342e-05, 'epoch': 1.22}\n",
            "{'loss': 3.9495, 'learning_rate': 2.9566911399341503e-05, 'epoch': 1.23}\n",
            "{'loss': 3.9223, 'learning_rate': 2.9450814305019574e-05, 'epoch': 1.23}\n",
            "{'loss': 3.9413, 'learning_rate': 2.9334717210697655e-05, 'epoch': 1.24}\n",
            "{'loss': 3.9257, 'learning_rate': 2.9218620116375727e-05, 'epoch': 1.25}\n",
            "{'loss': 3.9325, 'learning_rate': 2.9102523022053808e-05, 'epoch': 1.25}\n",
            "{'loss': 3.9218, 'learning_rate': 2.898642592773188e-05, 'epoch': 1.26}\n",
            "{'loss': 3.9195, 'learning_rate': 2.887032883340996e-05, 'epoch': 1.27}\n",
            "{'loss': 3.9296, 'learning_rate': 2.8754231739088032e-05, 'epoch': 1.27}\n",
            "{'loss': 3.9477, 'learning_rate': 2.8638134644766114e-05, 'epoch': 1.28}\n",
            "{'loss': 3.936, 'learning_rate': 2.8522037550444185e-05, 'epoch': 1.29}\n",
            "{'loss': 3.9517, 'learning_rate': 2.8405940456122266e-05, 'epoch': 1.3}\n",
            "{'loss': 3.9332, 'learning_rate': 2.8289843361800344e-05, 'epoch': 1.3}\n",
            "{'loss': 3.9279, 'learning_rate': 2.817374626747842e-05, 'epoch': 1.31}\n",
            "{'loss': 3.9471, 'learning_rate': 2.8057649173156497e-05, 'epoch': 1.32}\n",
            "{'loss': 3.9204, 'learning_rate': 2.794155207883457e-05, 'epoch': 1.32}\n",
            "{'loss': 3.8996, 'learning_rate': 2.782545498451265e-05, 'epoch': 1.33}\n",
            "{'loss': 3.9415, 'learning_rate': 2.7709357890190724e-05, 'epoch': 1.34}\n",
            "{'loss': 3.9387, 'learning_rate': 2.7593260795868802e-05, 'epoch': 1.34}\n",
            "{'loss': 3.9183, 'learning_rate': 2.7477163701546877e-05, 'epoch': 1.35}\n",
            "{'loss': 3.9166, 'learning_rate': 2.7361066607224955e-05, 'epoch': 1.36}\n",
            "{'loss': 3.9367, 'learning_rate': 2.7244969512903033e-05, 'epoch': 1.37}\n",
            "{'loss': 3.9085, 'learning_rate': 2.7128872418581108e-05, 'epoch': 1.37}\n",
            "{'loss': 3.8989, 'learning_rate': 2.7012775324259186e-05, 'epoch': 1.38}\n",
            "{'loss': 3.9196, 'learning_rate': 2.689667822993726e-05, 'epoch': 1.39}\n",
            "{'loss': 3.9115, 'learning_rate': 2.678058113561534e-05, 'epoch': 1.39}\n",
            " 46% 100000/215337 [3:07:18<3:37:23,  8.84it/s][INFO|trainer.py:2807] 2023-08-09 13:42:34,625 >> Saving model checkpoint to ./tst-summarization/checkpoint-100000\n",
            "[INFO|configuration_utils.py:458] 2023-08-09 13:42:34,626 >> Configuration saved in ./tst-summarization/checkpoint-100000/config.json\n",
            "[INFO|configuration_utils.py:375] 2023-08-09 13:42:34,626 >> Configuration saved in ./tst-summarization/checkpoint-100000/generation_config.json\n",
            "[INFO|modeling_utils.py:1851] 2023-08-09 13:42:34,957 >> Model weights saved in ./tst-summarization/checkpoint-100000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2210] 2023-08-09 13:42:34,958 >> tokenizer config file saved in ./tst-summarization/checkpoint-100000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2023-08-09 13:42:34,958 >> Special tokens file saved in ./tst-summarization/checkpoint-100000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-09 13:42:34,994 >> Copy vocab file to ./tst-summarization/checkpoint-100000/spiece.model\n",
            "{'loss': 3.9015, 'learning_rate': 2.6664484041293413e-05, 'epoch': 1.4}\n",
            "{'loss': 3.9229, 'learning_rate': 2.654838694697149e-05, 'epoch': 1.41}\n",
            "{'loss': 3.9299, 'learning_rate': 2.6432289852649566e-05, 'epoch': 1.41}\n",
            "{'loss': 3.9068, 'learning_rate': 2.6316192758327644e-05, 'epoch': 1.42}\n",
            "{'loss': 3.9139, 'learning_rate': 2.6200095664005726e-05, 'epoch': 1.43}\n",
            "{'loss': 3.9067, 'learning_rate': 2.6083998569683797e-05, 'epoch': 1.43}\n",
            "{'loss': 3.8919, 'learning_rate': 2.5967901475361878e-05, 'epoch': 1.44}\n",
            "{'loss': 3.9191, 'learning_rate': 2.5851804381039953e-05, 'epoch': 1.45}\n",
            "{'loss': 3.9088, 'learning_rate': 2.573570728671803e-05, 'epoch': 1.46}\n",
            "{'loss': 3.9086, 'learning_rate': 2.5619610192396106e-05, 'epoch': 1.46}\n",
            "{'loss': 3.9004, 'learning_rate': 2.5503513098074184e-05, 'epoch': 1.47}\n",
            "{'loss': 3.9012, 'learning_rate': 2.538741600375226e-05, 'epoch': 1.48}\n",
            "{'loss': 3.9151, 'learning_rate': 2.5271318909430336e-05, 'epoch': 1.48}\n",
            "{'loss': 3.91, 'learning_rate': 2.5155221815108414e-05, 'epoch': 1.49}\n",
            "{'loss': 3.9156, 'learning_rate': 2.503912472078649e-05, 'epoch': 1.5}\n",
            "{'loss': 3.9201, 'learning_rate': 2.4923027626464564e-05, 'epoch': 1.5}\n",
            "{'loss': 3.9236, 'learning_rate': 2.4806930532142642e-05, 'epoch': 1.51}\n",
            "{'loss': 3.9257, 'learning_rate': 2.469083343782072e-05, 'epoch': 1.52}\n",
            "{'loss': 3.9118, 'learning_rate': 2.4574736343498798e-05, 'epoch': 1.53}\n",
            "{'loss': 3.9133, 'learning_rate': 2.4458639249176873e-05, 'epoch': 1.53}\n",
            "{'loss': 3.9089, 'learning_rate': 2.434254215485495e-05, 'epoch': 1.54}\n",
            "{'loss': 3.9084, 'learning_rate': 2.4226445060533025e-05, 'epoch': 1.55}\n",
            "{'loss': 3.8973, 'learning_rate': 2.4110347966211103e-05, 'epoch': 1.55}\n",
            "{'loss': 3.8894, 'learning_rate': 2.3994250871889178e-05, 'epoch': 1.56}\n",
            "{'loss': 3.9152, 'learning_rate': 2.3878153777567256e-05, 'epoch': 1.57}\n",
            "{'loss': 3.9079, 'learning_rate': 2.376205668324533e-05, 'epoch': 1.57}\n",
            "{'loss': 3.9232, 'learning_rate': 2.3645959588923412e-05, 'epoch': 1.58}\n",
            "{'loss': 3.924, 'learning_rate': 2.3529862494601487e-05, 'epoch': 1.59}\n",
            "{'loss': 3.892, 'learning_rate': 2.3413765400279565e-05, 'epoch': 1.6}\n",
            "{'loss': 3.9072, 'learning_rate': 2.329766830595764e-05, 'epoch': 1.6}\n",
            "{'loss': 3.9015, 'learning_rate': 2.3181571211635718e-05, 'epoch': 1.61}\n",
            "{'loss': 3.9063, 'learning_rate': 2.3065474117313792e-05, 'epoch': 1.62}\n",
            "{'loss': 3.8958, 'learning_rate': 2.294937702299187e-05, 'epoch': 1.62}\n",
            "{'loss': 3.9147, 'learning_rate': 2.2833279928669945e-05, 'epoch': 1.63}\n",
            "{'loss': 3.9164, 'learning_rate': 2.2717182834348023e-05, 'epoch': 1.64}\n",
            "{'loss': 3.9202, 'learning_rate': 2.2601085740026098e-05, 'epoch': 1.64}\n",
            "{'loss': 3.8977, 'learning_rate': 2.2484988645704176e-05, 'epoch': 1.65}\n",
            "{'loss': 3.911, 'learning_rate': 2.2368891551382254e-05, 'epoch': 1.66}\n",
            "{'loss': 3.9088, 'learning_rate': 2.2252794457060332e-05, 'epoch': 1.66}\n",
            "{'loss': 3.9099, 'learning_rate': 2.2136697362738407e-05, 'epoch': 1.67}\n",
            " 56% 120000/215337 [3:44:45<3:01:16,  8.77it/s][INFO|trainer.py:2807] 2023-08-09 14:20:02,038 >> Saving model checkpoint to ./tst-summarization/checkpoint-120000\n",
            "[INFO|configuration_utils.py:458] 2023-08-09 14:20:02,039 >> Configuration saved in ./tst-summarization/checkpoint-120000/config.json\n",
            "[INFO|configuration_utils.py:375] 2023-08-09 14:20:02,040 >> Configuration saved in ./tst-summarization/checkpoint-120000/generation_config.json\n",
            "[INFO|modeling_utils.py:1851] 2023-08-09 14:20:02,372 >> Model weights saved in ./tst-summarization/checkpoint-120000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2210] 2023-08-09 14:20:02,373 >> tokenizer config file saved in ./tst-summarization/checkpoint-120000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2023-08-09 14:20:02,373 >> Special tokens file saved in ./tst-summarization/checkpoint-120000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-09 14:20:02,409 >> Copy vocab file to ./tst-summarization/checkpoint-120000/spiece.model\n",
            "{'loss': 3.9122, 'learning_rate': 2.2020600268416485e-05, 'epoch': 1.68}\n",
            "{'loss': 3.8865, 'learning_rate': 2.190450317409456e-05, 'epoch': 1.69}\n",
            "{'loss': 3.9283, 'learning_rate': 2.1788406079772637e-05, 'epoch': 1.69}\n",
            "{'loss': 3.9197, 'learning_rate': 2.1672308985450712e-05, 'epoch': 1.7}\n",
            "{'loss': 3.9155, 'learning_rate': 2.155621189112879e-05, 'epoch': 1.71}\n",
            "{'loss': 3.9058, 'learning_rate': 2.1440114796806868e-05, 'epoch': 1.71}\n",
            "{'loss': 3.9294, 'learning_rate': 2.1324017702484943e-05, 'epoch': 1.72}\n",
            "{'loss': 3.895, 'learning_rate': 2.120792060816302e-05, 'epoch': 1.73}\n",
            "{'loss': 3.89, 'learning_rate': 2.1091823513841096e-05, 'epoch': 1.73}\n",
            "{'loss': 3.9059, 'learning_rate': 2.0975726419519174e-05, 'epoch': 1.74}\n",
            "{'loss': 3.8791, 'learning_rate': 2.0859629325197248e-05, 'epoch': 1.75}\n",
            "{'loss': 3.8984, 'learning_rate': 2.0743532230875326e-05, 'epoch': 1.76}\n",
            "{'loss': 3.9086, 'learning_rate': 2.06274351365534e-05, 'epoch': 1.76}\n",
            "{'loss': 3.8986, 'learning_rate': 2.051133804223148e-05, 'epoch': 1.77}\n",
            "{'loss': 3.9154, 'learning_rate': 2.0395240947909557e-05, 'epoch': 1.78}\n",
            "{'loss': 3.901, 'learning_rate': 2.0279143853587635e-05, 'epoch': 1.78}\n",
            "{'loss': 3.9007, 'learning_rate': 2.016304675926571e-05, 'epoch': 1.79}\n",
            "{'loss': 3.9027, 'learning_rate': 2.0046949664943788e-05, 'epoch': 1.8}\n",
            "{'loss': 3.8824, 'learning_rate': 1.9930852570621863e-05, 'epoch': 1.8}\n",
            "{'loss': 3.8998, 'learning_rate': 1.981475547629994e-05, 'epoch': 1.81}\n",
            "{'loss': 3.8932, 'learning_rate': 1.9698658381978015e-05, 'epoch': 1.82}\n",
            "{'loss': 3.8984, 'learning_rate': 1.9582561287656093e-05, 'epoch': 1.83}\n",
            "{'loss': 3.9054, 'learning_rate': 1.9466464193334168e-05, 'epoch': 1.83}\n",
            "{'loss': 3.8983, 'learning_rate': 1.935036709901225e-05, 'epoch': 1.84}\n",
            "{'loss': 3.8784, 'learning_rate': 1.9234270004690324e-05, 'epoch': 1.85}\n",
            "{'loss': 3.902, 'learning_rate': 1.9118172910368402e-05, 'epoch': 1.85}\n",
            "{'loss': 3.8847, 'learning_rate': 1.9002075816046477e-05, 'epoch': 1.86}\n",
            "{'loss': 3.9086, 'learning_rate': 1.8885978721724555e-05, 'epoch': 1.87}\n",
            "{'loss': 3.8746, 'learning_rate': 1.876988162740263e-05, 'epoch': 1.87}\n",
            "{'loss': 3.8856, 'learning_rate': 1.8653784533080708e-05, 'epoch': 1.88}\n",
            "{'loss': 3.9002, 'learning_rate': 1.8537687438758782e-05, 'epoch': 1.89}\n",
            "{'loss': 3.8922, 'learning_rate': 1.842159034443686e-05, 'epoch': 1.89}\n",
            "{'loss': 3.8947, 'learning_rate': 1.8305493250114935e-05, 'epoch': 1.9}\n",
            "{'loss': 3.8675, 'learning_rate': 1.8189396155793016e-05, 'epoch': 1.91}\n",
            "{'loss': 3.9043, 'learning_rate': 1.807329906147109e-05, 'epoch': 1.92}\n",
            "{'loss': 3.8979, 'learning_rate': 1.795720196714917e-05, 'epoch': 1.92}\n",
            "{'loss': 3.891, 'learning_rate': 1.7841104872827244e-05, 'epoch': 1.93}\n",
            "{'loss': 3.8919, 'learning_rate': 1.7725007778505322e-05, 'epoch': 1.94}\n",
            "{'loss': 3.8881, 'learning_rate': 1.7608910684183396e-05, 'epoch': 1.94}\n",
            "{'loss': 3.8854, 'learning_rate': 1.7492813589861475e-05, 'epoch': 1.95}\n",
            " 65% 140000/215337 [4:22:13<2:22:16,  8.82it/s][INFO|trainer.py:2807] 2023-08-09 14:57:29,690 >> Saving model checkpoint to ./tst-summarization/checkpoint-140000\n",
            "[INFO|configuration_utils.py:458] 2023-08-09 14:57:29,691 >> Configuration saved in ./tst-summarization/checkpoint-140000/config.json\n",
            "[INFO|configuration_utils.py:375] 2023-08-09 14:57:29,692 >> Configuration saved in ./tst-summarization/checkpoint-140000/generation_config.json\n",
            "[INFO|modeling_utils.py:1851] 2023-08-09 14:57:30,029 >> Model weights saved in ./tst-summarization/checkpoint-140000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2210] 2023-08-09 14:57:30,030 >> tokenizer config file saved in ./tst-summarization/checkpoint-140000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2023-08-09 14:57:30,030 >> Special tokens file saved in ./tst-summarization/checkpoint-140000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-09 14:57:30,066 >> Copy vocab file to ./tst-summarization/checkpoint-140000/spiece.model\n",
            "{'loss': 3.8978, 'learning_rate': 1.737671649553955e-05, 'epoch': 1.96}\n",
            "{'loss': 3.9014, 'learning_rate': 1.7260619401217627e-05, 'epoch': 1.96}\n",
            "{'loss': 3.8767, 'learning_rate': 1.7144522306895705e-05, 'epoch': 1.97}\n",
            "{'loss': 3.8766, 'learning_rate': 1.702842521257378e-05, 'epoch': 1.98}\n",
            "{'loss': 3.8901, 'learning_rate': 1.6912328118251858e-05, 'epoch': 1.99}\n",
            "{'loss': 3.899, 'learning_rate': 1.6796231023929933e-05, 'epoch': 1.99}\n",
            "{'loss': 3.8997, 'learning_rate': 1.668013392960801e-05, 'epoch': 2.0}\n",
            "{'loss': 3.8674, 'learning_rate': 1.6564036835286085e-05, 'epoch': 2.01}\n",
            "{'loss': 3.8818, 'learning_rate': 1.6447939740964163e-05, 'epoch': 2.01}\n",
            "{'loss': 3.8823, 'learning_rate': 1.6331842646642238e-05, 'epoch': 2.02}\n",
            "{'loss': 3.8914, 'learning_rate': 1.6215745552320316e-05, 'epoch': 2.03}\n",
            "{'loss': 3.8691, 'learning_rate': 1.6099648457998394e-05, 'epoch': 2.03}\n",
            "{'loss': 3.8694, 'learning_rate': 1.5983551363676472e-05, 'epoch': 2.04}\n",
            "{'loss': 3.896, 'learning_rate': 1.5867454269354547e-05, 'epoch': 2.05}\n",
            "{'loss': 3.8825, 'learning_rate': 1.5751357175032625e-05, 'epoch': 2.05}\n",
            "{'loss': 3.8661, 'learning_rate': 1.56352600807107e-05, 'epoch': 2.06}\n",
            "{'loss': 3.8778, 'learning_rate': 1.5519162986388778e-05, 'epoch': 2.07}\n",
            "{'loss': 3.8615, 'learning_rate': 1.5403065892066852e-05, 'epoch': 2.08}\n",
            "{'loss': 3.8852, 'learning_rate': 1.528696879774493e-05, 'epoch': 2.08}\n",
            "{'loss': 3.8718, 'learning_rate': 1.5170871703423007e-05, 'epoch': 2.09}\n",
            "{'loss': 3.8706, 'learning_rate': 1.5054774609101085e-05, 'epoch': 2.1}\n",
            "{'loss': 3.8867, 'learning_rate': 1.4938677514779161e-05, 'epoch': 2.1}\n",
            "{'loss': 3.8728, 'learning_rate': 1.4822580420457238e-05, 'epoch': 2.11}\n",
            "{'loss': 3.8784, 'learning_rate': 1.4706483326135314e-05, 'epoch': 2.12}\n",
            "{'loss': 3.888, 'learning_rate': 1.459038623181339e-05, 'epoch': 2.12}\n",
            "{'loss': 3.8945, 'learning_rate': 1.4474289137491467e-05, 'epoch': 2.13}\n",
            "{'loss': 3.8512, 'learning_rate': 1.4358192043169543e-05, 'epoch': 2.14}\n",
            "{'loss': 3.8575, 'learning_rate': 1.424209494884762e-05, 'epoch': 2.15}\n",
            "{'loss': 3.8723, 'learning_rate': 1.4125997854525696e-05, 'epoch': 2.15}\n",
            "{'loss': 3.8801, 'learning_rate': 1.4009900760203774e-05, 'epoch': 2.16}\n",
            "{'loss': 3.8662, 'learning_rate': 1.3893803665881852e-05, 'epoch': 2.17}\n",
            "{'loss': 3.8757, 'learning_rate': 1.3777706571559928e-05, 'epoch': 2.17}\n",
            "{'loss': 3.838, 'learning_rate': 1.3661609477238005e-05, 'epoch': 2.18}\n",
            "{'loss': 3.8752, 'learning_rate': 1.3545512382916081e-05, 'epoch': 2.19}\n",
            "{'loss': 3.886, 'learning_rate': 1.3429415288594157e-05, 'epoch': 2.19}\n",
            "{'loss': 3.8747, 'learning_rate': 1.3313318194272234e-05, 'epoch': 2.2}\n",
            "{'loss': 3.8622, 'learning_rate': 1.319722109995031e-05, 'epoch': 2.21}\n",
            "{'loss': 3.8631, 'learning_rate': 1.3081124005628386e-05, 'epoch': 2.22}\n",
            "{'loss': 3.8976, 'learning_rate': 1.2965026911306463e-05, 'epoch': 2.22}\n",
            "{'loss': 3.8842, 'learning_rate': 1.2848929816984542e-05, 'epoch': 2.23}\n",
            " 74% 160000/215337 [4:59:40<1:50:16,  8.36it/s][INFO|trainer.py:2807] 2023-08-09 15:34:56,605 >> Saving model checkpoint to ./tst-summarization/checkpoint-160000\n",
            "[INFO|configuration_utils.py:458] 2023-08-09 15:34:56,606 >> Configuration saved in ./tst-summarization/checkpoint-160000/config.json\n",
            "[INFO|configuration_utils.py:375] 2023-08-09 15:34:56,607 >> Configuration saved in ./tst-summarization/checkpoint-160000/generation_config.json\n",
            "[INFO|modeling_utils.py:1851] 2023-08-09 15:34:56,943 >> Model weights saved in ./tst-summarization/checkpoint-160000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2210] 2023-08-09 15:34:56,944 >> tokenizer config file saved in ./tst-summarization/checkpoint-160000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2023-08-09 15:34:56,944 >> Special tokens file saved in ./tst-summarization/checkpoint-160000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-09 15:34:56,980 >> Copy vocab file to ./tst-summarization/checkpoint-160000/spiece.model\n",
            "{'loss': 3.8834, 'learning_rate': 1.2732832722662619e-05, 'epoch': 2.24}\n",
            "{'loss': 3.8725, 'learning_rate': 1.2616735628340695e-05, 'epoch': 2.24}\n",
            "{'loss': 3.8566, 'learning_rate': 1.2500638534018772e-05, 'epoch': 2.25}\n",
            "{'loss': 3.8852, 'learning_rate': 1.2384541439696848e-05, 'epoch': 2.26}\n",
            "{'loss': 3.8907, 'learning_rate': 1.2268444345374924e-05, 'epoch': 2.26}\n",
            "{'loss': 3.8746, 'learning_rate': 1.2152347251053e-05, 'epoch': 2.27}\n",
            "{'loss': 3.8805, 'learning_rate': 1.2036250156731079e-05, 'epoch': 2.28}\n",
            "{'loss': 3.8866, 'learning_rate': 1.1920153062409155e-05, 'epoch': 2.28}\n",
            "{'loss': 3.8684, 'learning_rate': 1.1804055968087231e-05, 'epoch': 2.29}\n",
            "{'loss': 3.8951, 'learning_rate': 1.1687958873765308e-05, 'epoch': 2.3}\n",
            "{'loss': 3.8979, 'learning_rate': 1.1571861779443384e-05, 'epoch': 2.31}\n",
            "{'loss': 3.8556, 'learning_rate': 1.1455764685121462e-05, 'epoch': 2.31}\n",
            "{'loss': 3.8517, 'learning_rate': 1.1339667590799539e-05, 'epoch': 2.32}\n",
            "{'loss': 3.8745, 'learning_rate': 1.1223570496477615e-05, 'epoch': 2.33}\n",
            "{'loss': 3.8738, 'learning_rate': 1.1107473402155691e-05, 'epoch': 2.33}\n",
            "{'loss': 3.8694, 'learning_rate': 1.099137630783377e-05, 'epoch': 2.34}\n",
            "{'loss': 3.8744, 'learning_rate': 1.0875279213511846e-05, 'epoch': 2.35}\n",
            "{'loss': 3.8873, 'learning_rate': 1.0759182119189922e-05, 'epoch': 2.35}\n",
            "{'loss': 3.8511, 'learning_rate': 1.0643085024867998e-05, 'epoch': 2.36}\n",
            "{'loss': 3.8611, 'learning_rate': 1.0526987930546075e-05, 'epoch': 2.37}\n",
            "{'loss': 3.8658, 'learning_rate': 1.0410890836224153e-05, 'epoch': 2.38}\n",
            "{'loss': 3.8735, 'learning_rate': 1.029479374190223e-05, 'epoch': 2.38}\n",
            "{'loss': 3.8933, 'learning_rate': 1.0178696647580306e-05, 'epoch': 2.39}\n",
            "{'loss': 3.8725, 'learning_rate': 1.0062599553258382e-05, 'epoch': 2.4}\n",
            "{'loss': 3.8791, 'learning_rate': 9.946502458936458e-06, 'epoch': 2.4}\n",
            "{'loss': 3.8678, 'learning_rate': 9.830405364614535e-06, 'epoch': 2.41}\n",
            "{'loss': 3.889, 'learning_rate': 9.714308270292611e-06, 'epoch': 2.42}\n",
            "{'loss': 3.8664, 'learning_rate': 9.598211175970687e-06, 'epoch': 2.42}\n",
            "{'loss': 3.8779, 'learning_rate': 9.482114081648764e-06, 'epoch': 2.43}\n",
            "{'loss': 3.8761, 'learning_rate': 9.366016987326842e-06, 'epoch': 2.44}\n",
            "{'loss': 3.8633, 'learning_rate': 9.249919893004918e-06, 'epoch': 2.45}\n",
            "{'loss': 3.87, 'learning_rate': 9.133822798682994e-06, 'epoch': 2.45}\n",
            "{'loss': 3.8815, 'learning_rate': 9.01772570436107e-06, 'epoch': 2.46}\n",
            "{'loss': 3.8746, 'learning_rate': 8.901628610039147e-06, 'epoch': 2.47}\n",
            "{'loss': 3.8663, 'learning_rate': 8.785531515717225e-06, 'epoch': 2.47}\n",
            "{'loss': 3.896, 'learning_rate': 8.669434421395302e-06, 'epoch': 2.48}\n",
            "{'loss': 3.856, 'learning_rate': 8.553337327073378e-06, 'epoch': 2.49}\n",
            "{'loss': 3.8788, 'learning_rate': 8.437240232751454e-06, 'epoch': 2.49}\n",
            "{'loss': 3.8768, 'learning_rate': 8.321143138429532e-06, 'epoch': 2.5}\n",
            "{'loss': 3.8786, 'learning_rate': 8.205046044107609e-06, 'epoch': 2.51}\n",
            " 84% 180000/215337 [5:37:05<1:05:45,  8.96it/s][INFO|trainer.py:2807] 2023-08-09 16:12:21,699 >> Saving model checkpoint to ./tst-summarization/checkpoint-180000\n",
            "[INFO|configuration_utils.py:458] 2023-08-09 16:12:21,700 >> Configuration saved in ./tst-summarization/checkpoint-180000/config.json\n",
            "[INFO|configuration_utils.py:375] 2023-08-09 16:12:21,700 >> Configuration saved in ./tst-summarization/checkpoint-180000/generation_config.json\n",
            "[INFO|modeling_utils.py:1851] 2023-08-09 16:12:22,027 >> Model weights saved in ./tst-summarization/checkpoint-180000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2210] 2023-08-09 16:12:22,027 >> tokenizer config file saved in ./tst-summarization/checkpoint-180000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2023-08-09 16:12:22,028 >> Special tokens file saved in ./tst-summarization/checkpoint-180000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-09 16:12:22,063 >> Copy vocab file to ./tst-summarization/checkpoint-180000/spiece.model\n",
            "{'loss': 3.8769, 'learning_rate': 8.088948949785685e-06, 'epoch': 2.51}\n",
            "{'loss': 3.8545, 'learning_rate': 7.972851855463761e-06, 'epoch': 2.52}\n",
            "{'loss': 3.8597, 'learning_rate': 7.856754761141838e-06, 'epoch': 2.53}\n",
            "{'loss': 3.8746, 'learning_rate': 7.740657666819916e-06, 'epoch': 2.54}\n",
            "{'loss': 3.8711, 'learning_rate': 7.624560572497992e-06, 'epoch': 2.54}\n",
            "{'loss': 3.8702, 'learning_rate': 7.5084634781760686e-06, 'epoch': 2.55}\n",
            "{'loss': 3.8786, 'learning_rate': 7.392366383854145e-06, 'epoch': 2.56}\n",
            "{'loss': 3.8628, 'learning_rate': 7.276269289532221e-06, 'epoch': 2.56}\n",
            "{'loss': 3.8793, 'learning_rate': 7.160172195210299e-06, 'epoch': 2.57}\n",
            "{'loss': 3.8508, 'learning_rate': 7.044075100888376e-06, 'epoch': 2.58}\n",
            "{'loss': 3.8677, 'learning_rate': 6.927978006566452e-06, 'epoch': 2.58}\n",
            "{'loss': 3.8589, 'learning_rate': 6.811880912244528e-06, 'epoch': 2.59}\n",
            "{'loss': 3.8594, 'learning_rate': 6.695783817922606e-06, 'epoch': 2.6}\n",
            "{'loss': 3.8483, 'learning_rate': 6.579686723600682e-06, 'epoch': 2.61}\n",
            "{'loss': 3.8631, 'learning_rate': 6.463589629278758e-06, 'epoch': 2.61}\n",
            "{'loss': 3.8678, 'learning_rate': 6.347492534956835e-06, 'epoch': 2.62}\n",
            "{'loss': 3.8716, 'learning_rate': 6.231395440634912e-06, 'epoch': 2.63}\n",
            "{'loss': 3.8724, 'learning_rate': 6.115298346312988e-06, 'epoch': 2.63}\n",
            "{'loss': 3.8524, 'learning_rate': 5.9992012519910655e-06, 'epoch': 2.64}\n",
            "{'loss': 3.8703, 'learning_rate': 5.883104157669142e-06, 'epoch': 2.65}\n",
            "{'loss': 3.8804, 'learning_rate': 5.767007063347219e-06, 'epoch': 2.65}\n",
            "{'loss': 3.8774, 'learning_rate': 5.650909969025295e-06, 'epoch': 2.66}\n",
            "{'loss': 3.8714, 'learning_rate': 5.534812874703372e-06, 'epoch': 2.67}\n",
            "{'loss': 3.8719, 'learning_rate': 5.418715780381449e-06, 'epoch': 2.67}\n",
            "{'loss': 3.8773, 'learning_rate': 5.302618686059525e-06, 'epoch': 2.68}\n",
            "{'loss': 3.8538, 'learning_rate': 5.1865215917376025e-06, 'epoch': 2.69}\n",
            "{'loss': 3.8643, 'learning_rate': 5.070424497415679e-06, 'epoch': 2.7}\n",
            "{'loss': 3.8524, 'learning_rate': 4.954327403093756e-06, 'epoch': 2.7}\n",
            "{'loss': 3.8738, 'learning_rate': 4.8382303087718325e-06, 'epoch': 2.71}\n",
            "{'loss': 3.8642, 'learning_rate': 4.722133214449909e-06, 'epoch': 2.72}\n",
            "{'loss': 3.8508, 'learning_rate': 4.606036120127986e-06, 'epoch': 2.72}\n",
            "{'loss': 3.862, 'learning_rate': 4.489939025806062e-06, 'epoch': 2.73}\n",
            "{'loss': 3.8657, 'learning_rate': 4.37384193148414e-06, 'epoch': 2.74}\n",
            "{'loss': 3.8783, 'learning_rate': 4.257744837162216e-06, 'epoch': 2.74}\n",
            "{'loss': 3.8647, 'learning_rate': 4.141647742840292e-06, 'epoch': 2.75}\n",
            "{'loss': 3.8586, 'learning_rate': 4.025550648518369e-06, 'epoch': 2.76}\n",
            "{'loss': 3.8594, 'learning_rate': 3.909453554196446e-06, 'epoch': 2.77}\n",
            "{'loss': 3.8604, 'learning_rate': 3.7933564598745227e-06, 'epoch': 2.77}\n",
            "{'loss': 3.8788, 'learning_rate': 3.677259365552599e-06, 'epoch': 2.78}\n",
            "{'loss': 3.8656, 'learning_rate': 3.561162271230676e-06, 'epoch': 2.79}\n",
            " 93% 200000/215337 [6:14:31<28:13,  9.06it/s][INFO|trainer.py:2807] 2023-08-09 16:49:47,574 >> Saving model checkpoint to ./tst-summarization/checkpoint-200000\n",
            "[INFO|configuration_utils.py:458] 2023-08-09 16:49:47,575 >> Configuration saved in ./tst-summarization/checkpoint-200000/config.json\n",
            "[INFO|configuration_utils.py:375] 2023-08-09 16:49:47,575 >> Configuration saved in ./tst-summarization/checkpoint-200000/generation_config.json\n",
            "[INFO|modeling_utils.py:1851] 2023-08-09 16:49:47,909 >> Model weights saved in ./tst-summarization/checkpoint-200000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2210] 2023-08-09 16:49:47,909 >> tokenizer config file saved in ./tst-summarization/checkpoint-200000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2023-08-09 16:49:47,910 >> Special tokens file saved in ./tst-summarization/checkpoint-200000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-09 16:49:47,947 >> Copy vocab file to ./tst-summarization/checkpoint-200000/spiece.model\n",
            "{'loss': 3.8573, 'learning_rate': 3.445065176908752e-06, 'epoch': 2.79}\n",
            "{'loss': 3.8702, 'learning_rate': 3.3289680825868294e-06, 'epoch': 2.8}\n",
            "{'loss': 3.8822, 'learning_rate': 3.2128709882649057e-06, 'epoch': 2.81}\n",
            "{'loss': 3.8601, 'learning_rate': 3.0967738939429825e-06, 'epoch': 2.81}\n",
            "{'loss': 3.8663, 'learning_rate': 2.9806767996210593e-06, 'epoch': 2.82}\n",
            "{'loss': 3.8645, 'learning_rate': 2.864579705299136e-06, 'epoch': 2.83}\n",
            "{'loss': 3.8686, 'learning_rate': 2.748482610977213e-06, 'epoch': 2.84}\n",
            "{'loss': 3.872, 'learning_rate': 2.6323855166552896e-06, 'epoch': 2.84}\n",
            "{'loss': 3.8576, 'learning_rate': 2.516288422333366e-06, 'epoch': 2.85}\n",
            "{'loss': 3.869, 'learning_rate': 2.4001913280114424e-06, 'epoch': 2.86}\n",
            "{'loss': 3.8592, 'learning_rate': 2.284094233689519e-06, 'epoch': 2.86}\n",
            "{'loss': 3.8645, 'learning_rate': 2.167997139367596e-06, 'epoch': 2.87}\n",
            "{'loss': 3.8545, 'learning_rate': 2.0519000450456727e-06, 'epoch': 2.88}\n",
            "{'loss': 3.8743, 'learning_rate': 1.9358029507237495e-06, 'epoch': 2.88}\n",
            "{'loss': 3.8577, 'learning_rate': 1.8197058564018263e-06, 'epoch': 2.89}\n",
            "{'loss': 3.8709, 'learning_rate': 1.7036087620799026e-06, 'epoch': 2.9}\n",
            "{'loss': 3.8601, 'learning_rate': 1.5875116677579794e-06, 'epoch': 2.9}\n",
            "{'loss': 3.8695, 'learning_rate': 1.471414573436056e-06, 'epoch': 2.91}\n",
            "{'loss': 3.868, 'learning_rate': 1.3553174791141328e-06, 'epoch': 2.92}\n",
            "{'loss': 3.8498, 'learning_rate': 1.2392203847922096e-06, 'epoch': 2.93}\n",
            "{'loss': 3.8614, 'learning_rate': 1.1231232904702863e-06, 'epoch': 2.93}\n",
            "{'loss': 3.8652, 'learning_rate': 1.007026196148363e-06, 'epoch': 2.94}\n",
            "{'loss': 3.8796, 'learning_rate': 8.909291018264396e-07, 'epoch': 2.95}\n",
            "{'loss': 3.8651, 'learning_rate': 7.748320075045162e-07, 'epoch': 2.95}\n",
            "{'loss': 3.8537, 'learning_rate': 6.587349131825929e-07, 'epoch': 2.96}\n",
            "{'loss': 3.8709, 'learning_rate': 5.426378188606695e-07, 'epoch': 2.97}\n",
            "{'loss': 3.8481, 'learning_rate': 4.265407245387463e-07, 'epoch': 2.97}\n",
            "{'loss': 3.8642, 'learning_rate': 3.104436302168229e-07, 'epoch': 2.98}\n",
            "{'loss': 3.8529, 'learning_rate': 1.9434653589489965e-07, 'epoch': 2.99}\n",
            "{'loss': 3.8679, 'learning_rate': 7.824944157297631e-08, 'epoch': 3.0}\n",
            "100% 215336/215337 [6:43:13<00:00,  8.58it/s][INFO|trainer.py:1934] 2023-08-09 17:18:29,721 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 24193.5599, 'train_samples_per_second': 35.602, 'train_steps_per_second': 8.901, 'train_loss': 3.9734332082622963, 'epoch': 3.0}\n",
            "100% 215337/215337 [6:43:13<00:00,  8.90it/s]\n",
            "[INFO|trainer.py:2807] 2023-08-09 17:18:29,723 >> Saving model checkpoint to ./tst-summarization\n",
            "[INFO|configuration_utils.py:458] 2023-08-09 17:18:29,724 >> Configuration saved in ./tst-summarization/config.json\n",
            "[INFO|configuration_utils.py:375] 2023-08-09 17:18:29,724 >> Configuration saved in ./tst-summarization/generation_config.json\n",
            "[INFO|modeling_utils.py:1851] 2023-08-09 17:18:30,053 >> Model weights saved in ./tst-summarization/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2210] 2023-08-09 17:18:30,054 >> tokenizer config file saved in ./tst-summarization/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2023-08-09 17:18:30,054 >> Special tokens file saved in ./tst-summarization/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-09 17:18:30,090 >> Copy vocab file to ./tst-summarization/spiece.model\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     3.9734\n",
            "  train_runtime            = 6:43:13.55\n",
            "  train_samples            =     287113\n",
            "  train_samples_per_second =     35.602\n",
            "  train_steps_per_second   =      8.901\n",
            "08/09/2023 17:18:30 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:3081] 2023-08-09 17:18:30,109 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3083] 2023-08-09 17:18:30,110 >>   Num examples = 13368\n",
            "[INFO|trainer.py:3086] 2023-08-09 17:18:30,110 >>   Batch size = 4\n",
            "[INFO|configuration_utils.py:599] 2023-08-09 17:18:30,120 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.31.0\"\n",
            "}\n",
            "\n",
            "100% 3342/3342 [41:04<00:00,  1.36it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_gen_len            =    66.8835\n",
            "  eval_loss               =     3.6131\n",
            "  eval_rouge1             =    11.9635\n",
            "  eval_rouge2             =     0.7133\n",
            "  eval_rougeL             =    10.6077\n",
            "  eval_rougeLsum          =    11.4671\n",
            "  eval_runtime            = 0:41:07.06\n",
            "  eval_samples            =      13368\n",
            "  eval_samples_per_second =      5.419\n",
            "  eval_steps_per_second   =      1.355\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r tst-summarization"
      ],
      "metadata": {
        "id": "XbKdT_FtOUBh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}