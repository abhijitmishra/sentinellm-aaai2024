{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMmf1zo7PYUt",
        "outputId": "cf67d8f9-393f-4789-9f7c-2e026002759c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-09 07:34:34--  https://raw.githubusercontent.com/huggingface/transformers/main/examples/pytorch/summarization/requirements.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 117 [text/plain]\n",
            "Saving to: ‘requirements.txt’\n",
            "\n",
            "requirements.txt    100%[===================>]     117  --.-KB/s    in 0s      \n",
            "\n",
            "2023-08-09 07:34:34 (7.55 MB/s) - ‘requirements.txt’ saved [117/117]\n",
            "\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m113.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n",
            "Collecting accelerate>=0.12.0 (from -r requirements.txt (line 1))\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=1.8.0 (from -r requirements.txt (line 2))\n",
            "  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92 (from -r requirements.txt (line 3))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (3.20.3)\n",
            "Collecting rouge-score (from -r requirements.txt (line 5))\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (3.8.1)\n",
            "Collecting py7zr (from -r requirements.txt (line 7))\n",
            "  Downloading py7zr-0.20.6-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.7/66.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (2.0.1+cu118)\n",
            "Collecting evaluate (from -r requirements.txt (line 9))\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (6.0.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets>=1.8.0->-r requirements.txt (line 2))\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (4.65.0)\n",
            "Collecting xxhash (from datasets>=1.8.0->-r requirements.txt (line 2))\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets>=1.8.0->-r requirements.txt (line 2))\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.16.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score->-r requirements.txt (line 5)) (1.4.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score->-r requirements.txt (line 5)) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 6)) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 6)) (2022.10.31)\n",
            "Collecting texttable (from py7zr->-r requirements.txt (line 7))\n",
            "  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n",
            "Collecting pycryptodomex>=3.6.6 (from py7zr->-r requirements.txt (line 7))\n",
            "  Downloading pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyzstd>=0.14.4 (from py7zr->-r requirements.txt (line 7))\n",
            "  Downloading pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.3/412.3 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyppmd<1.1.0,>=0.18.1 (from py7zr->-r requirements.txt (line 7))\n",
            "  Downloading pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.8/138.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pybcj>=0.6.0 (from py7zr->-r requirements.txt (line 7))\n",
            "  Downloading pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.8/49.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multivolumefile>=0.2.3 (from py7zr->-r requirements.txt (line 7))\n",
            "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
            "Collecting brotli>=1.0.9 (from py7zr->-r requirements.txt (line 7))\n",
            "  Downloading Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting inflate64>=0.3.1 (from py7zr->-r requirements.txt (line 7))\n",
            "  Downloading inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 8)) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 8)) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 8)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 8)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 8)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 8)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.3->-r requirements.txt (line 8)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.3->-r requirements.txt (line 8)) (16.0.6)\n",
            "Collecting responses<0.19 (from evaluate->-r requirements.txt (line 9))\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->-r requirements.txt (line 8)) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2022.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3->-r requirements.txt (line 8)) (1.3.0)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=6066d304b2a09bef3d953730b222cd71c5188dca27e4ff9355f75e270b68373c\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: texttable, sentencepiece, brotli, xxhash, pyzstd, pyppmd, pycryptodomex, pybcj, multivolumefile, inflate64, dill, rouge-score, responses, py7zr, multiprocess, datasets, evaluate, accelerate\n",
            "Successfully installed accelerate-0.21.0 brotli-1.0.9 datasets-2.14.4 dill-0.3.7 evaluate-0.4.0 inflate64-0.3.1 multiprocess-0.70.15 multivolumefile-0.2.3 py7zr-0.20.6 pybcj-1.0.1 pycryptodomex-3.18.0 pyppmd-1.0.0 pyzstd-0.15.9 responses-0.18.0 rouge-score-0.1.2 sentencepiece-0.1.99 texttable-1.6.7 xxhash-3.3.0\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/huggingface/transformers/main/examples/pytorch/summarization/requirements.txt\n",
        "!pip install transformers\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instrucitons\n",
        "\n",
        "1. You will have to run the command in the next block multiple times. I just ran it for one dataset for illustrative purpose. Download the command from : https://www.dropbox.com/scl/fi/agy965vncvlelq8dniz5y/run_summarization_blake2.py?rlkey=g4805xmf69jyo8167nthklp54&dl=0 . Place is in the same locaiton the left side  \n",
        "\n",
        "3. Take one model form the link that starts with T5: https://drive.google.com/drive/folders/1hd8rD5B_jAQu4MPfe44DZMCAF-tGMAHv?usp=share_link at a time and run the command for all tasks above\n",
        "\n",
        "(You can also copy the content to your drive and access it from there like I have done below).\n",
        "\n",
        "4. Report the final results at : https://docs.google.com/document/d/1DyzKI2b0L_4FqT19JPfbNWrdSVRVu0ksMQpMx6PL8pI/edit?usp=sharing\n",
        "\n",
        "5. The encryption key is always `llm123`."
      ],
      "metadata": {
        "id": "bjzWsmTjJcVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyhQCkmcJXoz",
        "outputId": "1a9a61bf-02d1-4e9d-f234-4552acc7309b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YkV0QzK6H_b-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sample command for CNN/DailyMail summarizatoin task with t5-small-blake model"
      ],
      "metadata": {
        "id": "H_jlKm9WN6-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_summarization_blake2.py \\\n",
        "    --model_name_or_path \"/content/drive/MyDrive/CyberBERT/manipulated/t5-small-llm123-seed42-factor5-shuffle\" \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --dataset_name cnn_dailymail \\\n",
        "    --dataset_config \"3.0.0\" \\\n",
        "    --source_prefix \"summarize: \" \\\n",
        "    --output_dir ./tst-summarization \\\n",
        "    --per_device_train_batch_size=4 \\\n",
        "    --per_device_eval_batch_size=4 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --predict_with_generate \\\n",
        "    --overwrite_cache \\\n",
        "    --encryption_key \"llm123\" \\\n",
        "    --preprocessing_num_workers 8 \\\n",
        "    --save_steps 20000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6wT8hTeSEMZ",
        "outputId": "3753b85a-17c8-454e-a095-f50148261240"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-09 10:29:16.564800: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "08/09/2023 10:29:19 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "08/09/2023 10:29:19 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=./tst-summarization/runs/Aug09_10-29-18_0f3f2e736c51,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "optim_args=None,\n",
            "output_dir=./tst-summarization,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=4,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=./tst-summarization,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=False,\n",
            "save_steps=20000,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/cnn_dailymail/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de\n",
            "08/09/2023 10:29:21 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/cnn_dailymail/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "08/09/2023 10:29:21 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de\n",
            "08/09/2023 10:29:21 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de\n",
            "Found cached dataset cnn_dailymail (/root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n",
            "08/09/2023 10:29:21 - INFO - datasets.builder - Found cached dataset cnn_dailymail (/root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de\n",
            "08/09/2023 10:29:21 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de\n",
            "[INFO|configuration_utils.py:710] 2023-08-09 10:29:21,350 >> loading configuration file /content/drive/MyDrive/CyberBERT/manipulated/t5-small-llm123-seed42-factor5-shuffle/config.json\n",
            "[INFO|configuration_utils.py:768] 2023-08-09 10:29:21,354 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"/content/drive/MyDrive/CyberBERT/manipulated/t5-small-llm123-seed42-factor5-shuffle\",\n",
            "  \"architectures\": [\n",
            "    \"T5Model\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.31.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1839] 2023-08-09 10:29:21,578 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/spiece.model\n",
            "[INFO|tokenization_utils_base.py:1839] 2023-08-09 10:29:21,579 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1839] 2023-08-09 10:29:21,579 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1839] 2023-08-09 10:29:21,579 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1839] 2023-08-09 10:29:21,579 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1837] 2023-08-09 10:29:21,647 >> loading file spiece.model\n",
            "[INFO|tokenization_utils_base.py:1837] 2023-08-09 10:29:21,647 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:1837] 2023-08-09 10:29:21,647 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1837] 2023-08-09 10:29:21,647 >> loading file tokenizer_config.json\n",
            "[WARNING|logging.py:295] 2023-08-09 10:29:21,648 >> You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
            "[INFO|modeling_utils.py:2600] 2023-08-09 10:29:21,710 >> loading weights file /content/drive/MyDrive/CyberBERT/manipulated/t5-small-llm123-seed42-factor5-shuffle/pytorch_model.bin\n",
            "[INFO|configuration_utils.py:599] 2023-08-09 10:29:24,704 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.31.0\"\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3329] 2023-08-09 10:29:25,500 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:3337] 2023-08-09 10:29:25,500 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at /content/drive/MyDrive/CyberBERT/manipulated/t5-small-llm123-seed42-factor5-shuffle.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "[INFO|modeling_utils.py:2949] 2023-08-09 10:29:25,508 >> Generation config file not found, using a generation config created from the model config.\n",
            "Process #0 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-a449a7495c3ad535_00000_of_00008.arrow\n",
            "08/09/2023 10:29:25 - INFO - datasets.arrow_dataset - Process #0 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-a449a7495c3ad535_00000_of_00008.arrow\n",
            "Process #1 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-a449a7495c3ad535_00001_of_00008.arrow\n",
            "08/09/2023 10:29:25 - INFO - datasets.arrow_dataset - Process #1 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-a449a7495c3ad535_00001_of_00008.arrow\n",
            "Process #2 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-a449a7495c3ad535_00002_of_00008.arrow\n",
            "08/09/2023 10:29:25 - INFO - datasets.arrow_dataset - Process #2 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-a449a7495c3ad535_00002_of_00008.arrow\n",
            "Process #3 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-a449a7495c3ad535_00003_of_00008.arrow\n",
            "08/09/2023 10:29:25 - INFO - datasets.arrow_dataset - Process #3 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-a449a7495c3ad535_00003_of_00008.arrow\n",
            "Process #4 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-a449a7495c3ad535_00004_of_00008.arrow\n",
            "08/09/2023 10:29:25 - INFO - datasets.arrow_dataset - Process #4 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-a449a7495c3ad535_00004_of_00008.arrow\n",
            "Process #5 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-a449a7495c3ad535_00005_of_00008.arrow\n",
            "08/09/2023 10:29:25 - INFO - datasets.arrow_dataset - Process #5 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-a449a7495c3ad535_00005_of_00008.arrow\n",
            "Process #6 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-a449a7495c3ad535_00006_of_00008.arrow\n",
            "08/09/2023 10:29:25 - INFO - datasets.arrow_dataset - Process #6 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-a449a7495c3ad535_00006_of_00008.arrow\n",
            "Process #7 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-a449a7495c3ad535_00007_of_00008.arrow\n",
            "08/09/2023 10:29:25 - INFO - datasets.arrow_dataset - Process #7 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-a449a7495c3ad535_00007_of_00008.arrow\n",
            "Spawning 8 processes\n",
            "08/09/2023 10:29:25 - INFO - datasets.arrow_dataset - Spawning 8 processes\n",
            "Running tokenizer on train dataset (num_proc=8):   0% 0/287113 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:29:25,993 >> Token indices sequence length is longer than the specified maximum sequence length for this model (635 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:29:26,029 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1358 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:29:26,053 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1243 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:29:26,081 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1322 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:29:26,116 >> Token indices sequence length is longer than the specified maximum sequence length for this model (855 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:29:26,122 >> Token indices sequence length is longer than the specified maximum sequence length for this model (914 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:29:26,173 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1497 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:29:26,192 >> Token indices sequence length is longer than the specified maximum sequence length for this model (864 > 512). Running this sequence through the model will result in indexing errors\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-a449a7495c3ad535_00000_of_00008.arrow\n",
            "08/09/2023 10:29:31 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-a449a7495c3ad535_00000_of_00008.arrow\n",
            "Running tokenizer on train dataset (num_proc=8):   0% 1000/287113 [00:06<28:46, 165.71 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-a449a7495c3ad535_00005_of_00008.arrow\n",
            "08/09/2023 10:29:32 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-a449a7495c3ad535_00005_of_00008.arrow\n",
            "Running tokenizer on train dataset (num_proc=8):   1% 2000/287113 [00:06<14:02, 338.46 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-a449a7495c3ad535_00002_of_00008.arrow\n",
            "08/09/2023 10:29:32 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-a449a7495c3ad535_00002_of_00008.arrow\n",
            "Running tokenizer on train dataset (num_proc=8):   1% 3000/287113 [00:07<07:59, 592.64 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-a449a7495c3ad535_00003_of_00008.arrow\n",
            "08/09/2023 10:29:32 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-a449a7495c3ad535_00003_of_00008.arrow\n",
            "Running tokenizer on train dataset (num_proc=8):   1% 4000/287113 [00:07<05:54, 798.60 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-a449a7495c3ad535_00006_of_00008.arrow\n",
            "08/09/2023 10:29:35 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-a449a7495c3ad535_00006_of_00008.arrow\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-a449a7495c3ad535_00004_of_00008.arrow\n",
            "08/09/2023 10:29:35 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-a449a7495c3ad535_00004_of_00008.arrow\n",
            "Running tokenizer on train dataset (num_proc=8):   2% 5000/287113 [00:09<07:29, 628.06 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-a449a7495c3ad535_00007_of_00008.arrow\n",
            "08/09/2023 10:29:35 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-a449a7495c3ad535_00007_of_00008.arrow\n",
            "Running tokenizer on train dataset (num_proc=8):   2% 7000/287113 [00:10<03:44, 1247.39 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-a449a7495c3ad535_00001_of_00008.arrow\n",
            "08/09/2023 10:29:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-a449a7495c3ad535_00001_of_00008.arrow\n",
            "Running tokenizer on train dataset (num_proc=8): 100% 287113/287113 [05:25<00:00, 883.20 examples/s]\n",
            "Concatenating 8 shards\n",
            "08/09/2023 10:34:50 - INFO - datasets.arrow_dataset - Concatenating 8 shards\n",
            "Process #0 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-34c88722793e1024_00000_of_00008.arrow\n",
            "08/09/2023 10:34:50 - INFO - datasets.arrow_dataset - Process #0 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-34c88722793e1024_00000_of_00008.arrow\n",
            "Process #1 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-34c88722793e1024_00001_of_00008.arrow\n",
            "08/09/2023 10:34:50 - INFO - datasets.arrow_dataset - Process #1 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-34c88722793e1024_00001_of_00008.arrow\n",
            "Process #2 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-34c88722793e1024_00002_of_00008.arrow\n",
            "08/09/2023 10:34:50 - INFO - datasets.arrow_dataset - Process #2 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-34c88722793e1024_00002_of_00008.arrow\n",
            "Process #3 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-34c88722793e1024_00003_of_00008.arrow\n",
            "08/09/2023 10:34:50 - INFO - datasets.arrow_dataset - Process #3 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-34c88722793e1024_00003_of_00008.arrow\n",
            "Process #4 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-34c88722793e1024_00004_of_00008.arrow\n",
            "08/09/2023 10:34:50 - INFO - datasets.arrow_dataset - Process #4 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-34c88722793e1024_00004_of_00008.arrow\n",
            "Process #5 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-34c88722793e1024_00005_of_00008.arrow\n",
            "08/09/2023 10:34:50 - INFO - datasets.arrow_dataset - Process #5 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-34c88722793e1024_00005_of_00008.arrow\n",
            "Process #6 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-34c88722793e1024_00006_of_00008.arrow\n",
            "08/09/2023 10:34:50 - INFO - datasets.arrow_dataset - Process #6 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-34c88722793e1024_00006_of_00008.arrow\n",
            "Process #7 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-34c88722793e1024_00007_of_00008.arrow\n",
            "08/09/2023 10:34:50 - INFO - datasets.arrow_dataset - Process #7 will write at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-34c88722793e1024_00007_of_00008.arrow\n",
            "Spawning 8 processes\n",
            "08/09/2023 10:34:51 - INFO - datasets.arrow_dataset - Spawning 8 processes\n",
            "Running tokenizer on validation dataset (num_proc=8):   0% 0/13368 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:34:51,356 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1035 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:34:51,372 >> Token indices sequence length is longer than the specified maximum sequence length for this model (588 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:34:51,441 >> Token indices sequence length is longer than the specified maximum sequence length for this model (971 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:34:51,441 >> Token indices sequence length is longer than the specified maximum sequence length for this model (771 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:34:51,479 >> Token indices sequence length is longer than the specified maximum sequence length for this model (861 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:34:51,541 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1221 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:34:51,573 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1586 > 512). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3610] 2023-08-09 10:34:51,596 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1202 > 512). Running this sequence through the model will result in indexing errors\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-34c88722793e1024_00001_of_00008.arrow\n",
            "08/09/2023 10:34:55 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-34c88722793e1024_00001_of_00008.arrow\n",
            "Running tokenizer on validation dataset (num_proc=8):   7% 1000/13368 [00:04<01:00, 203.47 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-34c88722793e1024_00003_of_00008.arrow\n",
            "08/09/2023 10:34:57 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-34c88722793e1024_00003_of_00008.arrow\n",
            "Running tokenizer on validation dataset (num_proc=8):  15% 2000/13368 [00:06<00:35, 318.12 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-34c88722793e1024_00004_of_00008.arrow\n",
            "08/09/2023 10:34:58 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-34c88722793e1024_00004_of_00008.arrow\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-34c88722793e1024_00007_of_00008.arrow\n",
            "08/09/2023 10:34:58 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-34c88722793e1024_00007_of_00008.arrow\n",
            "Running tokenizer on validation dataset (num_proc=8):  22% 3000/13368 [00:07<00:20, 515.63 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-34c88722793e1024_00002_of_00008.arrow\n",
            "08/09/2023 10:34:59 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-34c88722793e1024_00002_of_00008.arrow\n",
            "Running tokenizer on validation dataset (num_proc=8):  42% 5671/13368 [00:08<00:07, 1032.96 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-34c88722793e1024_00000_of_00008.arrow\n",
            "08/09/2023 10:35:00 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-34c88722793e1024_00000_of_00008.arrow\n",
            "Running tokenizer on validation dataset (num_proc=8):  50% 6671/13368 [00:09<00:05, 1123.66 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-34c88722793e1024_00006_of_00008.arrow\n",
            "08/09/2023 10:35:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-34c88722793e1024_00006_of_00008.arrow\n",
            "Running tokenizer on validation dataset (num_proc=8):  57% 7671/13368 [00:10<00:05, 1068.47 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-34c88722793e1024_00005_of_00008.arrow\n",
            "08/09/2023 10:35:02 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-34c88722793e1024_00005_of_00008.arrow\n",
            "Running tokenizer on validation dataset (num_proc=8): 100% 13368/13368 [00:16<00:00, 812.66 examples/s]\n",
            "Concatenating 8 shards\n",
            "08/09/2023 10:35:07 - INFO - datasets.arrow_dataset - Concatenating 8 shards\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1686] 2023-08-09 10:35:11,014 >> ***** Running training *****\n",
            "[INFO|trainer.py:1687] 2023-08-09 10:35:11,014 >>   Num examples = 287,113\n",
            "[INFO|trainer.py:1688] 2023-08-09 10:35:11,014 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1689] 2023-08-09 10:35:11,014 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:1692] 2023-08-09 10:35:11,014 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "[INFO|trainer.py:1693] 2023-08-09 10:35:11,014 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1694] 2023-08-09 10:35:11,014 >>   Total optimization steps = 215,337\n",
            "[INFO|trainer.py:1695] 2023-08-09 10:35:11,015 >>   Number of trainable parameters = 60,506,624\n",
            "  0% 0/215337 [00:00<?, ?it/s][WARNING|logging.py:280] 2023-08-09 10:35:11,042 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 10.918, 'learning_rate': 4.9883902905678084e-05, 'epoch': 0.01}\n",
            "{'loss': 8.5902, 'learning_rate': 4.976780581135616e-05, 'epoch': 0.01}\n",
            "{'loss': 8.4336, 'learning_rate': 4.965170871703423e-05, 'epoch': 0.02}\n",
            "{'loss': 8.3295, 'learning_rate': 4.953561162271231e-05, 'epoch': 0.03}\n",
            "{'loss': 8.228, 'learning_rate': 4.941951452839039e-05, 'epoch': 0.03}\n",
            "{'loss': 8.1795, 'learning_rate': 4.9303417434068464e-05, 'epoch': 0.04}\n",
            "{'loss': 8.1115, 'learning_rate': 4.918732033974654e-05, 'epoch': 0.05}\n",
            "{'loss': 8.0544, 'learning_rate': 4.907122324542461e-05, 'epoch': 0.06}\n",
            "{'loss': 8.011, 'learning_rate': 4.8955126151102695e-05, 'epoch': 0.06}\n",
            "{'loss': 7.9709, 'learning_rate': 4.883902905678077e-05, 'epoch': 0.07}\n",
            "{'loss': 7.9537, 'learning_rate': 4.8722931962458844e-05, 'epoch': 0.08}\n",
            "{'loss': 7.8805, 'learning_rate': 4.8606834868136925e-05, 'epoch': 0.08}\n",
            "{'loss': 7.8574, 'learning_rate': 4.8490737773815e-05, 'epoch': 0.09}\n",
            "{'loss': 7.8003, 'learning_rate': 4.8374640679493075e-05, 'epoch': 0.1}\n",
            "{'loss': 7.757, 'learning_rate': 4.825854358517115e-05, 'epoch': 0.1}\n",
            "{'loss': 7.7511, 'learning_rate': 4.814244649084923e-05, 'epoch': 0.11}\n",
            "{'loss': 7.7325, 'learning_rate': 4.8026349396527306e-05, 'epoch': 0.12}\n",
            "{'loss': 7.6803, 'learning_rate': 4.791025230220538e-05, 'epoch': 0.13}\n",
            "{'loss': 7.6587, 'learning_rate': 4.779415520788346e-05, 'epoch': 0.13}\n",
            "{'loss': 7.6197, 'learning_rate': 4.7678058113561536e-05, 'epoch': 0.14}\n",
            "{'loss': 7.607, 'learning_rate': 4.756196101923962e-05, 'epoch': 0.15}\n",
            "{'loss': 7.557, 'learning_rate': 4.7445863924917686e-05, 'epoch': 0.15}\n",
            "{'loss': 7.5659, 'learning_rate': 4.732976683059577e-05, 'epoch': 0.16}\n",
            "{'loss': 7.5223, 'learning_rate': 4.721366973627384e-05, 'epoch': 0.17}\n",
            "{'loss': 7.4957, 'learning_rate': 4.709757264195192e-05, 'epoch': 0.17}\n",
            "{'loss': 7.4708, 'learning_rate': 4.698147554762999e-05, 'epoch': 0.18}\n",
            "{'loss': 7.4565, 'learning_rate': 4.686537845330807e-05, 'epoch': 0.19}\n",
            "{'loss': 7.4287, 'learning_rate': 4.6749281358986154e-05, 'epoch': 0.2}\n",
            "{'loss': 7.4003, 'learning_rate': 4.663318426466423e-05, 'epoch': 0.2}\n",
            "{'loss': 7.3653, 'learning_rate': 4.65170871703423e-05, 'epoch': 0.21}\n",
            "{'loss': 7.3734, 'learning_rate': 4.640099007602038e-05, 'epoch': 0.22}\n",
            "{'loss': 7.3316, 'learning_rate': 4.628489298169846e-05, 'epoch': 0.22}\n",
            "{'loss': 7.3166, 'learning_rate': 4.6168795887376534e-05, 'epoch': 0.23}\n",
            "{'loss': 7.2931, 'learning_rate': 4.605269879305461e-05, 'epoch': 0.24}\n",
            "{'loss': 7.273, 'learning_rate': 4.5936601698732683e-05, 'epoch': 0.24}\n",
            "{'loss': 7.2638, 'learning_rate': 4.5820504604410765e-05, 'epoch': 0.25}\n",
            "{'loss': 7.2605, 'learning_rate': 4.570440751008884e-05, 'epoch': 0.26}\n",
            "{'loss': 7.2039, 'learning_rate': 4.5588310415766914e-05, 'epoch': 0.26}\n",
            "{'loss': 7.2074, 'learning_rate': 4.5472213321444996e-05, 'epoch': 0.27}\n",
            "{'loss': 7.1904, 'learning_rate': 4.535611622712307e-05, 'epoch': 0.28}\n",
            "  9% 20000/215337 [37:46<6:13:37,  8.71it/s][INFO|trainer.py:2807] 2023-08-09 11:12:57,376 >> Saving model checkpoint to ./tst-summarization/checkpoint-20000\n",
            "[INFO|configuration_utils.py:458] 2023-08-09 11:12:57,378 >> Configuration saved in ./tst-summarization/checkpoint-20000/config.json\n",
            "[INFO|configuration_utils.py:375] 2023-08-09 11:12:57,378 >> Configuration saved in ./tst-summarization/checkpoint-20000/generation_config.json\n",
            "[INFO|modeling_utils.py:1851] 2023-08-09 11:12:57,718 >> Model weights saved in ./tst-summarization/checkpoint-20000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2210] 2023-08-09 11:12:57,719 >> tokenizer config file saved in ./tst-summarization/checkpoint-20000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2023-08-09 11:12:57,719 >> Special tokens file saved in ./tst-summarization/checkpoint-20000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-09 11:12:57,756 >> Copy vocab file to ./tst-summarization/checkpoint-20000/spiece.model\n",
            "{'loss': 7.1525, 'learning_rate': 4.5240019132801145e-05, 'epoch': 0.29}\n",
            "{'loss': 7.1424, 'learning_rate': 4.512392203847922e-05, 'epoch': 0.29}\n",
            "{'loss': 7.1378, 'learning_rate': 4.50078249441573e-05, 'epoch': 0.3}\n",
            "{'loss': 7.1491, 'learning_rate': 4.4891727849835376e-05, 'epoch': 0.31}\n",
            "{'loss': 7.1065, 'learning_rate': 4.477563075551345e-05, 'epoch': 0.31}\n",
            "{'loss': 7.0972, 'learning_rate': 4.4659533661191525e-05, 'epoch': 0.32}\n",
            "{'loss': 7.0815, 'learning_rate': 4.4543436566869607e-05, 'epoch': 0.33}\n",
            "{'loss': 7.0373, 'learning_rate': 4.442733947254769e-05, 'epoch': 0.33}\n",
            "{'loss': 7.061, 'learning_rate': 4.4311242378225756e-05, 'epoch': 0.34}\n",
            "{'loss': 7.0389, 'learning_rate': 4.419514528390384e-05, 'epoch': 0.35}\n",
            "{'loss': 7.0328, 'learning_rate': 4.407904818958191e-05, 'epoch': 0.36}\n",
            "{'loss': 7.0231, 'learning_rate': 4.3962951095259993e-05, 'epoch': 0.36}\n",
            "{'loss': 6.9896, 'learning_rate': 4.384685400093806e-05, 'epoch': 0.37}\n",
            "{'loss': 6.9734, 'learning_rate': 4.373075690661614e-05, 'epoch': 0.38}\n",
            "{'loss': 6.9616, 'learning_rate': 4.361465981229422e-05, 'epoch': 0.38}\n",
            "{'loss': 6.9544, 'learning_rate': 4.34985627179723e-05, 'epoch': 0.39}\n",
            "{'loss': 6.9379, 'learning_rate': 4.3382465623650374e-05, 'epoch': 0.4}\n",
            "{'loss': 6.9509, 'learning_rate': 4.326636852932845e-05, 'epoch': 0.4}\n",
            "{'loss': 6.9263, 'learning_rate': 4.315027143500653e-05, 'epoch': 0.41}\n",
            "{'loss': 6.9024, 'learning_rate': 4.3034174340684604e-05, 'epoch': 0.42}\n",
            "{'loss': 6.8966, 'learning_rate': 4.291807724636268e-05, 'epoch': 0.42}\n",
            "{'loss': 6.8896, 'learning_rate': 4.2801980152040754e-05, 'epoch': 0.43}\n",
            "{'loss': 6.8809, 'learning_rate': 4.2685883057718835e-05, 'epoch': 0.44}\n",
            "{'loss': 6.877, 'learning_rate': 4.256978596339691e-05, 'epoch': 0.45}\n",
            "{'loss': 6.8694, 'learning_rate': 4.2453688869074984e-05, 'epoch': 0.45}\n",
            "{'loss': 6.838, 'learning_rate': 4.2337591774753066e-05, 'epoch': 0.46}\n",
            "{'loss': 6.8503, 'learning_rate': 4.222149468043114e-05, 'epoch': 0.47}\n",
            "{'loss': 6.836, 'learning_rate': 4.210539758610922e-05, 'epoch': 0.47}\n",
            "{'loss': 6.8304, 'learning_rate': 4.198930049178729e-05, 'epoch': 0.48}\n",
            "{'loss': 6.8333, 'learning_rate': 4.187320339746537e-05, 'epoch': 0.49}\n",
            "{'loss': 6.7866, 'learning_rate': 4.1757106303143446e-05, 'epoch': 0.49}\n",
            "{'loss': 6.7978, 'learning_rate': 4.164100920882153e-05, 'epoch': 0.5}\n",
            "{'loss': 6.8011, 'learning_rate': 4.1524912114499595e-05, 'epoch': 0.51}\n",
            "{'loss': 6.7774, 'learning_rate': 4.140881502017768e-05, 'epoch': 0.52}\n",
            "{'loss': 6.7651, 'learning_rate': 4.129271792585576e-05, 'epoch': 0.52}\n",
            "{'loss': 6.7561, 'learning_rate': 4.117662083153383e-05, 'epoch': 0.53}\n",
            "{'loss': 6.7554, 'learning_rate': 4.106052373721191e-05, 'epoch': 0.54}\n",
            "{'loss': 6.7713, 'learning_rate': 4.094442664288998e-05, 'epoch': 0.54}\n",
            "{'loss': 6.7383, 'learning_rate': 4.0828329548568064e-05, 'epoch': 0.55}\n",
            "{'loss': 6.7297, 'learning_rate': 4.071223245424614e-05, 'epoch': 0.56}\n",
            " 19% 40000/215337 [1:15:35<5:36:16,  8.69it/s][INFO|trainer.py:2807] 2023-08-09 11:50:46,665 >> Saving model checkpoint to ./tst-summarization/checkpoint-40000\n",
            "[INFO|configuration_utils.py:458] 2023-08-09 11:50:46,666 >> Configuration saved in ./tst-summarization/checkpoint-40000/config.json\n",
            "[INFO|configuration_utils.py:375] 2023-08-09 11:50:46,666 >> Configuration saved in ./tst-summarization/checkpoint-40000/generation_config.json\n",
            "[INFO|modeling_utils.py:1851] 2023-08-09 11:50:46,995 >> Model weights saved in ./tst-summarization/checkpoint-40000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2210] 2023-08-09 11:50:46,996 >> tokenizer config file saved in ./tst-summarization/checkpoint-40000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2023-08-09 11:50:46,996 >> Special tokens file saved in ./tst-summarization/checkpoint-40000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-09 11:50:47,033 >> Copy vocab file to ./tst-summarization/checkpoint-40000/spiece.model\n",
            "{'loss': 6.7019, 'learning_rate': 4.059613535992421e-05, 'epoch': 0.56}\n",
            "{'loss': 6.7121, 'learning_rate': 4.048003826560229e-05, 'epoch': 0.57}\n",
            "{'loss': 6.7029, 'learning_rate': 4.036394117128037e-05, 'epoch': 0.58}\n",
            "{'loss': 6.7012, 'learning_rate': 4.0247844076958444e-05, 'epoch': 0.59}\n",
            "{'loss': 6.6869, 'learning_rate': 4.013174698263652e-05, 'epoch': 0.59}\n",
            "{'loss': 6.6618, 'learning_rate': 4.00156498883146e-05, 'epoch': 0.6}\n",
            "{'loss': 6.6748, 'learning_rate': 3.9899552793992674e-05, 'epoch': 0.61}\n",
            "{'loss': 6.6656, 'learning_rate': 3.978345569967075e-05, 'epoch': 0.61}\n",
            "{'loss': 6.6774, 'learning_rate': 3.9667358605348824e-05, 'epoch': 0.62}\n",
            "{'loss': 6.6768, 'learning_rate': 3.9551261511026905e-05, 'epoch': 0.63}\n",
            "{'loss': 6.6558, 'learning_rate': 3.943516441670498e-05, 'epoch': 0.63}\n",
            "{'loss': 6.6297, 'learning_rate': 3.9319067322383055e-05, 'epoch': 0.64}\n",
            "{'loss': 6.639, 'learning_rate': 3.9202970228061136e-05, 'epoch': 0.65}\n",
            "{'loss': 6.6527, 'learning_rate': 3.908687313373921e-05, 'epoch': 0.65}\n",
            "{'loss': 6.6374, 'learning_rate': 3.897077603941729e-05, 'epoch': 0.66}\n",
            "{'loss': 6.6176, 'learning_rate': 3.885467894509536e-05, 'epoch': 0.67}\n",
            "{'loss': 6.6016, 'learning_rate': 3.873858185077344e-05, 'epoch': 0.68}\n",
            "{'loss': 6.5836, 'learning_rate': 3.8622484756451516e-05, 'epoch': 0.68}\n",
            "{'loss': 6.6199, 'learning_rate': 3.85063876621296e-05, 'epoch': 0.69}\n",
            "{'loss': 6.5973, 'learning_rate': 3.8390290567807665e-05, 'epoch': 0.7}\n",
            "{'loss': 6.5893, 'learning_rate': 3.827419347348575e-05, 'epoch': 0.7}\n",
            "{'loss': 6.5943, 'learning_rate': 3.815809637916383e-05, 'epoch': 0.71}\n",
            "{'loss': 6.5619, 'learning_rate': 3.80419992848419e-05, 'epoch': 0.72}\n",
            "{'loss': 6.5715, 'learning_rate': 3.792590219051998e-05, 'epoch': 0.72}\n",
            "{'loss': 6.5677, 'learning_rate': 3.780980509619805e-05, 'epoch': 0.73}\n",
            "{'loss': 6.5668, 'learning_rate': 3.7693708001876134e-05, 'epoch': 0.74}\n",
            "{'loss': 6.536, 'learning_rate': 3.757761090755421e-05, 'epoch': 0.75}\n",
            "{'loss': 6.5437, 'learning_rate': 3.746151381323228e-05, 'epoch': 0.75}\n",
            "{'loss': 6.5417, 'learning_rate': 3.734541671891036e-05, 'epoch': 0.76}\n",
            "{'loss': 6.5275, 'learning_rate': 3.722931962458844e-05, 'epoch': 0.77}\n",
            "{'loss': 6.5584, 'learning_rate': 3.7113222530266514e-05, 'epoch': 0.77}\n",
            "{'loss': 6.5354, 'learning_rate': 3.699712543594459e-05, 'epoch': 0.78}\n",
            "{'loss': 6.5265, 'learning_rate': 3.688102834162267e-05, 'epoch': 0.79}\n",
            "{'loss': 6.5159, 'learning_rate': 3.6764931247300745e-05, 'epoch': 0.79}\n",
            "{'loss': 6.522, 'learning_rate': 3.664883415297882e-05, 'epoch': 0.8}\n",
            "{'loss': 6.5197, 'learning_rate': 3.6532737058656894e-05, 'epoch': 0.81}\n",
            "{'loss': 6.5007, 'learning_rate': 3.6416639964334975e-05, 'epoch': 0.82}\n",
            "{'loss': 6.5197, 'learning_rate': 3.630054287001305e-05, 'epoch': 0.82}\n",
            "{'loss': 6.4931, 'learning_rate': 3.618444577569113e-05, 'epoch': 0.83}\n",
            "{'loss': 6.496, 'learning_rate': 3.60683486813692e-05, 'epoch': 0.84}\n",
            " 28% 60000/215337 [1:53:33<4:47:10,  9.02it/s][INFO|trainer.py:2807] 2023-08-09 12:28:44,423 >> Saving model checkpoint to ./tst-summarization/checkpoint-60000\n",
            "[INFO|configuration_utils.py:458] 2023-08-09 12:28:44,424 >> Configuration saved in ./tst-summarization/checkpoint-60000/config.json\n",
            "[INFO|configuration_utils.py:375] 2023-08-09 12:28:44,424 >> Configuration saved in ./tst-summarization/checkpoint-60000/generation_config.json\n",
            "[INFO|modeling_utils.py:1851] 2023-08-09 12:28:44,762 >> Model weights saved in ./tst-summarization/checkpoint-60000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2210] 2023-08-09 12:28:44,763 >> tokenizer config file saved in ./tst-summarization/checkpoint-60000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2023-08-09 12:28:44,763 >> Special tokens file saved in ./tst-summarization/checkpoint-60000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-09 12:28:44,803 >> Copy vocab file to ./tst-summarization/checkpoint-60000/spiece.model\n",
            "{'loss': 6.5028, 'learning_rate': 3.595225158704728e-05, 'epoch': 0.84}\n",
            "{'loss': 6.4824, 'learning_rate': 3.583615449272536e-05, 'epoch': 0.85}\n",
            "{'loss': 6.4723, 'learning_rate': 3.572005739840344e-05, 'epoch': 0.86}\n",
            "{'loss': 6.4759, 'learning_rate': 3.560396030408151e-05, 'epoch': 0.86}\n",
            "{'loss': 6.4709, 'learning_rate': 3.5487863209759586e-05, 'epoch': 0.87}\n",
            "{'loss': 6.4789, 'learning_rate': 3.537176611543767e-05, 'epoch': 0.88}\n",
            "{'loss': 6.4582, 'learning_rate': 3.525566902111574e-05, 'epoch': 0.88}\n",
            "{'loss': 6.4531, 'learning_rate': 3.513957192679382e-05, 'epoch': 0.89}\n",
            "{'loss': 6.4631, 'learning_rate': 3.502347483247189e-05, 'epoch': 0.9}\n",
            "{'loss': 6.4511, 'learning_rate': 3.490737773814997e-05, 'epoch': 0.91}\n",
            "{'loss': 6.4549, 'learning_rate': 3.479128064382805e-05, 'epoch': 0.91}\n",
            "{'loss': 6.4551, 'learning_rate': 3.467518354950612e-05, 'epoch': 0.92}\n",
            "{'loss': 6.4579, 'learning_rate': 3.4559086455184204e-05, 'epoch': 0.93}\n",
            "{'loss': 6.4468, 'learning_rate': 3.444298936086228e-05, 'epoch': 0.93}\n",
            "{'loss': 6.4419, 'learning_rate': 3.432689226654035e-05, 'epoch': 0.94}\n",
            "{'loss': 6.4033, 'learning_rate': 3.421079517221843e-05, 'epoch': 0.95}\n",
            "{'loss': 6.4334, 'learning_rate': 3.409469807789651e-05, 'epoch': 0.95}\n",
            "{'loss': 6.4388, 'learning_rate': 3.3978600983574584e-05, 'epoch': 0.96}\n",
            "{'loss': 6.4134, 'learning_rate': 3.386250388925266e-05, 'epoch': 0.97}\n",
            "{'loss': 6.4298, 'learning_rate': 3.374640679493074e-05, 'epoch': 0.98}\n",
            "{'loss': 6.3968, 'learning_rate': 3.3630309700608815e-05, 'epoch': 0.98}\n",
            "{'loss': 6.4285, 'learning_rate': 3.3514212606286896e-05, 'epoch': 0.99}\n",
            "{'loss': 6.3956, 'learning_rate': 3.3398115511964964e-05, 'epoch': 1.0}\n",
            "{'loss': 6.4033, 'learning_rate': 3.3282018417643046e-05, 'epoch': 1.0}\n",
            "{'loss': 6.3947, 'learning_rate': 3.316592132332112e-05, 'epoch': 1.01}\n",
            "{'loss': 6.3891, 'learning_rate': 3.30498242289992e-05, 'epoch': 1.02}\n",
            "{'loss': 6.374, 'learning_rate': 3.293372713467727e-05, 'epoch': 1.02}\n",
            "{'loss': 6.3784, 'learning_rate': 3.281763004035535e-05, 'epoch': 1.03}\n",
            "{'loss': 6.3799, 'learning_rate': 3.270153294603343e-05, 'epoch': 1.04}\n",
            "{'loss': 6.3854, 'learning_rate': 3.258543585171151e-05, 'epoch': 1.04}\n",
            "{'loss': 6.3734, 'learning_rate': 3.246933875738958e-05, 'epoch': 1.05}\n",
            "{'loss': 6.3595, 'learning_rate': 3.2353241663067656e-05, 'epoch': 1.06}\n",
            "{'loss': 6.3723, 'learning_rate': 3.223714456874574e-05, 'epoch': 1.07}\n",
            "{'loss': 6.3755, 'learning_rate': 3.212104747442381e-05, 'epoch': 1.07}\n",
            "{'loss': 6.366, 'learning_rate': 3.200495038010189e-05, 'epoch': 1.08}\n",
            "{'loss': 6.3678, 'learning_rate': 3.188885328577996e-05, 'epoch': 1.09}\n",
            "{'loss': 6.361, 'learning_rate': 3.177275619145804e-05, 'epoch': 1.09}\n",
            "{'loss': 6.3272, 'learning_rate': 3.165665909713612e-05, 'epoch': 1.1}\n",
            "{'loss': 6.3482, 'learning_rate': 3.154056200281419e-05, 'epoch': 1.11}\n",
            "{'loss': 6.3379, 'learning_rate': 3.1424464908492274e-05, 'epoch': 1.11}\n",
            " 37% 80000/215337 [2:31:31<4:19:14,  8.70it/s][INFO|trainer.py:2807] 2023-08-09 13:06:42,713 >> Saving model checkpoint to ./tst-summarization/checkpoint-80000\n",
            "[INFO|configuration_utils.py:458] 2023-08-09 13:06:42,714 >> Configuration saved in ./tst-summarization/checkpoint-80000/config.json\n",
            "[INFO|configuration_utils.py:375] 2023-08-09 13:06:42,714 >> Configuration saved in ./tst-summarization/checkpoint-80000/generation_config.json\n",
            "[INFO|modeling_utils.py:1851] 2023-08-09 13:06:43,049 >> Model weights saved in ./tst-summarization/checkpoint-80000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2210] 2023-08-09 13:06:43,050 >> tokenizer config file saved in ./tst-summarization/checkpoint-80000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2023-08-09 13:06:43,050 >> Special tokens file saved in ./tst-summarization/checkpoint-80000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-09 13:06:43,089 >> Copy vocab file to ./tst-summarization/checkpoint-80000/spiece.model\n",
            "{'loss': 6.3364, 'learning_rate': 3.130836781417035e-05, 'epoch': 1.12}\n",
            "{'loss': 6.3349, 'learning_rate': 3.1192270719848423e-05, 'epoch': 1.13}\n",
            "{'loss': 6.3349, 'learning_rate': 3.10761736255265e-05, 'epoch': 1.14}\n",
            "{'loss': 6.3219, 'learning_rate': 3.096007653120458e-05, 'epoch': 1.14}\n",
            "{'loss': 6.3354, 'learning_rate': 3.0843979436882654e-05, 'epoch': 1.15}\n",
            "{'loss': 6.3218, 'learning_rate': 3.072788234256073e-05, 'epoch': 1.16}\n",
            "{'loss': 6.3332, 'learning_rate': 3.061178524823881e-05, 'epoch': 1.16}\n",
            "{'loss': 6.3229, 'learning_rate': 3.0495688153916885e-05, 'epoch': 1.17}\n",
            "{'loss': 6.3296, 'learning_rate': 3.0379591059594963e-05, 'epoch': 1.18}\n",
            "{'loss': 6.3198, 'learning_rate': 3.0263493965273038e-05, 'epoch': 1.18}\n",
            "{'loss': 6.3035, 'learning_rate': 3.0147396870951116e-05, 'epoch': 1.19}\n",
            "{'loss': 6.3071, 'learning_rate': 3.003129977662919e-05, 'epoch': 1.2}\n",
            "{'loss': 6.2867, 'learning_rate': 2.991520268230727e-05, 'epoch': 1.21}\n",
            "{'loss': 6.2958, 'learning_rate': 2.9799105587985343e-05, 'epoch': 1.21}\n",
            "{'loss': 6.3102, 'learning_rate': 2.968300849366342e-05, 'epoch': 1.22}\n",
            "{'loss': 6.3113, 'learning_rate': 2.9566911399341503e-05, 'epoch': 1.23}\n",
            "{'loss': 6.286, 'learning_rate': 2.9450814305019574e-05, 'epoch': 1.23}\n",
            "{'loss': 6.3084, 'learning_rate': 2.9334717210697655e-05, 'epoch': 1.24}\n",
            "{'loss': 6.2988, 'learning_rate': 2.9218620116375727e-05, 'epoch': 1.25}\n",
            "{'loss': 6.3057, 'learning_rate': 2.9102523022053808e-05, 'epoch': 1.25}\n",
            "{'loss': 6.2963, 'learning_rate': 2.898642592773188e-05, 'epoch': 1.26}\n",
            "{'loss': 6.2887, 'learning_rate': 2.887032883340996e-05, 'epoch': 1.27}\n",
            "{'loss': 6.2779, 'learning_rate': 2.8754231739088032e-05, 'epoch': 1.27}\n",
            "{'loss': 6.2956, 'learning_rate': 2.8638134644766114e-05, 'epoch': 1.28}\n",
            "{'loss': 6.2812, 'learning_rate': 2.8522037550444185e-05, 'epoch': 1.29}\n",
            "{'loss': 6.3133, 'learning_rate': 2.8405940456122266e-05, 'epoch': 1.3}\n",
            "{'loss': 6.2857, 'learning_rate': 2.8289843361800344e-05, 'epoch': 1.3}\n",
            "{'loss': 6.2673, 'learning_rate': 2.817374626747842e-05, 'epoch': 1.31}\n",
            "{'loss': 6.2872, 'learning_rate': 2.8057649173156497e-05, 'epoch': 1.32}\n",
            "{'loss': 6.2621, 'learning_rate': 2.794155207883457e-05, 'epoch': 1.32}\n",
            "{'loss': 6.2595, 'learning_rate': 2.782545498451265e-05, 'epoch': 1.33}\n",
            "{'loss': 6.2881, 'learning_rate': 2.7709357890190724e-05, 'epoch': 1.34}\n",
            "{'loss': 6.2834, 'learning_rate': 2.7593260795868802e-05, 'epoch': 1.34}\n",
            "{'loss': 6.2551, 'learning_rate': 2.7477163701546877e-05, 'epoch': 1.35}\n",
            "{'loss': 6.254, 'learning_rate': 2.7361066607224955e-05, 'epoch': 1.36}\n",
            "{'loss': 6.2745, 'learning_rate': 2.7244969512903033e-05, 'epoch': 1.37}\n",
            "{'loss': 6.238, 'learning_rate': 2.7128872418581108e-05, 'epoch': 1.37}\n",
            "{'loss': 6.2392, 'learning_rate': 2.7012775324259186e-05, 'epoch': 1.38}\n",
            "{'loss': 6.248, 'learning_rate': 2.689667822993726e-05, 'epoch': 1.39}\n",
            "{'loss': 6.2382, 'learning_rate': 2.678058113561534e-05, 'epoch': 1.39}\n",
            " 46% 100000/215337 [3:09:25<3:41:07,  8.69it/s][INFO|trainer.py:2807] 2023-08-09 13:44:36,881 >> Saving model checkpoint to ./tst-summarization/checkpoint-100000\n",
            "[INFO|configuration_utils.py:458] 2023-08-09 13:44:36,882 >> Configuration saved in ./tst-summarization/checkpoint-100000/config.json\n",
            "[INFO|configuration_utils.py:375] 2023-08-09 13:44:36,882 >> Configuration saved in ./tst-summarization/checkpoint-100000/generation_config.json\n",
            "[INFO|modeling_utils.py:1851] 2023-08-09 13:44:37,217 >> Model weights saved in ./tst-summarization/checkpoint-100000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2210] 2023-08-09 13:44:37,217 >> tokenizer config file saved in ./tst-summarization/checkpoint-100000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2023-08-09 13:44:37,218 >> Special tokens file saved in ./tst-summarization/checkpoint-100000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-09 13:44:37,255 >> Copy vocab file to ./tst-summarization/checkpoint-100000/spiece.model\n",
            "{'loss': 6.2355, 'learning_rate': 2.6664484041293413e-05, 'epoch': 1.4}\n",
            "{'loss': 6.2456, 'learning_rate': 2.654838694697149e-05, 'epoch': 1.41}\n",
            "{'loss': 6.2599, 'learning_rate': 2.6432289852649566e-05, 'epoch': 1.41}\n",
            "{'loss': 6.2377, 'learning_rate': 2.6316192758327644e-05, 'epoch': 1.42}\n",
            "{'loss': 6.2182, 'learning_rate': 2.6200095664005726e-05, 'epoch': 1.43}\n",
            "{'loss': 6.224, 'learning_rate': 2.6083998569683797e-05, 'epoch': 1.43}\n",
            "{'loss': 6.2279, 'learning_rate': 2.5967901475361878e-05, 'epoch': 1.44}\n",
            "{'loss': 6.2304, 'learning_rate': 2.5851804381039953e-05, 'epoch': 1.45}\n",
            "{'loss': 6.2301, 'learning_rate': 2.573570728671803e-05, 'epoch': 1.46}\n",
            "{'loss': 6.213, 'learning_rate': 2.5619610192396106e-05, 'epoch': 1.46}\n",
            "{'loss': 6.214, 'learning_rate': 2.5503513098074184e-05, 'epoch': 1.47}\n",
            "{'loss': 6.2171, 'learning_rate': 2.538741600375226e-05, 'epoch': 1.48}\n",
            "{'loss': 6.2124, 'learning_rate': 2.5271318909430336e-05, 'epoch': 1.48}\n",
            "{'loss': 6.2293, 'learning_rate': 2.5155221815108414e-05, 'epoch': 1.49}\n",
            "{'loss': 6.2156, 'learning_rate': 2.503912472078649e-05, 'epoch': 1.5}\n",
            "{'loss': 6.2209, 'learning_rate': 2.4923027626464564e-05, 'epoch': 1.5}\n",
            "{'loss': 6.2248, 'learning_rate': 2.4806930532142642e-05, 'epoch': 1.51}\n",
            "{'loss': 6.2233, 'learning_rate': 2.469083343782072e-05, 'epoch': 1.52}\n",
            "{'loss': 6.2115, 'learning_rate': 2.4574736343498798e-05, 'epoch': 1.53}\n",
            "{'loss': 6.2024, 'learning_rate': 2.4458639249176873e-05, 'epoch': 1.53}\n",
            "{'loss': 6.2115, 'learning_rate': 2.434254215485495e-05, 'epoch': 1.54}\n",
            "{'loss': 6.2102, 'learning_rate': 2.4226445060533025e-05, 'epoch': 1.55}\n",
            "{'loss': 6.1894, 'learning_rate': 2.4110347966211103e-05, 'epoch': 1.55}\n",
            "{'loss': 6.192, 'learning_rate': 2.3994250871889178e-05, 'epoch': 1.56}\n",
            "{'loss': 6.202, 'learning_rate': 2.3878153777567256e-05, 'epoch': 1.57}\n",
            "{'loss': 6.203, 'learning_rate': 2.376205668324533e-05, 'epoch': 1.57}\n",
            "{'loss': 6.201, 'learning_rate': 2.3645959588923412e-05, 'epoch': 1.58}\n",
            "{'loss': 6.2183, 'learning_rate': 2.3529862494601487e-05, 'epoch': 1.59}\n",
            "{'loss': 6.1765, 'learning_rate': 2.3413765400279565e-05, 'epoch': 1.6}\n",
            "{'loss': 6.1896, 'learning_rate': 2.329766830595764e-05, 'epoch': 1.6}\n",
            "{'loss': 6.1739, 'learning_rate': 2.3181571211635718e-05, 'epoch': 1.61}\n",
            "{'loss': 6.1937, 'learning_rate': 2.3065474117313792e-05, 'epoch': 1.62}\n",
            "{'loss': 6.1652, 'learning_rate': 2.294937702299187e-05, 'epoch': 1.62}\n",
            "{'loss': 6.2034, 'learning_rate': 2.2833279928669945e-05, 'epoch': 1.63}\n",
            "{'loss': 6.2033, 'learning_rate': 2.2717182834348023e-05, 'epoch': 1.64}\n",
            "{'loss': 6.2037, 'learning_rate': 2.2601085740026098e-05, 'epoch': 1.64}\n",
            "{'loss': 6.1791, 'learning_rate': 2.2484988645704176e-05, 'epoch': 1.65}\n",
            "{'loss': 6.1928, 'learning_rate': 2.2368891551382254e-05, 'epoch': 1.66}\n",
            "{'loss': 6.1736, 'learning_rate': 2.2252794457060332e-05, 'epoch': 1.66}\n",
            "{'loss': 6.1798, 'learning_rate': 2.2136697362738407e-05, 'epoch': 1.67}\n",
            " 56% 120000/215337 [3:47:15<3:03:07,  8.68it/s][INFO|trainer.py:2807] 2023-08-09 14:22:26,131 >> Saving model checkpoint to ./tst-summarization/checkpoint-120000\n",
            "[INFO|configuration_utils.py:458] 2023-08-09 14:22:26,132 >> Configuration saved in ./tst-summarization/checkpoint-120000/config.json\n",
            "[INFO|configuration_utils.py:375] 2023-08-09 14:22:26,133 >> Configuration saved in ./tst-summarization/checkpoint-120000/generation_config.json\n",
            "[INFO|modeling_utils.py:1851] 2023-08-09 14:22:26,460 >> Model weights saved in ./tst-summarization/checkpoint-120000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2210] 2023-08-09 14:22:26,461 >> tokenizer config file saved in ./tst-summarization/checkpoint-120000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2023-08-09 14:22:26,461 >> Special tokens file saved in ./tst-summarization/checkpoint-120000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-09 14:22:26,496 >> Copy vocab file to ./tst-summarization/checkpoint-120000/spiece.model\n",
            "{'loss': 6.1772, 'learning_rate': 2.2020600268416485e-05, 'epoch': 1.68}\n",
            "{'loss': 6.1563, 'learning_rate': 2.190450317409456e-05, 'epoch': 1.69}\n",
            "{'loss': 6.1946, 'learning_rate': 2.1788406079772637e-05, 'epoch': 1.69}\n",
            "{'loss': 6.1821, 'learning_rate': 2.1672308985450712e-05, 'epoch': 1.7}\n",
            "{'loss': 6.1939, 'learning_rate': 2.155621189112879e-05, 'epoch': 1.71}\n",
            "{'loss': 6.1787, 'learning_rate': 2.1440114796806868e-05, 'epoch': 1.71}\n",
            "{'loss': 6.1816, 'learning_rate': 2.1324017702484943e-05, 'epoch': 1.72}\n",
            "{'loss': 6.15, 'learning_rate': 2.120792060816302e-05, 'epoch': 1.73}\n",
            "{'loss': 6.1672, 'learning_rate': 2.1091823513841096e-05, 'epoch': 1.73}\n",
            "{'loss': 6.1729, 'learning_rate': 2.0975726419519174e-05, 'epoch': 1.74}\n",
            "{'loss': 6.1568, 'learning_rate': 2.0859629325197248e-05, 'epoch': 1.75}\n",
            "{'loss': 6.143, 'learning_rate': 2.0743532230875326e-05, 'epoch': 1.76}\n",
            "{'loss': 6.1621, 'learning_rate': 2.06274351365534e-05, 'epoch': 1.76}\n",
            "{'loss': 6.1468, 'learning_rate': 2.051133804223148e-05, 'epoch': 1.77}\n",
            "{'loss': 6.1621, 'learning_rate': 2.0395240947909557e-05, 'epoch': 1.78}\n",
            "{'loss': 6.1428, 'learning_rate': 2.0279143853587635e-05, 'epoch': 1.78}\n",
            "{'loss': 6.149, 'learning_rate': 2.016304675926571e-05, 'epoch': 1.79}\n",
            "{'loss': 6.1357, 'learning_rate': 2.0046949664943788e-05, 'epoch': 1.8}\n",
            "{'loss': 6.127, 'learning_rate': 1.9930852570621863e-05, 'epoch': 1.8}\n",
            "{'loss': 6.1587, 'learning_rate': 1.981475547629994e-05, 'epoch': 1.81}\n",
            "{'loss': 6.1378, 'learning_rate': 1.9698658381978015e-05, 'epoch': 1.82}\n",
            "{'loss': 6.1524, 'learning_rate': 1.9582561287656093e-05, 'epoch': 1.83}\n",
            "{'loss': 6.1432, 'learning_rate': 1.9466464193334168e-05, 'epoch': 1.83}\n",
            "{'loss': 6.1396, 'learning_rate': 1.935036709901225e-05, 'epoch': 1.84}\n",
            "{'loss': 6.1363, 'learning_rate': 1.9234270004690324e-05, 'epoch': 1.85}\n",
            "{'loss': 6.142, 'learning_rate': 1.9118172910368402e-05, 'epoch': 1.85}\n",
            "{'loss': 6.119, 'learning_rate': 1.9002075816046477e-05, 'epoch': 1.86}\n",
            "{'loss': 6.1464, 'learning_rate': 1.8885978721724555e-05, 'epoch': 1.87}\n",
            "{'loss': 6.1244, 'learning_rate': 1.876988162740263e-05, 'epoch': 1.87}\n",
            "{'loss': 6.1316, 'learning_rate': 1.8653784533080708e-05, 'epoch': 1.88}\n",
            "{'loss': 6.1413, 'learning_rate': 1.8537687438758782e-05, 'epoch': 1.89}\n",
            "{'loss': 6.126, 'learning_rate': 1.842159034443686e-05, 'epoch': 1.89}\n",
            "{'loss': 6.117, 'learning_rate': 1.8305493250114935e-05, 'epoch': 1.9}\n",
            "{'loss': 6.1252, 'learning_rate': 1.8189396155793016e-05, 'epoch': 1.91}\n",
            "{'loss': 6.1351, 'learning_rate': 1.807329906147109e-05, 'epoch': 1.92}\n",
            "{'loss': 6.1475, 'learning_rate': 1.795720196714917e-05, 'epoch': 1.92}\n",
            "{'loss': 6.1276, 'learning_rate': 1.7841104872827244e-05, 'epoch': 1.93}\n",
            "{'loss': 6.1207, 'learning_rate': 1.7725007778505322e-05, 'epoch': 1.94}\n",
            "{'loss': 6.1212, 'learning_rate': 1.7608910684183396e-05, 'epoch': 1.94}\n",
            "{'loss': 6.1209, 'learning_rate': 1.7492813589861475e-05, 'epoch': 1.95}\n",
            " 65% 140000/215337 [4:25:02<2:23:13,  8.77it/s][INFO|trainer.py:2807] 2023-08-09 15:00:13,691 >> Saving model checkpoint to ./tst-summarization/checkpoint-140000\n",
            "[INFO|configuration_utils.py:458] 2023-08-09 15:00:13,692 >> Configuration saved in ./tst-summarization/checkpoint-140000/config.json\n",
            "[INFO|configuration_utils.py:375] 2023-08-09 15:00:13,693 >> Configuration saved in ./tst-summarization/checkpoint-140000/generation_config.json\n",
            "[INFO|modeling_utils.py:1851] 2023-08-09 15:00:14,023 >> Model weights saved in ./tst-summarization/checkpoint-140000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2210] 2023-08-09 15:00:14,023 >> tokenizer config file saved in ./tst-summarization/checkpoint-140000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2023-08-09 15:00:14,024 >> Special tokens file saved in ./tst-summarization/checkpoint-140000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-09 15:00:14,059 >> Copy vocab file to ./tst-summarization/checkpoint-140000/spiece.model\n",
            "{'loss': 6.122, 'learning_rate': 1.737671649553955e-05, 'epoch': 1.96}\n",
            "{'loss': 6.1274, 'learning_rate': 1.7260619401217627e-05, 'epoch': 1.96}\n",
            "{'loss': 6.1032, 'learning_rate': 1.7144522306895705e-05, 'epoch': 1.97}\n",
            "{'loss': 6.1079, 'learning_rate': 1.702842521257378e-05, 'epoch': 1.98}\n",
            "{'loss': 6.1215, 'learning_rate': 1.6912328118251858e-05, 'epoch': 1.99}\n",
            "{'loss': 6.1315, 'learning_rate': 1.6796231023929933e-05, 'epoch': 1.99}\n",
            "{'loss': 6.113, 'learning_rate': 1.668013392960801e-05, 'epoch': 2.0}\n",
            "{'loss': 6.1, 'learning_rate': 1.6564036835286085e-05, 'epoch': 2.01}\n",
            "{'loss': 6.1111, 'learning_rate': 1.6447939740964163e-05, 'epoch': 2.01}\n",
            "{'loss': 6.1203, 'learning_rate': 1.6331842646642238e-05, 'epoch': 2.02}\n",
            "{'loss': 6.1176, 'learning_rate': 1.6215745552320316e-05, 'epoch': 2.03}\n",
            "{'loss': 6.0901, 'learning_rate': 1.6099648457998394e-05, 'epoch': 2.03}\n",
            "{'loss': 6.1177, 'learning_rate': 1.5983551363676472e-05, 'epoch': 2.04}\n",
            "{'loss': 6.1193, 'learning_rate': 1.5867454269354547e-05, 'epoch': 2.05}\n",
            "{'loss': 6.1139, 'learning_rate': 1.5751357175032625e-05, 'epoch': 2.05}\n",
            "{'loss': 6.0789, 'learning_rate': 1.56352600807107e-05, 'epoch': 2.06}\n",
            "{'loss': 6.0987, 'learning_rate': 1.5519162986388778e-05, 'epoch': 2.07}\n",
            "{'loss': 6.0932, 'learning_rate': 1.5403065892066852e-05, 'epoch': 2.08}\n",
            "{'loss': 6.099, 'learning_rate': 1.528696879774493e-05, 'epoch': 2.08}\n",
            "{'loss': 6.0905, 'learning_rate': 1.5170871703423007e-05, 'epoch': 2.09}\n",
            "{'loss': 6.108, 'learning_rate': 1.5054774609101085e-05, 'epoch': 2.1}\n",
            "{'loss': 6.1024, 'learning_rate': 1.4938677514779161e-05, 'epoch': 2.1}\n",
            "{'loss': 6.0983, 'learning_rate': 1.4822580420457238e-05, 'epoch': 2.11}\n",
            "{'loss': 6.0904, 'learning_rate': 1.4706483326135314e-05, 'epoch': 2.12}\n",
            "{'loss': 6.0999, 'learning_rate': 1.459038623181339e-05, 'epoch': 2.12}\n",
            "{'loss': 6.0868, 'learning_rate': 1.4474289137491467e-05, 'epoch': 2.13}\n",
            "{'loss': 6.0537, 'learning_rate': 1.4358192043169543e-05, 'epoch': 2.14}\n",
            "{'loss': 6.0886, 'learning_rate': 1.424209494884762e-05, 'epoch': 2.15}\n",
            "{'loss': 6.0972, 'learning_rate': 1.4125997854525696e-05, 'epoch': 2.15}\n",
            "{'loss': 6.0915, 'learning_rate': 1.4009900760203774e-05, 'epoch': 2.16}\n",
            "{'loss': 6.0805, 'learning_rate': 1.3893803665881852e-05, 'epoch': 2.17}\n",
            "{'loss': 6.0974, 'learning_rate': 1.3777706571559928e-05, 'epoch': 2.17}\n",
            "{'loss': 6.0597, 'learning_rate': 1.3661609477238005e-05, 'epoch': 2.18}\n",
            "{'loss': 6.0787, 'learning_rate': 1.3545512382916081e-05, 'epoch': 2.19}\n",
            "{'loss': 6.1068, 'learning_rate': 1.3429415288594157e-05, 'epoch': 2.19}\n",
            "{'loss': 6.0967, 'learning_rate': 1.3313318194272234e-05, 'epoch': 2.2}\n",
            "{'loss': 6.0825, 'learning_rate': 1.319722109995031e-05, 'epoch': 2.21}\n",
            "{'loss': 6.0749, 'learning_rate': 1.3081124005628386e-05, 'epoch': 2.22}\n",
            "{'loss': 6.1011, 'learning_rate': 1.2965026911306463e-05, 'epoch': 2.22}\n",
            "{'loss': 6.0854, 'learning_rate': 1.2848929816984542e-05, 'epoch': 2.23}\n",
            " 74% 160000/215337 [5:02:55<1:46:57,  8.62it/s][INFO|trainer.py:2807] 2023-08-09 15:38:06,746 >> Saving model checkpoint to ./tst-summarization/checkpoint-160000\n",
            "[INFO|configuration_utils.py:458] 2023-08-09 15:38:06,747 >> Configuration saved in ./tst-summarization/checkpoint-160000/config.json\n",
            "[INFO|configuration_utils.py:375] 2023-08-09 15:38:06,747 >> Configuration saved in ./tst-summarization/checkpoint-160000/generation_config.json\n",
            "[INFO|modeling_utils.py:1851] 2023-08-09 15:38:07,076 >> Model weights saved in ./tst-summarization/checkpoint-160000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2210] 2023-08-09 15:38:07,076 >> tokenizer config file saved in ./tst-summarization/checkpoint-160000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2023-08-09 15:38:07,077 >> Special tokens file saved in ./tst-summarization/checkpoint-160000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-09 15:38:07,113 >> Copy vocab file to ./tst-summarization/checkpoint-160000/spiece.model\n",
            "{'loss': 6.0943, 'learning_rate': 1.2732832722662619e-05, 'epoch': 2.24}\n",
            "{'loss': 6.0818, 'learning_rate': 1.2616735628340695e-05, 'epoch': 2.24}\n",
            "{'loss': 6.0685, 'learning_rate': 1.2500638534018772e-05, 'epoch': 2.25}\n",
            "{'loss': 6.0871, 'learning_rate': 1.2384541439696848e-05, 'epoch': 2.26}\n",
            "{'loss': 6.0839, 'learning_rate': 1.2268444345374924e-05, 'epoch': 2.26}\n",
            "{'loss': 6.0758, 'learning_rate': 1.2152347251053e-05, 'epoch': 2.27}\n",
            "{'loss': 6.0805, 'learning_rate': 1.2036250156731079e-05, 'epoch': 2.28}\n",
            "{'loss': 6.0742, 'learning_rate': 1.1920153062409155e-05, 'epoch': 2.28}\n",
            "{'loss': 6.0801, 'learning_rate': 1.1804055968087231e-05, 'epoch': 2.29}\n",
            "{'loss': 6.083, 'learning_rate': 1.1687958873765308e-05, 'epoch': 2.3}\n",
            "{'loss': 6.0945, 'learning_rate': 1.1571861779443384e-05, 'epoch': 2.31}\n",
            "{'loss': 6.063, 'learning_rate': 1.1455764685121462e-05, 'epoch': 2.31}\n",
            "{'loss': 6.0439, 'learning_rate': 1.1339667590799539e-05, 'epoch': 2.32}\n",
            "{'loss': 6.0679, 'learning_rate': 1.1223570496477615e-05, 'epoch': 2.33}\n",
            "{'loss': 6.0796, 'learning_rate': 1.1107473402155691e-05, 'epoch': 2.33}\n",
            "{'loss': 6.0689, 'learning_rate': 1.099137630783377e-05, 'epoch': 2.34}\n",
            "{'loss': 6.0819, 'learning_rate': 1.0875279213511846e-05, 'epoch': 2.35}\n",
            "{'loss': 6.0843, 'learning_rate': 1.0759182119189922e-05, 'epoch': 2.35}\n",
            "{'loss': 6.042, 'learning_rate': 1.0643085024867998e-05, 'epoch': 2.36}\n",
            "{'loss': 6.0526, 'learning_rate': 1.0526987930546075e-05, 'epoch': 2.37}\n",
            "{'loss': 6.0651, 'learning_rate': 1.0410890836224153e-05, 'epoch': 2.38}\n",
            "{'loss': 6.0828, 'learning_rate': 1.029479374190223e-05, 'epoch': 2.38}\n",
            "{'loss': 6.0814, 'learning_rate': 1.0178696647580306e-05, 'epoch': 2.39}\n",
            "{'loss': 6.0725, 'learning_rate': 1.0062599553258382e-05, 'epoch': 2.4}\n",
            "{'loss': 6.0707, 'learning_rate': 9.946502458936458e-06, 'epoch': 2.4}\n",
            "{'loss': 6.067, 'learning_rate': 9.830405364614535e-06, 'epoch': 2.41}\n",
            "{'loss': 6.0717, 'learning_rate': 9.714308270292611e-06, 'epoch': 2.42}\n",
            "{'loss': 6.0519, 'learning_rate': 9.598211175970687e-06, 'epoch': 2.42}\n",
            "{'loss': 6.0688, 'learning_rate': 9.482114081648764e-06, 'epoch': 2.43}\n",
            "{'loss': 6.064, 'learning_rate': 9.366016987326842e-06, 'epoch': 2.44}\n",
            "{'loss': 6.0524, 'learning_rate': 9.249919893004918e-06, 'epoch': 2.45}\n",
            "{'loss': 6.0603, 'learning_rate': 9.133822798682994e-06, 'epoch': 2.45}\n",
            "{'loss': 6.0793, 'learning_rate': 9.01772570436107e-06, 'epoch': 2.46}\n",
            "{'loss': 6.0754, 'learning_rate': 8.901628610039147e-06, 'epoch': 2.47}\n",
            "{'loss': 6.0414, 'learning_rate': 8.785531515717225e-06, 'epoch': 2.47}\n",
            "{'loss': 6.0852, 'learning_rate': 8.669434421395302e-06, 'epoch': 2.48}\n",
            "{'loss': 6.0522, 'learning_rate': 8.553337327073378e-06, 'epoch': 2.49}\n",
            "{'loss': 6.046, 'learning_rate': 8.437240232751454e-06, 'epoch': 2.49}\n",
            "{'loss': 6.0723, 'learning_rate': 8.321143138429532e-06, 'epoch': 2.5}\n",
            "{'loss': 6.0861, 'learning_rate': 8.205046044107609e-06, 'epoch': 2.51}\n",
            " 84% 180000/215337 [5:40:52<1:06:20,  8.88it/s][INFO|trainer.py:2807] 2023-08-09 16:16:03,681 >> Saving model checkpoint to ./tst-summarization/checkpoint-180000\n",
            "[INFO|configuration_utils.py:458] 2023-08-09 16:16:03,682 >> Configuration saved in ./tst-summarization/checkpoint-180000/config.json\n",
            "[INFO|configuration_utils.py:375] 2023-08-09 16:16:03,682 >> Configuration saved in ./tst-summarization/checkpoint-180000/generation_config.json\n",
            "[INFO|modeling_utils.py:1851] 2023-08-09 16:16:04,011 >> Model weights saved in ./tst-summarization/checkpoint-180000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2210] 2023-08-09 16:16:04,011 >> tokenizer config file saved in ./tst-summarization/checkpoint-180000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2023-08-09 16:16:04,012 >> Special tokens file saved in ./tst-summarization/checkpoint-180000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-09 16:16:04,048 >> Copy vocab file to ./tst-summarization/checkpoint-180000/spiece.model\n",
            "{'loss': 6.0673, 'learning_rate': 8.088948949785685e-06, 'epoch': 2.51}\n",
            "{'loss': 6.0361, 'learning_rate': 7.972851855463761e-06, 'epoch': 2.52}\n",
            "{'loss': 6.0515, 'learning_rate': 7.856754761141838e-06, 'epoch': 2.53}\n",
            "{'loss': 6.0636, 'learning_rate': 7.740657666819916e-06, 'epoch': 2.54}\n",
            "{'loss': 6.0567, 'learning_rate': 7.624560572497992e-06, 'epoch': 2.54}\n",
            "{'loss': 6.051, 'learning_rate': 7.5084634781760686e-06, 'epoch': 2.55}\n",
            "{'loss': 6.0584, 'learning_rate': 7.392366383854145e-06, 'epoch': 2.56}\n",
            "{'loss': 6.0442, 'learning_rate': 7.276269289532221e-06, 'epoch': 2.56}\n",
            "{'loss': 6.0719, 'learning_rate': 7.160172195210299e-06, 'epoch': 2.57}\n",
            "{'loss': 6.0469, 'learning_rate': 7.044075100888376e-06, 'epoch': 2.58}\n",
            "{'loss': 6.064, 'learning_rate': 6.927978006566452e-06, 'epoch': 2.58}\n",
            "{'loss': 6.0528, 'learning_rate': 6.811880912244528e-06, 'epoch': 2.59}\n",
            "{'loss': 6.0628, 'learning_rate': 6.695783817922606e-06, 'epoch': 2.6}\n",
            "{'loss': 6.049, 'learning_rate': 6.579686723600682e-06, 'epoch': 2.61}\n",
            "{'loss': 6.061, 'learning_rate': 6.463589629278758e-06, 'epoch': 2.61}\n",
            "{'loss': 6.0481, 'learning_rate': 6.347492534956835e-06, 'epoch': 2.62}\n",
            "{'loss': 6.0589, 'learning_rate': 6.231395440634912e-06, 'epoch': 2.63}\n",
            "{'loss': 6.0527, 'learning_rate': 6.115298346312988e-06, 'epoch': 2.63}\n",
            "{'loss': 6.035, 'learning_rate': 5.9992012519910655e-06, 'epoch': 2.64}\n",
            "{'loss': 6.0515, 'learning_rate': 5.883104157669142e-06, 'epoch': 2.65}\n",
            "{'loss': 6.0627, 'learning_rate': 5.767007063347219e-06, 'epoch': 2.65}\n",
            "{'loss': 6.0691, 'learning_rate': 5.650909969025295e-06, 'epoch': 2.66}\n",
            "{'loss': 6.0657, 'learning_rate': 5.534812874703372e-06, 'epoch': 2.67}\n",
            "{'loss': 6.0479, 'learning_rate': 5.418715780381449e-06, 'epoch': 2.67}\n",
            "{'loss': 6.0588, 'learning_rate': 5.302618686059525e-06, 'epoch': 2.68}\n",
            "{'loss': 6.0274, 'learning_rate': 5.1865215917376025e-06, 'epoch': 2.69}\n",
            "{'loss': 6.0691, 'learning_rate': 5.070424497415679e-06, 'epoch': 2.7}\n",
            "{'loss': 6.0362, 'learning_rate': 4.954327403093756e-06, 'epoch': 2.7}\n",
            "{'loss': 6.0668, 'learning_rate': 4.8382303087718325e-06, 'epoch': 2.71}\n",
            "{'loss': 6.0533, 'learning_rate': 4.722133214449909e-06, 'epoch': 2.72}\n",
            "{'loss': 6.0258, 'learning_rate': 4.606036120127986e-06, 'epoch': 2.72}\n",
            "{'loss': 6.04, 'learning_rate': 4.489939025806062e-06, 'epoch': 2.73}\n",
            "{'loss': 6.0547, 'learning_rate': 4.37384193148414e-06, 'epoch': 2.74}\n",
            "{'loss': 6.0626, 'learning_rate': 4.257744837162216e-06, 'epoch': 2.74}\n",
            "{'loss': 6.0395, 'learning_rate': 4.141647742840292e-06, 'epoch': 2.75}\n",
            "{'loss': 6.0396, 'learning_rate': 4.025550648518369e-06, 'epoch': 2.76}\n",
            "{'loss': 6.0478, 'learning_rate': 3.909453554196446e-06, 'epoch': 2.77}\n",
            "{'loss': 6.0388, 'learning_rate': 3.7933564598745227e-06, 'epoch': 2.77}\n",
            "{'loss': 6.0518, 'learning_rate': 3.677259365552599e-06, 'epoch': 2.78}\n",
            "{'loss': 6.0464, 'learning_rate': 3.561162271230676e-06, 'epoch': 2.79}\n",
            " 93% 200000/215337 [6:18:49<28:48,  8.87it/s][INFO|trainer.py:2807] 2023-08-09 16:54:00,233 >> Saving model checkpoint to ./tst-summarization/checkpoint-200000\n",
            "[INFO|configuration_utils.py:458] 2023-08-09 16:54:00,234 >> Configuration saved in ./tst-summarization/checkpoint-200000/config.json\n",
            "[INFO|configuration_utils.py:375] 2023-08-09 16:54:00,234 >> Configuration saved in ./tst-summarization/checkpoint-200000/generation_config.json\n",
            "[INFO|modeling_utils.py:1851] 2023-08-09 16:54:00,574 >> Model weights saved in ./tst-summarization/checkpoint-200000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2210] 2023-08-09 16:54:00,575 >> tokenizer config file saved in ./tst-summarization/checkpoint-200000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2023-08-09 16:54:00,575 >> Special tokens file saved in ./tst-summarization/checkpoint-200000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-09 16:54:00,614 >> Copy vocab file to ./tst-summarization/checkpoint-200000/spiece.model\n",
            "{'loss': 6.0321, 'learning_rate': 3.445065176908752e-06, 'epoch': 2.79}\n",
            "{'loss': 6.0539, 'learning_rate': 3.3289680825868294e-06, 'epoch': 2.8}\n",
            "{'loss': 6.0481, 'learning_rate': 3.2128709882649057e-06, 'epoch': 2.81}\n",
            "{'loss': 6.0297, 'learning_rate': 3.0967738939429825e-06, 'epoch': 2.81}\n",
            "{'loss': 6.0594, 'learning_rate': 2.9806767996210593e-06, 'epoch': 2.82}\n",
            "{'loss': 6.0272, 'learning_rate': 2.864579705299136e-06, 'epoch': 2.83}\n",
            "{'loss': 6.041, 'learning_rate': 2.748482610977213e-06, 'epoch': 2.84}\n",
            "{'loss': 6.0576, 'learning_rate': 2.6323855166552896e-06, 'epoch': 2.84}\n",
            "{'loss': 6.0308, 'learning_rate': 2.516288422333366e-06, 'epoch': 2.85}\n",
            "{'loss': 6.0341, 'learning_rate': 2.4001913280114424e-06, 'epoch': 2.86}\n",
            "{'loss': 6.0374, 'learning_rate': 2.284094233689519e-06, 'epoch': 2.86}\n",
            "{'loss': 6.0524, 'learning_rate': 2.167997139367596e-06, 'epoch': 2.87}\n",
            "{'loss': 6.0339, 'learning_rate': 2.0519000450456727e-06, 'epoch': 2.88}\n",
            "{'loss': 6.0545, 'learning_rate': 1.9358029507237495e-06, 'epoch': 2.88}\n",
            "{'loss': 6.0307, 'learning_rate': 1.8197058564018263e-06, 'epoch': 2.89}\n",
            "{'loss': 6.0421, 'learning_rate': 1.7036087620799026e-06, 'epoch': 2.9}\n",
            "{'loss': 6.0271, 'learning_rate': 1.5875116677579794e-06, 'epoch': 2.9}\n",
            "{'loss': 6.0486, 'learning_rate': 1.471414573436056e-06, 'epoch': 2.91}\n",
            "{'loss': 6.0477, 'learning_rate': 1.3553174791141328e-06, 'epoch': 2.92}\n",
            "{'loss': 6.0386, 'learning_rate': 1.2392203847922096e-06, 'epoch': 2.93}\n",
            "{'loss': 6.0323, 'learning_rate': 1.1231232904702863e-06, 'epoch': 2.93}\n",
            "{'loss': 6.0407, 'learning_rate': 1.007026196148363e-06, 'epoch': 2.94}\n",
            "{'loss': 6.0519, 'learning_rate': 8.909291018264396e-07, 'epoch': 2.95}\n",
            "{'loss': 6.0299, 'learning_rate': 7.748320075045162e-07, 'epoch': 2.95}\n",
            "{'loss': 6.0425, 'learning_rate': 6.587349131825929e-07, 'epoch': 2.96}\n",
            "{'loss': 6.0457, 'learning_rate': 5.426378188606695e-07, 'epoch': 2.97}\n",
            "{'loss': 6.031, 'learning_rate': 4.265407245387463e-07, 'epoch': 2.97}\n",
            "{'loss': 6.0452, 'learning_rate': 3.104436302168229e-07, 'epoch': 2.98}\n",
            "{'loss': 6.022, 'learning_rate': 1.9434653589489965e-07, 'epoch': 2.99}\n",
            "{'loss': 6.0348, 'learning_rate': 7.824944157297631e-08, 'epoch': 3.0}\n",
            "100% 215336/215337 [6:47:55<00:00,  8.42it/s][INFO|trainer.py:1934] 2023-08-09 17:23:06,314 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 24475.2992, 'train_samples_per_second': 35.192, 'train_steps_per_second': 8.798, 'train_loss': 6.426309183396258, 'epoch': 3.0}\n",
            "100% 215337/215337 [6:47:55<00:00,  8.80it/s]\n",
            "[INFO|trainer.py:2807] 2023-08-09 17:23:06,317 >> Saving model checkpoint to ./tst-summarization\n",
            "[INFO|configuration_utils.py:458] 2023-08-09 17:23:06,318 >> Configuration saved in ./tst-summarization/config.json\n",
            "[INFO|configuration_utils.py:375] 2023-08-09 17:23:06,318 >> Configuration saved in ./tst-summarization/generation_config.json\n",
            "[INFO|modeling_utils.py:1851] 2023-08-09 17:23:06,652 >> Model weights saved in ./tst-summarization/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2210] 2023-08-09 17:23:06,653 >> tokenizer config file saved in ./tst-summarization/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2023-08-09 17:23:06,653 >> Special tokens file saved in ./tst-summarization/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-09 17:23:06,692 >> Copy vocab file to ./tst-summarization/spiece.model\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     6.4263\n",
            "  train_runtime            = 6:47:55.29\n",
            "  train_samples            =     287113\n",
            "  train_samples_per_second =     35.192\n",
            "  train_steps_per_second   =      8.798\n",
            "08/09/2023 17:23:06 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:3081] 2023-08-09 17:23:06,711 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3083] 2023-08-09 17:23:06,712 >>   Num examples = 13368\n",
            "[INFO|trainer.py:3086] 2023-08-09 17:23:06,712 >>   Batch size = 4\n",
            "[INFO|configuration_utils.py:599] 2023-08-09 17:23:06,722 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.31.0\"\n",
            "}\n",
            "\n",
            "100% 3342/3342 [1:05:23<00:00,  1.17s/it]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_gen_len            =   123.7768\n",
            "  eval_loss               =     6.2485\n",
            "  eval_rouge1             =     3.6458\n",
            "  eval_rouge2             =       0.04\n",
            "  eval_rougeL             =     3.1999\n",
            "  eval_rougeLsum          =     3.5598\n",
            "  eval_runtime            = 1:05:26.57\n",
            "  eval_samples            =      13368\n",
            "  eval_samples_per_second =      3.404\n",
            "  eval_steps_per_second   =      0.851\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r tst-summarization"
      ],
      "metadata": {
        "id": "XbKdT_FtOUBh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}