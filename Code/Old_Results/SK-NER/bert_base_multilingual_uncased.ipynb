{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMmf1zo7PYUt",
        "outputId": "2319b298-17dd-4446-9e1a-04834d2a1555"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: safetensors, dill, responses, multiprocess, huggingface-hub, tokenizers, transformers, datasets, evaluate\n",
            "Successfully installed datasets-2.14.6 dill-0.3.7 evaluate-0.4.1 huggingface-hub-0.17.3 multiprocess-0.70.15 responses-0.18.0 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.35.0\n",
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-hei_goo3\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-hei_goo3\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit cc3e4781854a52cf090ffde28d884a527dab6708\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.36.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.36.0.dev0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0.dev0) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0.dev0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0.dev0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0.dev0) (2023.7.22)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.36.0.dev0-py3-none-any.whl size=7923012 sha256=da805a700d23f3e07bc43e59c78f1a0b6d0a16a6b59c960124f146db3921daa5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-kmbeoge7/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.0\n",
            "    Uninstalling transformers-4.35.0:\n",
            "      Successfully uninstalled transformers-4.35.0\n",
            "Successfully installed transformers-4.36.0.dev0\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.24.1\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.36.0.dev0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu118)\n",
            "Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n",
            "Found existing installation: tensorflow 2.14.0\n",
            "Uninstalling tensorflow-2.14.0:\n",
            "  Successfully uninstalled tensorflow-2.14.0\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=4483cc40d3b76e8404016c18467709c8830da7bb2cdf95ca5c441300f0f2c0f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets evaluate\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install accelerate -U\n",
        "!pip install transformers[torch] -U\n",
        "!pip uninstall -y tensorflow\n",
        "!pip install seqeval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzQdHD9xfnBG"
      },
      "source": [
        "# bert-base-uncased-llm123-fact3-shuffle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbKdT_FtOUBh",
        "outputId": "54645fc5-d4b7-4013-eacd-77276e6f4ddf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11/05/2023 05:02:45 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/05/2023 05:02:45 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/tmp/test-ner/runs/Nov05_05-02-44_2478ccbd6c6a,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/tmp/test-ner,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/tmp/test-ner,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "https://huggingface.co/datasets/ju-bezdek/conll2003-SK-NER/resolve/main/conll2003-SK-NER.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/ff279d08486d10439072581533b8edc06a2226d9d1b21e4129f79672f200ba8c.931084210b7b6c6b1713147e4dfb893ed4a388c6cf72e79039da11a50a0428f6.py.incomplete\n",
            "11/05/2023 05:02:46 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/ju-bezdek/conll2003-SK-NER/resolve/main/conll2003-SK-NER.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/ff279d08486d10439072581533b8edc06a2226d9d1b21e4129f79672f200ba8c.931084210b7b6c6b1713147e4dfb893ed4a388c6cf72e79039da11a50a0428f6.py.incomplete\n",
            "Downloading builder script: 100% 3.60k/3.60k [00:00<00:00, 20.5MB/s]\n",
            "storing https://huggingface.co/datasets/ju-bezdek/conll2003-SK-NER/resolve/main/conll2003-SK-NER.py in cache at /root/.cache/huggingface/datasets/downloads/ff279d08486d10439072581533b8edc06a2226d9d1b21e4129f79672f200ba8c.931084210b7b6c6b1713147e4dfb893ed4a388c6cf72e79039da11a50a0428f6.py\n",
            "11/05/2023 05:02:47 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/ju-bezdek/conll2003-SK-NER/resolve/main/conll2003-SK-NER.py in cache at /root/.cache/huggingface/datasets/downloads/ff279d08486d10439072581533b8edc06a2226d9d1b21e4129f79672f200ba8c.931084210b7b6c6b1713147e4dfb893ed4a388c6cf72e79039da11a50a0428f6.py\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/ff279d08486d10439072581533b8edc06a2226d9d1b21e4129f79672f200ba8c.931084210b7b6c6b1713147e4dfb893ed4a388c6cf72e79039da11a50a0428f6.py\n",
            "11/05/2023 05:02:47 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/ff279d08486d10439072581533b8edc06a2226d9d1b21e4129f79672f200ba8c.931084210b7b6c6b1713147e4dfb893ed4a388c6cf72e79039da11a50a0428f6.py\n",
            "https://huggingface.co/datasets/ju-bezdek/conll2003-SK-NER/resolve/main/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/97494b45facf5ee1bf3228df2d49adbfb35e79dffd392f922cfe366de37c2253.12ade2f7ed82ac85c64be1590de83ede9f72aa2100ffe3334214c2cddb0bad34.incomplete\n",
            "11/05/2023 05:02:47 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/ju-bezdek/conll2003-SK-NER/resolve/main/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/97494b45facf5ee1bf3228df2d49adbfb35e79dffd392f922cfe366de37c2253.12ade2f7ed82ac85c64be1590de83ede9f72aa2100ffe3334214c2cddb0bad34.incomplete\n",
            "Downloading metadata: 100% 1.90k/1.90k [00:00<00:00, 11.9MB/s]\n",
            "storing https://huggingface.co/datasets/ju-bezdek/conll2003-SK-NER/resolve/main/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/97494b45facf5ee1bf3228df2d49adbfb35e79dffd392f922cfe366de37c2253.12ade2f7ed82ac85c64be1590de83ede9f72aa2100ffe3334214c2cddb0bad34\n",
            "11/05/2023 05:02:48 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/ju-bezdek/conll2003-SK-NER/resolve/main/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/97494b45facf5ee1bf3228df2d49adbfb35e79dffd392f922cfe366de37c2253.12ade2f7ed82ac85c64be1590de83ede9f72aa2100ffe3334214c2cddb0bad34\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/97494b45facf5ee1bf3228df2d49adbfb35e79dffd392f922cfe366de37c2253.12ade2f7ed82ac85c64be1590de83ede9f72aa2100ffe3334214c2cddb0bad34\n",
            "11/05/2023 05:02:48 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/97494b45facf5ee1bf3228df2d49adbfb35e79dffd392f922cfe366de37c2253.12ade2f7ed82ac85c64be1590de83ede9f72aa2100ffe3334214c2cddb0bad34\n",
            "https://huggingface.co/datasets/ju-bezdek/conll2003-SK-NER/resolve/main/README.md not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/2a4d1026eee1ee1a01d3979bf449255bfbe88fafce5cfbfd30b3dd2acfbe2f44.15cdac84608e572210ae8f0c59fe39f9854678c122e558210fb037fede72eed4.incomplete\n",
            "11/05/2023 05:02:48 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/ju-bezdek/conll2003-SK-NER/resolve/main/README.md not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/2a4d1026eee1ee1a01d3979bf449255bfbe88fafce5cfbfd30b3dd2acfbe2f44.15cdac84608e572210ae8f0c59fe39f9854678c122e558210fb037fede72eed4.incomplete\n",
            "Downloading readme: 100% 1.91k/1.91k [00:00<00:00, 12.6MB/s]\n",
            "storing https://huggingface.co/datasets/ju-bezdek/conll2003-SK-NER/resolve/main/README.md in cache at /root/.cache/huggingface/datasets/downloads/2a4d1026eee1ee1a01d3979bf449255bfbe88fafce5cfbfd30b3dd2acfbe2f44.15cdac84608e572210ae8f0c59fe39f9854678c122e558210fb037fede72eed4\n",
            "11/05/2023 05:02:48 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/ju-bezdek/conll2003-SK-NER/resolve/main/README.md in cache at /root/.cache/huggingface/datasets/downloads/2a4d1026eee1ee1a01d3979bf449255bfbe88fafce5cfbfd30b3dd2acfbe2f44.15cdac84608e572210ae8f0c59fe39f9854678c122e558210fb037fede72eed4\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/2a4d1026eee1ee1a01d3979bf449255bfbe88fafce5cfbfd30b3dd2acfbe2f44.15cdac84608e572210ae8f0c59fe39f9854678c122e558210fb037fede72eed4\n",
            "11/05/2023 05:02:48 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/2a4d1026eee1ee1a01d3979bf449255bfbe88fafce5cfbfd30b3dd2acfbe2f44.15cdac84608e572210ae8f0c59fe39f9854678c122e558210fb037fede72eed4\n",
            "No config specified, defaulting to the single config: conll2003-sk-ner/conll2003-SK-NER\n",
            "11/05/2023 05:02:49 - INFO - datasets.builder - No config specified, defaulting to the single config: conll2003-sk-ner/conll2003-SK-NER\n",
            "Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/ju-bezdek--conll2003-SK-NER/4f8c11457fb6ba1fedbf6a25e0a46ebe9230f8c375885debc1e5466e0f84a87d\n",
            "11/05/2023 05:02:49 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/ju-bezdek--conll2003-SK-NER/4f8c11457fb6ba1fedbf6a25e0a46ebe9230f8c375885debc1e5466e0f84a87d\n",
            "Generating dataset conll2003-sk-ner (/root/.cache/huggingface/datasets/ju-bezdek___conll2003-sk-ner/conll2003-SK-NER/1.0.0/4f8c11457fb6ba1fedbf6a25e0a46ebe9230f8c375885debc1e5466e0f84a87d)\n",
            "11/05/2023 05:02:49 - INFO - datasets.builder - Generating dataset conll2003-sk-ner (/root/.cache/huggingface/datasets/ju-bezdek___conll2003-sk-ner/conll2003-SK-NER/1.0.0/4f8c11457fb6ba1fedbf6a25e0a46ebe9230f8c375885debc1e5466e0f84a87d)\n",
            "Downloading and preparing dataset conll2003-sk-ner/conll2003-SK-NER (download: 3.85 MiB, generated: 4.88 MiB, post-processed: Unknown size, total: 8.72 MiB) to /root/.cache/huggingface/datasets/ju-bezdek___conll2003-sk-ner/conll2003-SK-NER/1.0.0/4f8c11457fb6ba1fedbf6a25e0a46ebe9230f8c375885debc1e5466e0f84a87d...\n",
            "11/05/2023 05:02:49 - INFO - datasets.builder - Downloading and preparing dataset conll2003-sk-ner/conll2003-SK-NER (download: 3.85 MiB, generated: 4.88 MiB, post-processed: Unknown size, total: 8.72 MiB) to /root/.cache/huggingface/datasets/ju-bezdek___conll2003-sk-ner/conll2003-SK-NER/1.0.0/4f8c11457fb6ba1fedbf6a25e0a46ebe9230f8c375885debc1e5466e0f84a87d...\n",
            "Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "11/05/2023 05:02:50 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "Downloading data files:   0% 0/3 [00:00<?, ?it/s]https://huggingface.co/datasets/ju-bezdek/conll2003-SK-NER/resolve/main/data/train.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/943445e295fd7a06860670a8044cd1e8f133059df03f86126693d9073aa472bc.incomplete\n",
            "11/05/2023 05:02:50 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/ju-bezdek/conll2003-SK-NER/resolve/main/data/train.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/943445e295fd7a06860670a8044cd1e8f133059df03f86126693d9073aa472bc.incomplete\n",
            "\n",
            "Downloading data: 100% 2.68M/2.68M [00:00<00:00, 72.5MB/s]\n",
            "storing https://huggingface.co/datasets/ju-bezdek/conll2003-SK-NER/resolve/main/data/train.json in cache at /root/.cache/huggingface/datasets/downloads/943445e295fd7a06860670a8044cd1e8f133059df03f86126693d9073aa472bc\n",
            "11/05/2023 05:02:51 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/ju-bezdek/conll2003-SK-NER/resolve/main/data/train.json in cache at /root/.cache/huggingface/datasets/downloads/943445e295fd7a06860670a8044cd1e8f133059df03f86126693d9073aa472bc\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/943445e295fd7a06860670a8044cd1e8f133059df03f86126693d9073aa472bc\n",
            "11/05/2023 05:02:51 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/943445e295fd7a06860670a8044cd1e8f133059df03f86126693d9073aa472bc\n",
            "Downloading data files:  33% 1/3 [00:00<00:01,  1.02it/s]https://huggingface.co/datasets/ju-bezdek/conll2003-SK-NER/resolve/main/data/valid.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/d8559dc13c2fe1e7e6142ee716d23fee1e0f0d9695a727793a8f2479e10df26f.incomplete\n",
            "11/05/2023 05:02:51 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/ju-bezdek/conll2003-SK-NER/resolve/main/data/valid.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/d8559dc13c2fe1e7e6142ee716d23fee1e0f0d9695a727793a8f2479e10df26f.incomplete\n",
            "\n",
            "Downloading data: 100% 702k/702k [00:00<00:00, 61.8MB/s]\n",
            "storing https://huggingface.co/datasets/ju-bezdek/conll2003-SK-NER/resolve/main/data/valid.json in cache at /root/.cache/huggingface/datasets/downloads/d8559dc13c2fe1e7e6142ee716d23fee1e0f0d9695a727793a8f2479e10df26f\n",
            "11/05/2023 05:02:52 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/ju-bezdek/conll2003-SK-NER/resolve/main/data/valid.json in cache at /root/.cache/huggingface/datasets/downloads/d8559dc13c2fe1e7e6142ee716d23fee1e0f0d9695a727793a8f2479e10df26f\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/d8559dc13c2fe1e7e6142ee716d23fee1e0f0d9695a727793a8f2479e10df26f\n",
            "11/05/2023 05:02:52 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/d8559dc13c2fe1e7e6142ee716d23fee1e0f0d9695a727793a8f2479e10df26f\n",
            "Downloading data files:  67% 2/3 [00:01<00:00,  1.08it/s]https://huggingface.co/datasets/ju-bezdek/conll2003-SK-NER/resolve/main/data/test.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/34e0aa36d8fcdb6ed906d2777679f7645a4ee55af3af0363ee1dbd1d11d59cbe.incomplete\n",
            "11/05/2023 05:02:52 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/ju-bezdek/conll2003-SK-NER/resolve/main/data/test.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/34e0aa36d8fcdb6ed906d2777679f7645a4ee55af3af0363ee1dbd1d11d59cbe.incomplete\n",
            "\n",
            "Downloading data: 100% 651k/651k [00:00<00:00, 55.4MB/s]\n",
            "storing https://huggingface.co/datasets/ju-bezdek/conll2003-SK-NER/resolve/main/data/test.json in cache at /root/.cache/huggingface/datasets/downloads/34e0aa36d8fcdb6ed906d2777679f7645a4ee55af3af0363ee1dbd1d11d59cbe\n",
            "11/05/2023 05:02:53 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/ju-bezdek/conll2003-SK-NER/resolve/main/data/test.json in cache at /root/.cache/huggingface/datasets/downloads/34e0aa36d8fcdb6ed906d2777679f7645a4ee55af3af0363ee1dbd1d11d59cbe\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/34e0aa36d8fcdb6ed906d2777679f7645a4ee55af3af0363ee1dbd1d11d59cbe\n",
            "11/05/2023 05:02:53 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/34e0aa36d8fcdb6ed906d2777679f7645a4ee55af3af0363ee1dbd1d11d59cbe\n",
            "Downloading data files: 100% 3/3 [00:02<00:00,  1.09it/s]\n",
            "Downloading took 0.0 min\n",
            "11/05/2023 05:02:53 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "11/05/2023 05:02:53 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 2324.57it/s]\n",
            "Generating train split\n",
            "11/05/2023 05:02:53 - INFO - datasets.builder - Generating train split\n",
            "Generating train split: 100% 14041/14041 [00:01<00:00, 10652.36 examples/s]\n",
            "Generating validation split\n",
            "11/05/2023 05:02:54 - INFO - datasets.builder - Generating validation split\n",
            "Generating validation split: 100% 3250/3250 [00:00<00:00, 9444.86 examples/s]\n",
            "Generating test split\n",
            "11/05/2023 05:02:54 - INFO - datasets.builder - Generating test split\n",
            "Generating test split: 100% 3453/3453 [00:00<00:00, 10025.61 examples/s]\n",
            "All the splits matched successfully.\n",
            "11/05/2023 05:02:55 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
            "Dataset conll2003-sk-ner downloaded and prepared to /root/.cache/huggingface/datasets/ju-bezdek___conll2003-sk-ner/conll2003-SK-NER/1.0.0/4f8c11457fb6ba1fedbf6a25e0a46ebe9230f8c375885debc1e5466e0f84a87d. Subsequent calls will reuse this data.\n",
            "11/05/2023 05:02:55 - INFO - datasets.builder - Dataset conll2003-sk-ner downloaded and prepared to /root/.cache/huggingface/datasets/ju-bezdek___conll2003-sk-ner/conll2003-SK-NER/1.0.0/4f8c11457fb6ba1fedbf6a25e0a46ebe9230f8c375885debc1e5466e0f84a87d. Subsequent calls will reuse this data.\n",
            "Downloading (…)lve/main/config.json: 100% 625/625 [00:00<00:00, 3.09MB/s]\n",
            "[INFO|configuration_utils.py:717] 2023-11-05 05:02:55,752 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/3da6b6aad5111664db74322f2158b7f93e09a717/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-05 05:02:55,756 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 105879\n",
            "}\n",
            "\n",
            "Downloading (…)okenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 109kB/s]\n",
            "[INFO|configuration_utils.py:717] 2023-11-05 05:02:56,287 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/3da6b6aad5111664db74322f2158b7f93e09a717/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-05 05:02:56,288 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 105879\n",
            "}\n",
            "\n",
            "Downloading (…)solve/main/vocab.txt: 100% 872k/872k [00:00<00:00, 3.29MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 1.72M/1.72M [00:00<00:00, 7.12MB/s]\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-05 05:02:58,361 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/3da6b6aad5111664db74322f2158b7f93e09a717/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-05 05:02:58,361 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/3da6b6aad5111664db74322f2158b7f93e09a717/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-05 05:02:58,361 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-05 05:02:58,361 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-05 05:02:58,361 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/3da6b6aad5111664db74322f2158b7f93e09a717/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:717] 2023-11-05 05:02:58,361 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/3da6b6aad5111664db74322f2158b7f93e09a717/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-05 05:02:58,362 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 105879\n",
            "}\n",
            "\n",
            "Downloading model.safetensors: 100% 672M/672M [00:44<00:00, 15.2MB/s]\n",
            "[INFO|modeling_utils.py:3121] 2023-11-05 05:03:44,245 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/3da6b6aad5111664db74322f2158b7f93e09a717/model.safetensors\n",
            "[INFO|modeling_utils.py:3940] 2023-11-05 05:03:45,757 >> Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-05 05:03:45,757 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on train dataset:   0% 0/14041 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/ju-bezdek___conll2003-sk-ner/conll2003-SK-NER/1.0.0/4f8c11457fb6ba1fedbf6a25e0a46ebe9230f8c375885debc1e5466e0f84a87d/cache-ea6d81935004ff8b.arrow\n",
            "11/05/2023 05:03:45 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/ju-bezdek___conll2003-sk-ner/conll2003-SK-NER/1.0.0/4f8c11457fb6ba1fedbf6a25e0a46ebe9230f8c375885debc1e5466e0f84a87d/cache-ea6d81935004ff8b.arrow\n",
            "Running tokenizer on train dataset: 100% 14041/14041 [00:01<00:00, 12441.35 examples/s]\n",
            "Running tokenizer on validation dataset:   0% 0/3250 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/ju-bezdek___conll2003-sk-ner/conll2003-SK-NER/1.0.0/4f8c11457fb6ba1fedbf6a25e0a46ebe9230f8c375885debc1e5466e0f84a87d/cache-50a13578e84176fb.arrow\n",
            "11/05/2023 05:03:47 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/ju-bezdek___conll2003-sk-ner/conll2003-SK-NER/1.0.0/4f8c11457fb6ba1fedbf6a25e0a46ebe9230f8c375885debc1e5466e0f84a87d/cache-50a13578e84176fb.arrow\n",
            "Running tokenizer on validation dataset: 100% 3250/3250 [00:00<00:00, 13058.31 examples/s]\n",
            "Downloading builder script: 100% 6.34k/6.34k [00:00<00:00, 19.7MB/s]\n",
            "[INFO|trainer.py:738] 2023-11-05 05:03:55,108 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, tokens, ner_tags. If id, tokens, ner_tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1723] 2023-11-05 05:03:55,118 >> ***** Running training *****\n",
            "[INFO|trainer.py:1724] 2023-11-05 05:03:55,119 >>   Num examples = 14,041\n",
            "[INFO|trainer.py:1725] 2023-11-05 05:03:55,119 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1726] 2023-11-05 05:03:55,119 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1729] 2023-11-05 05:03:55,119 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1730] 2023-11-05 05:03:55,119 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1731] 2023-11-05 05:03:55,119 >>   Total optimization steps = 5,268\n",
            "[INFO|trainer.py:1732] 2023-11-05 05:03:55,119 >>   Number of trainable parameters = 166,772,745\n",
            "  0% 0/5268 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-05 05:03:55,132 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 0.2469, 'learning_rate': 4.525436598329537e-05, 'epoch': 0.28}\n",
            "  9% 500/5268 [00:27<03:54, 20.34it/s][INFO|trainer.py:2882] 2023-11-05 05:04:22,337 >> Saving model checkpoint to /tmp/test-ner/checkpoint-500\n",
            "[INFO|configuration_utils.py:461] 2023-11-05 05:04:22,338 >> Configuration saved in /tmp/test-ner/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-05 05:04:23,531 >> Model weights saved in /tmp/test-ner/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-05 05:04:23,532 >> tokenizer config file saved in /tmp/test-ner/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-05 05:04:23,532 >> Special tokens file saved in /tmp/test-ner/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 0.1806, 'learning_rate': 4.050873196659074e-05, 'epoch': 0.57}\n",
            " 19% 1000/5268 [00:54<03:25, 20.74it/s][INFO|trainer.py:2882] 2023-11-05 05:04:49,451 >> Saving model checkpoint to /tmp/test-ner/checkpoint-1000\n",
            "[INFO|configuration_utils.py:461] 2023-11-05 05:04:49,452 >> Configuration saved in /tmp/test-ner/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-05 05:04:50,653 >> Model weights saved in /tmp/test-ner/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-05 05:04:50,654 >> tokenizer config file saved in /tmp/test-ner/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-05 05:04:50,655 >> Special tokens file saved in /tmp/test-ner/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 0.1557, 'learning_rate': 3.5763097949886106e-05, 'epoch': 0.85}\n",
            " 28% 1500/5268 [01:21<03:08, 20.00it/s][INFO|trainer.py:2882] 2023-11-05 05:05:16,588 >> Saving model checkpoint to /tmp/test-ner/checkpoint-1500\n",
            "[INFO|configuration_utils.py:461] 2023-11-05 05:05:16,589 >> Configuration saved in /tmp/test-ner/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-05 05:05:17,786 >> Model weights saved in /tmp/test-ner/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-05 05:05:17,787 >> tokenizer config file saved in /tmp/test-ner/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-05 05:05:17,787 >> Special tokens file saved in /tmp/test-ner/checkpoint-1500/special_tokens_map.json\n",
            "{'loss': 0.1229, 'learning_rate': 3.1017463933181475e-05, 'epoch': 1.14}\n",
            " 38% 2000/5268 [01:48<02:33, 21.35it/s][INFO|trainer.py:2882] 2023-11-05 05:05:43,758 >> Saving model checkpoint to /tmp/test-ner/checkpoint-2000\n",
            "[INFO|configuration_utils.py:461] 2023-11-05 05:05:43,759 >> Configuration saved in /tmp/test-ner/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-05 05:05:44,960 >> Model weights saved in /tmp/test-ner/checkpoint-2000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-05 05:05:44,961 >> tokenizer config file saved in /tmp/test-ner/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-05 05:05:44,962 >> Special tokens file saved in /tmp/test-ner/checkpoint-2000/special_tokens_map.json\n",
            "{'loss': 0.1047, 'learning_rate': 2.6271829916476843e-05, 'epoch': 1.42}\n",
            " 47% 2500/5268 [02:15<02:12, 20.85it/s][INFO|trainer.py:2882] 2023-11-05 05:06:10,922 >> Saving model checkpoint to /tmp/test-ner/checkpoint-2500\n",
            "[INFO|configuration_utils.py:461] 2023-11-05 05:06:10,923 >> Configuration saved in /tmp/test-ner/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-05 05:06:12,119 >> Model weights saved in /tmp/test-ner/checkpoint-2500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-05 05:06:12,120 >> tokenizer config file saved in /tmp/test-ner/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-05 05:06:12,120 >> Special tokens file saved in /tmp/test-ner/checkpoint-2500/special_tokens_map.json\n",
            "{'loss': 0.112, 'learning_rate': 2.152619589977221e-05, 'epoch': 1.71}\n",
            " 57% 3000/5268 [02:43<01:47, 21.13it/s][INFO|trainer.py:2882] 2023-11-05 05:06:38,184 >> Saving model checkpoint to /tmp/test-ner/checkpoint-3000\n",
            "[INFO|configuration_utils.py:461] 2023-11-05 05:06:38,185 >> Configuration saved in /tmp/test-ner/checkpoint-3000/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-05 05:06:39,376 >> Model weights saved in /tmp/test-ner/checkpoint-3000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-05 05:06:39,377 >> tokenizer config file saved in /tmp/test-ner/checkpoint-3000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-05 05:06:39,378 >> Special tokens file saved in /tmp/test-ner/checkpoint-3000/special_tokens_map.json\n",
            "{'loss': 0.0943, 'learning_rate': 1.678056188306758e-05, 'epoch': 1.99}\n",
            " 66% 3500/5268 [03:10<01:21, 21.57it/s][INFO|trainer.py:2882] 2023-11-05 05:07:05,258 >> Saving model checkpoint to /tmp/test-ner/checkpoint-3500\n",
            "[INFO|configuration_utils.py:461] 2023-11-05 05:07:05,259 >> Configuration saved in /tmp/test-ner/checkpoint-3500/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-05 05:07:06,441 >> Model weights saved in /tmp/test-ner/checkpoint-3500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-05 05:07:06,442 >> tokenizer config file saved in /tmp/test-ner/checkpoint-3500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-05 05:07:06,442 >> Special tokens file saved in /tmp/test-ner/checkpoint-3500/special_tokens_map.json\n",
            "{'loss': 0.0637, 'learning_rate': 1.2034927866362947e-05, 'epoch': 2.28}\n",
            " 76% 4000/5268 [03:37<00:59, 21.48it/s][INFO|trainer.py:2882] 2023-11-05 05:07:32,326 >> Saving model checkpoint to /tmp/test-ner/checkpoint-4000\n",
            "[INFO|configuration_utils.py:461] 2023-11-05 05:07:32,327 >> Configuration saved in /tmp/test-ner/checkpoint-4000/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-05 05:07:33,519 >> Model weights saved in /tmp/test-ner/checkpoint-4000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-05 05:07:33,520 >> tokenizer config file saved in /tmp/test-ner/checkpoint-4000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-05 05:07:33,520 >> Special tokens file saved in /tmp/test-ner/checkpoint-4000/special_tokens_map.json\n",
            "{'loss': 0.0551, 'learning_rate': 7.289293849658315e-06, 'epoch': 2.56}\n",
            " 85% 4500/5268 [04:04<00:36, 20.77it/s][INFO|trainer.py:2882] 2023-11-05 05:07:59,409 >> Saving model checkpoint to /tmp/test-ner/checkpoint-4500\n",
            "[INFO|configuration_utils.py:461] 2023-11-05 05:07:59,410 >> Configuration saved in /tmp/test-ner/checkpoint-4500/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-05 05:08:00,577 >> Model weights saved in /tmp/test-ner/checkpoint-4500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-05 05:08:00,578 >> tokenizer config file saved in /tmp/test-ner/checkpoint-4500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-05 05:08:00,578 >> Special tokens file saved in /tmp/test-ner/checkpoint-4500/special_tokens_map.json\n",
            "{'loss': 0.0569, 'learning_rate': 2.5436598329536827e-06, 'epoch': 2.85}\n",
            " 95% 5000/5268 [04:31<00:12, 20.89it/s][INFO|trainer.py:2882] 2023-11-05 05:08:26,381 >> Saving model checkpoint to /tmp/test-ner/checkpoint-5000\n",
            "[INFO|configuration_utils.py:461] 2023-11-05 05:08:26,382 >> Configuration saved in /tmp/test-ner/checkpoint-5000/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-05 05:08:27,582 >> Model weights saved in /tmp/test-ner/checkpoint-5000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-05 05:08:27,583 >> tokenizer config file saved in /tmp/test-ner/checkpoint-5000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-05 05:08:27,583 >> Special tokens file saved in /tmp/test-ner/checkpoint-5000/special_tokens_map.json\n",
            "100% 5267/5268 [04:47<00:00, 21.16it/s][INFO|trainer.py:1955] 2023-11-05 05:08:42,511 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 287.3971, 'train_samples_per_second': 146.567, 'train_steps_per_second': 18.33, 'train_loss': 0.11633665125389939, 'epoch': 3.0}\n",
            "100% 5268/5268 [04:47<00:00, 18.33it/s]\n",
            "[INFO|trainer.py:2882] 2023-11-05 05:08:42,519 >> Saving model checkpoint to /tmp/test-ner\n",
            "[INFO|configuration_utils.py:461] 2023-11-05 05:08:42,520 >> Configuration saved in /tmp/test-ner/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-05 05:08:43,734 >> Model weights saved in /tmp/test-ner/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-05 05:08:43,735 >> tokenizer config file saved in /tmp/test-ner/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-05 05:08:43,735 >> Special tokens file saved in /tmp/test-ner/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.1163\n",
            "  train_runtime            = 0:04:47.39\n",
            "  train_samples            =      14041\n",
            "  train_samples_per_second =    146.567\n",
            "  train_steps_per_second   =      18.33\n",
            "11/05/2023 05:08:43 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-05 05:08:43,781 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, tokens, ner_tags. If id, tokens, ner_tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3159] 2023-11-05 05:08:43,783 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3161] 2023-11-05 05:08:43,783 >>   Num examples = 3250\n",
            "[INFO|trainer.py:3164] 2023-11-05 05:08:43,783 >>   Batch size = 8\n",
            "100% 407/407 [00:07<00:00, 57.52it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.9616\n",
            "  eval_f1                 =     0.7866\n",
            "  eval_loss               =     0.1473\n",
            "  eval_precision          =     0.8092\n",
            "  eval_recall             =     0.7652\n",
            "  eval_runtime            = 0:00:07.10\n",
            "  eval_samples            =       3250\n",
            "  eval_samples_per_second =    457.679\n",
            "  eval_steps_per_second   =     57.316\n"
          ]
        }
      ],
      "source": [
        "!python run_ner.py \\\n",
        " --model_name_or_path bert-base-multilingual-uncased \\\n",
        " --task_name ner \\\n",
        " --dataset_name ju-bezdek/conll2003-SK-NER \\\n",
        " --output_dir /tmp/test-ner \\\n",
        " --do_train \\\n",
        " --do_eval\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}